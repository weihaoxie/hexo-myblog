<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2019_Scale-Aware Trident Networks for Object Detection_ICCV2019(Oral)_LiY et al</title>
      <link href="/post/879cef5f.html"/>
      <url>/post/879cef5f.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>尺度变化对于目标检测来说，是个很关键的挑战。为了应对该挑战，有两个主要的方向，一个是利用图像金字塔，另一个则是使用网络内的特征金字塔来近似图像金字塔。多尺度的训练和测试以及SNIP都是利用了图像金字塔带来了整体检出性能的提高，特别是SNIP提高更是显著，但是这种方式大大增加了推断的时间，如fig1(a)所示。SSD和FPN则是利用了网络中的多层特征来近似图像金字塔，虽然取得了一定的效果，但是通过不同的特征层来提取目标的不同尺度信息并无法替代图像金字塔，因为输入两个不同分辨率的图像，其在网络中对应于同个图像分辨率下的同个目标区域的特征是不同的，这里特征金字塔中的每层特征都有一系列不同的参数，使得不同层的特征不具有同等的表达能力，这样也有过拟合的风险，如fig1(b)所示。不管图像金字塔还是特征金字塔，都有相同的动机，就是不同尺度的目标应该具有不同的感受野。为了利用图像金字塔和特征金字塔的优点，摈弃它们的缺点，作者在这篇论文中，首先通过一个探究性的实验，探究在目标检测任务中感受野对于不同尺度的目标有什么影响。然后基于对探究性实验的分析得出的结论，提出一个新颖的三分支网络(TridentNet)，如fig1(c)所示，该网络可以使用同等的表达能力为同个目标，生成不同尺度的特征图。TridentNet是一个并行的多分支结构，每个分支共享相同的参数，但是具有不同的感受野，不同感受野的实现主要是靠调整孔洞卷积的dilated rate参数。为了让具有不同感受野的分支能够适应不同的尺度，作者提出了一个尺度适应的训练框架。该训练框架主要是在对每个分支进行训练的时候，只使用适应该分支感受野大小的目标来训练该分支。另外由于TridentNet在推断的时候需要跑3次网络，比较影响推断速度，为此在推断的时候作者提出了TridentNet的快速近似版本。TridentNet的近似版只需利用中间分支跑一次网络，相较于原版快速版本只牺牲了很小的性能，但是相较于普通的目标检测器，快速版本可以在没有额外参数和计算代价的情况下极大提高检出性能，也使得该方法更加能够应用到实际中。最后作者在使用ResNet-101作为backbone的情况下，将COCO目标检测的mAP提高到48.4，这是目前单模型能达到的最高性能。<br><a href="https://git.io/fj5vR" target="_blank" rel="noopener">代码:https://git.io/fj5vR</a><br><img src="/post/879cef5f/18FE1E09-1D36-4C34-8A60-8906298A2A39.png"></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-本文的创新点及贡献"><a href="#2-1-本文的创新点及贡献" class="headerlink" title="2.1 本文的创新点及贡献"></a>2.1 本文的创新点及贡献</h2><ol><li>通过探究不同感受野对不同的尺度的目标的影响，揭露了为获得更好的性能，大目标应该使用更大的感受野，而小目标则应该使用相对小的感受野。</li><li>提出TridentNet去解决目标检测任务中目标尺度变化范围较大的问题。该方法通过孔洞卷积来实现不同尺度大小的目标，使用不同的感受野进行识别；并使用尺度适应的方式选择合适大小的目标来训练具有不同感受野的分支。最后通过该方法可以为同个目标生成具有同等表达能力的多尺度特征，但是只需要输入单分辨率的图像。</li><li>为了加快推断速度，作者提出了TridentNet的近似版本，该版本受益于权重共享的trident-block设计，只需要利用中间分支跑一次网络，在没有增加额外参数和计算代价的情况下，性能稍微比原版差一点，但是比起最先进的目标检测方法性能要提高很多。</li><li>作者在COCO数据集上验证了该方法的有效性，在使用ResNet-101作为backbone的情况下，mAP提高到48.4，这是目前单模型下能达到的最高性能。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-探究不同感受野对不同尺度的目标检出性能的影响"><a href="#3-1-探究不同感受野对不同尺度的目标检出性能的影响" class="headerlink" title="3.1 探究不同感受野对不同尺度的目标检出性能的影响"></a>3.1 探究不同感受野对不同尺度的目标检出性能的影响</h2><p>对于检出器来说，有很多设计因素会影响到最终的检出性能，包括网络深度，下采样率和感受野。网络深度和降采样率对结果的影响已经在很多工作中被调查过了，一般来说深度越深效果越好，下采样率越小效果越好。但是目前还没有对感受野的影响进行调查，为此作者首先设计了一个探究性实验，探究了感受野对目标检测的性能的影响。<br>为了让检出器在同等复杂度的情况下，具有不同的感受野，作者使用空洞卷积替代了backbone网络中某些卷积层，利用dialation rate来控制网络的感受野。<br>作者以ResNet-C4作为backbone，测试了具有不同感受野的Faster RCNN在COCO检出任务上的性能。评价指标使用COCO-style的mmAP，并给出了所有目标，小目标，中等目标和大目标的mmAP。作者测试了两个backbone，ResNet-50和ResNet-101，并通过改变第4个stage中的residual blocks中的$3\times3$卷积层的dilation rate，来得到同等复杂度不同感受野的Faster R-CNN，dilation rate的变化范围从1到3。<br><img src="/post/879cef5f/F44BA5E0-D793-47A0-9237-B53F4C05AEA6.png"><br>Table1总结了两个不同backbone在不同的dilation rate下的性能。从Table1中可以看出随着感受野的增大，小目标的性能在不断下降，而大目标的性能则在不断提高。从中也可以总结出两点：1）网络的感受野会影响不同尺度的目标的检出性能，即适合的感受野能提高特定尺度范围内的目标的检出性能。2）尽管ResNet-101理论上具有足够大的感受野去覆盖COCO数据集中所有大尺度的目标，大目标的检出性能也可以通过进一步增大感受野来得到提升。这也暗示了有效的感受野要小于理论的感受野。通过增大dilation rate可以提高有效感受野使大目标的性能得到提高，但是同时也会影响到小目标的性能。所以不同尺度的目标要平衡好不同的感受野，才能达到更好的效果。</p><h2 id="3-2-TridentNet"><a href="#3-2-TridentNet" class="headerlink" title="3.2 TridentNet"></a>3.2 TridentNet</h2><p>如Fig2所示，TridentNet主要包括了权重共享的Trident blocks以及适应不同分支的感受野的尺度适应训练框架。另外为了加快推断速度，作者提出TridentNet的快速版本。下面分别就这3部分做介绍。<br><img src="/post/879cef5f/50AE4300-DA10-42AE-B59D-579FAA411666.png"></p><h3 id="3-2-1-权重共享的Trident-blocks"><a href="#3-2-1-权重共享的Trident-blocks" class="headerlink" title="3.2.1 权重共享的Trident blocks"></a>3.2.1 权重共享的Trident blocks</h3><p>作者提出的TridentNet其实就是利用提到的Trident block去替代检测器中某些卷积blocks。Trident block包括多个并行分支，每个分支与原本的卷积blocks共享相同的结构，除了采用不同的dilation rate。<br><img src="/post/879cef5f/0A05715B-0F45-4A6D-80EE-A860B33AB719.png"><br>以ResNet为例，每个bottlenet形式的residual block包括3个卷积层，kernal size分别为$1\times1,3\times3,1\times1$。如Fig3所示，与其对应的trident block则是具有同样的卷积层数，同样的kernal size，但是每个分支的$3\times3$卷积层具有不同的dilation rates。通过堆叠多个trident block可以很容易去控制不同分支的感受野。一般来说，会将trident block放置在最后一个stage，因为最后一个stage的stride比较大，这样各个分支的感受野差距比较明显。<br>由于每个分支具有相同的结构，所以可以让多个分支的权重进行共享，这样也避免了过拟合。在本文中，每个分支除了dilation rate不同之外，其它的都相同。权重共享带来了3个好处:1）相较于原本的检测器，不需要增加额外的参数；2）同个目标的不同尺度输入到网络之后得到的特征的表达能力是相同的；3）形态，姿势等变换参数可以从所有的目标中学习到，而不会减少训练目标的数量，从而不会影响到性能。</p><h3 id="3-2-2-适应不同分支的感受野的训练框架"><a href="#3-2-2-适应不同分支的感受野的训练框架" class="headerlink" title="3.2.2 适应不同分支的感受野的训练框架"></a>3.2.2 适应不同分支的感受野的训练框架</h3><p>从Table1中可以看出，感受野与目标尺度的不匹配会导致性能的下降。即使在多个分支的情况下，如果使用所有的目标来训练，那每个分支都存在尺度与感受野不匹配的目标，这将会影响每个分支的性能，从而影响到最终性能。为此作者在训练具有不同感受野的分支的时候，使用了尺度与其适应的目标来训练。<br>与SNIP类似，作者为每个分支定义了一个有效的尺度范围，在训练的时候只选择尺度在有效范围内的grouth truth和proposals。具体的对于在原图上(没resize之前)宽度为w，高度为h的ROIs，其有效的尺度范围如下：<br><img src="/post/879cef5f/B4EC03D2-CD2A-437F-89F1-CCDCCEA3DDFD.png"><br>这个训练框架可以同时应用到RPN和R-CNN。在训练RPN的时候，作者只选择每个分支对应的有效的grouth truth boxes为每个分支的anchor分配标签。在训练R-CNN的时候，作者将每个分支无效的proposals移除掉，不用于训练。</p><h3 id="3-2-3-用于推断的快速版本"><a href="#3-2-3-用于推断的快速版本" class="headerlink" title="3.2.3 用于推断的快速版本"></a>3.2.3 用于推断的快速版本</h3><p>TridentNet的推断过程是，首先为每个分支生成检出结果，然后将各个分支中没有落在其有效范围内的目标全部过滤掉，最后将所有有效的目标组合在一起，利用NMS或者soft-NMS过滤掉冗余的目标，得到最后的结果。<br>由于每个分支需要跑一次测试结果，这种方式会比较影响推断的速度，为此作者剔除了TridentNet的快速版本。TridentNet的快速版只需要对一个主要分支进行预测，这里作者使用了中间的分支，因为中间分支的有效范围同时覆盖了大目标和小目标。TridentNet的快速版本可以在牺牲很小的检出性能的情况下，达到与普通检出器同等的推断速度。这可能是由于权重共享策略，通过权重共享和多分支的训练，就等同于在网络内部应用多尺度增强。</p><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><p>作者在COCO数据集上进行实验，并按照通用的数据集使用方式，将80k训练集图片和验证集中的35k图片合并起来作为训练集，并利用验证集中的剩余5k图片作为一个小的测试集(minival)。在跟最先进方法比较的时候，作者也给出了它们在20k张测试集图片上的效果(test-dev)。</p><h2 id="4-1-实现细节"><a href="#4-1-实现细节" class="headerlink" title="4.1 实现细节"></a>4.1 实现细节</h2><ul><li>作者利用mxnet重新实现了Faster R-CNN检测器。</li><li>检测器的backbone是在ImageNet1000上进行预训练的，在使用预训练的模型初始化的时候，作者固定了第一个residual阶段，并冻结所有的BN参数。</li><li>输入的图片被rescale到短边为800。</li><li>采用随机翻转进行数据增强。</li><li>模型在8个GPU上进行训练，一个batchsize16张图片。</li><li>默认情况下，模型训练12个epoch。</li><li>起始学习率为0.02，然后在8个和第10个epoch，学习率分别乘以0.1。</li><li>这里的2$\times$和3$\times$分别表示将训练的epoch总数翻倍和翻三倍，对应的学习率机制也对应发生改变。</li><li>RPN网络接在ResNet的Conv4上，R-CNN网络接在ResNet的Conv5上。</li><li>默认使用3个分支，三个分支的dilation rates被设置为1，2，3。</li><li>对于每个分支，在NMS之前取top12000个候选目标，在NMS之后取top500个候选目标，并采集128个ROIs训练R-CNN。</li><li>当采用适应不同分支感受野的尺度适应框架来训练的时候，三个分支选择的目标尺度分别为[0,90]，[30,60]，[90,无穷大]。</li><li>评估方法使用平均精度AP，也给出IOU为0.5的AP和IOU为0.75的AP，作者也给出了COCO-style的$AP_s$,$AP_m$和$AP_l$，分别表示小目标(像素小于$32\times32$)、中等目标(像素大于$32\times32$小于$96\times96$)和大目标(像素大于$96\times96$)的平均精度。</li></ul><h2 id="4-2-探究提到方法中的不同结构对性能的影响"><a href="#4-2-探究提到方法中的不同结构对性能的影响" class="headerlink" title="4.2 探究提到方法中的不同结构对性能的影响"></a>4.2 探究提到方法中的不同结构对性能的影响</h2><h3 id="4-2-1-探究TridentNet各个组成部分对性能的影响"><a href="#4-2-1-探究TridentNet各个组成部分对性能的影响" class="headerlink" title="4.2.1 探究TridentNet各个组成部分对性能的影响"></a>4.2.1 探究TridentNet各个组成部分对性能的影响</h3><img src="/post/879cef5f/38F70A16-04C1-4CCE-99A3-9E1C6550AC53.png"><p>Table2给出了在Baseline的基础上，逐步增加各个组成部分后对整体检出性能的影响。这里的Baseline分别是以ResNet-101和ResNet-101-Deformable为backbone的faster R-CNN。在此基础上作者逐步应用多分支体系结构，权重共享设计和适应不同感受野的尺度适应训练框架来看看各个部分对整体检出性能的影响。<br>Table2(b)为直接使用3个具有不同的感受野不进行权重共享的分支来进行目标检测。从Table2中可以看到，使用多个不同感受野的分支，对两个不同backbone的faster R-CNN检出器的性能，都带来了提升。对于大目标的提升更是明显。这个也证明了使用不同的感受野对不同尺度目标检出性能的提高很关键。<br>Table2(d)为在多分支的基础上，使用尺度适应框架来训练的结果。从结果来看，整体的性能并没有得到提升，虽然它提高了小目标的检出性能，但是却降低了大目标的检出性能。作者认为虽然各个分支使用了合适的尺度进行训练，避免了使用过大过小的目标尺度的情况，但是整体训练样本却减少了，这样有可能带来过拟合。<br>Table2(c)为在多分支的基础上，进行权重共享，但是每个分支使用所有尺度的目标来训练。从Table2(c)中可以看出权重共享在两个不同的backbone下都得到了提高。通过权重共享的方式可以减少模型的参数避免过拟合，从而是检出器的性能得到提高。<br>Table2(e)为在权重共享的基础上，使用尺度适应的训练框架。由于权重共享设置，使得网络训练的时候仍然使用所有的样本来训练，减少了过拟合，从而是最终的结果得到提高。但是这里其实提高并不明显。<br>从结果也可以看出，提到的Trident block与deformable convolution是互补的，都带来了性能的提高。</p><h3 id="4-2-3-探究分支数对性能的影响"><a href="#4-2-3-探究分支数对性能的影响" class="headerlink" title="4.2.3 探究分支数对性能的影响"></a>4.2.3 探究分支数对性能的影响</h3><img src="/post/879cef5f/E8D9E9B0-2E0A-488B-8096-7BD3C760F62F.png"><p>Table3给出了使用1个到4个分支的情况下，TridentNet的性能，这里并没有应用尺度适应框架，避免干扰试验的客观性。从Table3可以看出多分支比起单分支效果要提高不少，但是当分支数超过3个之后，性能开始饱和，为此作者只选择3个分支，也因此把该方法命名为TridentNet。</p><h3 id="4-2-4-探究在网络中哪个stage放置Trident-blocks最好"><a href="#4-2-4-探究在网络中哪个stage放置Trident-blocks最好" class="headerlink" title="4.2.4 探究在网络中哪个stage放置Trident blocks最好"></a>4.2.4 探究在网络中哪个stage放置Trident blocks最好</h3><img src="/post/879cef5f/070AA4D7-6906-4BC1-8B70-B9F3C1649D37.png"><p>Table4给出了将Trident blocks放置在conv2，conv3，conv4 stage上的效果，对应的stride分别为4、8和16。从Table4可以看出，相较于放置在Conv4 stage，放置在Conv2，Conv3 stage上时，性能只比baseline提高一点点。这是因为conv2、conv3 stage对应的stride不够大，使得3个分支的感受野差异较小。</p><h3 id="4-2-5-探究放置多少Trident-blocks最好"><a href="#4-2-5-探究放置多少Trident-blocks最好" class="headerlink" title="4.2.5 探究放置多少Trident blocks最好"></a>4.2.5 探究放置多少Trident blocks最好</h3><img src="/post/879cef5f/10737DC6-2D77-4AC3-AE6D-54143948C949.png"><p>由于在Conv4 stage有很多个residual block，那应该放置多少个trident block合适呢。Fig4给出了将Conv4 stage中不同数量的residual block替换为trident block后的效果。从Fig4可以看出，当Trident block的数量超过10，TridentNet的性能就比较稳定了，也不会出现性能下降的情况。这也说明了在各个分支的感受野差异足够大的情况下，TridentNet中tridnet block的数量具有很强的鲁棒性。</p><h3 id="4-2-6-探究每个分支的性能"><a href="#4-2-6-探究每个分支的性能" class="headerlink" title="4.2.6 探究每个分支的性能"></a>4.2.6 探究每个分支的性能</h3><img src="/post/879cef5f/C82422A8-5257-46F5-BDFF-7903A02091F3.png"><p>Table5给出了TridentNet中不同分支的性能及多个分支组合的性能，这里的TridentNet作者也使用了尺度训练框架。从Table5可以看出，具有最小感受野且利用小尺度目标来训练的branch-1对小尺度目标的检出效果最好；branch-2则是对中尺度的目标效果最好；branch-3对大尺度的目标效果最好。最后三分支的组合方法，从多个分支中继承了它们的优点，取得了最好的效果。</p><h2 id="4-3-快速版本对性能的影响"><a href="#4-3-快速版本对性能的影响" class="headerlink" title="4.3 快速版本对性能的影响"></a>4.3 快速版本对性能的影响</h2><img src="/post/879cef5f/C388A557-D3F3-44BA-AAC5-4550FBC2DBFE.png"><p>为了减少TridentNet的推断时间，作者提出了TridentNet的快速版本，快速版本只使用了TridentNet的一个主要分支来进行推断，因此可以大大节省推断时间。如Table5所示，branch-2效果是最好的，很自然地成为主分支的候选。branch-2由于同时包含了其他两个分支的结果，可以得到比其他两个分支更优的效果。Table6为使用不同尺度来训练三个分支的情况下，TridentNet的快速版本的效果。从Table6中可以看出，随着中间分支适应的尺度范围的增大，整体的AP也不断的提高，从Table6(c)可以看出当中间分支适应所有尺度范围的情况下，比起baseline AP要高出1.4个点。在将3个分支适应的尺度全部扩大到支持所有尺度的情况下，AP进一步提高到40.0，这相较于普通版本AP只减低了0.6。作者认为这主要得益于权重共享策略，这种多分支的尺度无关的训练框架就类似在网络内执行了尺度增强策略。</p><h2 id="4-4-与最先进方法的比较"><a href="#4-4-与最先进方法的比较" class="headerlink" title="4.4 与最先进方法的比较"></a>4.4 与最先进方法的比较</h2><img src="/post/879cef5f/82235D7F-84F9-4A1E-B4DD-96F828CF5ACD.png"><p>Table7给出了最先进的目标检测方法及TridentNet的不同设置在COCO test-dev数据集下的测试结果。从Table7中可以看出，TridentNet应用到以ResNet-101为backbone的faster R-CNN检测器上并在2$\times$epoch数的情况下AP可以达到42.7。为了与SNIP和SNIPER进行公平的比较，在使用跟它们一致的多尺度训练，soft-NMS，Deformable卷积，large-batch BN和3$\times$的epochs数量的情况下，AP可以达到46.8，如Table7中的$TridentNet^·$，这个指标已经超过了SNIP和SNIPER，这里还没有使用图像金字塔。在使用图像金字塔进行测试的情况下，可以将$TridentNet^*$的性能进一步提高到48.4。这是目前为止以resnet-101为backbone的情况下达到的最优的性能。在使用快速版本+图像金字塔进行测试的时候，$TridentNet^·$的AP可以达到47.6。<br><img src="/post/879cef5f/360BD2E9-6EA1-483D-B3F6-9FDFCFB61BF8.png"><br>Table8给出了在R-CNN直接采用2个fc层来进行精细的分类和回归而不是采用conv5之后的网络层来进行精细的分类和回归的情况下，TridentNet和FPN，ASPP的对比情况。从Table8中可以看出，TridentNet在各个尺度上都要由于其他方法。在使用快速版本的情况下，可以AP可以达到41.0，比起baseline要高出1.2个点，这也进一步说明了该方法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 尺度问题 </tag>
            
            <tag> TridentNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python2.x和python3.x主要的差异</title>
      <link href="/post/b8f3def3.html"/>
      <url>/post/b8f3def3.html</url>
      
        <content type="html"><![CDATA[<p>参考：<br><a href="">https://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/key_differences_between_python_2_and_3.ipynb#The-next-function-and-next-method</a></p><h1 id="1-信息打印的区别，python3需要以函数的方式调用，而python2不需要"><a href="#1-信息打印的区别，python3需要以函数的方式调用，而python2不需要" class="headerlink" title="1.信息打印的区别，python3需要以函数的方式调用，而python2不需要"></a>1.信息打印的区别，python3需要以函数的方式调用，而python2不需要</h1><pre><code class="lang-python">%%script python2a = (2, 3)print a</code></pre><pre><code>(2, 3)</code></pre><pre><code class="lang-python">%%script python3a = (2, 3)print a</code></pre><pre><code>File &quot;&lt;stdin&gt;&quot;, line 2    print a          ^SyntaxError: Missing parentheses in call to &#39;print&#39;. Did you mean print(a)?</code></pre><pre><code class="lang-python">%%script python3a = (2, 3)print(a)</code></pre><pre><code>(2, 3)</code></pre><h1 id="2-整数除法的区别"><a href="#2-整数除法的区别" class="headerlink" title="2.整数除法的区别"></a>2.整数除法的区别</h1><p>python2默认是整除，python3默认是非整除</p><pre><code class="lang-python">%%script python2a = 2 / 1b = 2 // 1c = 2 / 1.0print a,b,c</code></pre><pre><code>2 2 2.0</code></pre><pre><code class="lang-python">%%script python3a = 2 / 1b = 2 // 1c = 2 / 1.0print(a,b,c)</code></pre><pre><code>2.0 2 2.0</code></pre><h1 id="3-字符串存储的区别，python3默认将字符串存储为unicode形式，而python2则是ASCLL方式，存储为unicode需要显式指定u’’-，python3增加了两个类型，byte类型和bytearray类型"><a href="#3-字符串存储的区别，python3默认将字符串存储为unicode形式，而python2则是ASCLL方式，存储为unicode需要显式指定u’’-，python3增加了两个类型，byte类型和bytearray类型" class="headerlink" title="3.字符串存储的区别，python3默认将字符串存储为unicode形式，而python2则是ASCLL方式，存储为unicode需要显式指定u’’ ，python3增加了两个类型，byte类型和bytearray类型"></a>3.字符串存储的区别，python3默认将字符串存储为unicode形式，而python2则是ASCLL方式，存储为unicode需要显式指定u’’ ，python3增加了两个类型，byte类型和bytearray类型</h1><pre><code class="lang-python">%%script python2s = &quot;I am Chinese.&quot;print type(s)s = u&quot;I am Chinese.&quot;print type(s)</code></pre><pre><code>&lt;type &#39;str&#39;&gt;&lt;type &#39;unicode&#39;&gt;</code></pre><p>python3 字符串当前类型都为unicode</p><p>并增加了byte类型和bytearray类型</p><pre><code class="lang-python">%%script python3import sysprint(&#39;Python %s.%s.%s&#39; %sys.version_info[:3])a = &quot;I am Chinese.&quot;b = &quot;我是中国人&quot;c = a + bprint(type(a),type(b),c)d = b&#39; bytes for storing data&#39;print(type(d))e = bytearray(b&#39;bytearrays&#39;)print(type(e))</code></pre><pre><code>Python 3.7.0&lt;class &#39;str&#39;&gt; &lt;class &#39;str&#39;&gt; I am Chinese.我是中国人&lt;class &#39;bytes&#39;&gt;&lt;class &#39;bytearray&#39;&gt;</code></pre><h1 id="4-异常处理的区别，抛出异常和接收异常表达方式存在差异"><a href="#4-异常处理的区别，抛出异常和接收异常表达方式存在差异" class="headerlink" title="4.异常处理的区别，抛出异常和接收异常表达方式存在差异"></a>4.异常处理的区别，抛出异常和接收异常表达方式存在差异</h1><pre><code class="lang-python">%%script python2try:    a = 1 / 0except Exception, err:    print errtry:    inputValue=&quot;test&quot;    if type(inputValue)!=type(1):        raise ValueError,&quot;input value is not a interger&quot;    else:        print inputValueexcept ValueError, err:    print err</code></pre><pre><code>integer division or modulo by zeroinput value is not a interger</code></pre><p>区别主要在1) raise ValueError(“input value is not a interger”) 2) except ValueError as err:</p><pre><code class="lang-python">%%script python3try:    a = 1 / 0except Exception as err:    print(err)try:    inputValue=&quot;test&quot;    if type(inputValue)!=type(1):        raise ValueError(&quot;input value is not a interger&quot;)    else:        print(inputValue)except ValueError as err:    print(err)</code></pre><pre><code>division by zeroinput value is not a interger</code></pre><h1 id="5-在列表推导式中，循环变量的作用域的区别"><a href="#5-在列表推导式中，循环变量的作用域的区别" class="headerlink" title="5.在列表推导式中，循环变量的作用域的区别"></a>5.在列表推导式中，循环变量的作用域的区别</h1><pre><code class="lang-python">%%script python2i = 1print &#39;before: i =&#39;, iprint &#39;comprehension: &#39;, [i for i in range(5)]print &#39;after: i =&#39;, i</code></pre><pre><code>before: i = 1comprehension:  [0, 1, 2, 3, 4]after: i = 4</code></pre><pre><code class="lang-python">%%script python3i = 1print(&#39;before: i =&#39;, i)print(&#39;comprehension: &#39;, [i for i in range(5)])print(&#39;after: i =&#39;, i)</code></pre><pre><code>before: i = 1comprehension:  [0, 1, 2, 3, 4]after: i = 1</code></pre><h1 id="6-在python3中取消了xrange函数，统一使用range"><a href="#6-在python3中取消了xrange函数，统一使用range" class="headerlink" title="6.在python3中取消了xrange函数，统一使用range"></a>6.在python3中取消了xrange函数，统一使用range</h1><p>在python3中range的表现跟python2中的xrange类似</p><pre><code class="lang-python">%%script python2a =[i for i in xrange(10)]print aprint range(10)print xrange(10)</code></pre><pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9][0, 1, 2, 3, 4, 5, 6, 7, 8, 9]xrange(10)</code></pre><pre><code class="lang-python">%%script python3a =[i for i in xrange(10)]print(a)</code></pre><pre><code>Traceback (most recent call last):  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;NameError: name &#39;xrange&#39; is not defined</code></pre><pre><code class="lang-python">%%script python3a =[i for i in range(10)]print(a)print(range(10))</code></pre><pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]range(0, 10)</code></pre><h1 id="7-对于可迭代的对象，python3没有-next-方法，只有next-而python2两者都支持"><a href="#7-对于可迭代的对象，python3没有-next-方法，只有next-而python2两者都支持" class="headerlink" title="7.对于可迭代的对象，python3没有.next()方法，只有next();而python2两者都支持"></a>7.对于可迭代的对象，python3没有.next()方法，只有next();而python2两者都支持</h1><pre><code class="lang-python">%%writefile text.txt abcde</code></pre><pre><code>Writing text.txt</code></pre><pre><code class="lang-python">%%script python2f = open(&quot;text.txt&quot;,&quot;r&quot;)print f.next()print next(f)</code></pre><pre><code>ab</code></pre><pre><code class="lang-python">%%script python3f = open(&quot;text.txt&quot;,&quot;r&quot;)print(next(f))print(f.next())</code></pre><pre><code>aTraceback (most recent call last):  File &quot;&lt;stdin&gt;&quot;, line 3, in &lt;module&gt;AttributeError: &#39;_io.TextIOWrapper&#39; object has no attribute &#39;next&#39;</code></pre><h1 id="8-当比较不可比较的对象时，python3会抛出TypeError的异常"><a href="#8-当比较不可比较的对象时，python3会抛出TypeError的异常" class="headerlink" title="8.当比较不可比较的对象时，python3会抛出TypeError的异常"></a>8.当比较不可比较的对象时，python3会抛出TypeError的异常</h1><pre><code class="lang-python">%%script python2print [1, 2] &gt; &#39;foo&#39;print (1, 2) &gt; &#39;foo&#39;print [1, 2] &gt; (1, 2)</code></pre><pre><code>FalseTrueFalse</code></pre><pre><code class="lang-python">%%script python3try:    print([1, 2] &gt; &#39;foo&#39;)except TypeError as err:    print(err)try:    print((1, 2) &gt; &#39;foo&#39;)except TypeError as err:    print(err)try:    print([1, 2] &gt; (1, 2))except TypeError as err:    print(err)</code></pre><pre><code>&#39;&gt;&#39; not supported between instances of &#39;list&#39; and &#39;str&#39;&#39;&gt;&#39; not supported between instances of &#39;tuple&#39; and &#39;str&#39;&#39;&gt;&#39; not supported between instances of &#39;list&#39; and &#39;tuple&#39;</code></pre><h1 id="9-在python3中input-函数接收的对象都会以str类型存储，而python2则会根据具体输入形式以与该形式相符合的方式进行存储"><a href="#9-在python3中input-函数接收的对象都会以str类型存储，而python2则会根据具体输入形式以与该形式相符合的方式进行存储" class="headerlink" title="9.在python3中input()函数接收的对象都会以str类型存储，而python2则会根据具体输入形式以与该形式相符合的方式进行存储"></a>9.在python3中input()函数接收的对象都会以str类型存储，而python2则会根据具体输入形式以与该形式相符合的方式进行存储</h1><pre><code>Python 2.7.6 [GCC 4.0.1 (Apple Inc. build 5493)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; my_input = input(&#39;enter a number: &#39;)enter a number: 123&gt;&gt;&gt; type(my_input)&lt;type &#39;int&#39;&gt;&gt;&gt;&gt; my_input = raw_input(&#39;enter a number: &#39;)enter a number: 123&gt;&gt;&gt; type(my_input)&lt;type &#39;str&#39;&gt;</code></pre><pre><code>Python 3.7.0 (default, Sep 18 2018, 18:47:22)[Clang 9.1.0 (clang-902.0.39.2)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; my_input = input(&#39;enter a number: &#39;)enter a number: 123&gt;&gt;&gt; type(my_input)&lt;class &#39;str&#39;&gt;&gt;&gt;&gt; my_input = raw_input(&#39;enter a number: &#39;)Traceback (most recent call last):  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;NameError: name &#39;raw_input&#39; is not defined</code></pre><h1 id="10-python3对于很多可迭代目标它会直接返回可迭代目标，而不是list，对于只需要用一次的数据这样可以节约内存，但是对于需要多次使用的数据这样做可能会影响效率。对于后者可以显式使用list函数。"><a href="#10-python3对于很多可迭代目标它会直接返回可迭代目标，而不是list，对于只需要用一次的数据这样可以节约内存，但是对于需要多次使用的数据这样做可能会影响效率。对于后者可以显式使用list函数。" class="headerlink" title="10.python3对于很多可迭代目标它会直接返回可迭代目标，而不是list，对于只需要用一次的数据这样可以节约内存，但是对于需要多次使用的数据这样做可能会影响效率。对于后者可以显式使用list函数。"></a>10.python3对于很多可迭代目标它会直接返回可迭代目标，而不是list，对于只需要用一次的数据这样可以节约内存，但是对于需要多次使用的数据这样做可能会影响效率。对于后者可以显式使用list函数。</h1><pre><code class="lang-python">%%script python3print(range(10))print(list(range(10)))</code></pre><pre><code>range(0, 10)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre><pre><code class="lang-python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络的基本概念</title>
      <link href="/post/0.html"/>
      <url>/post/0.html</url>
      
        <content type="html"><![CDATA[<h3 id="一、深度学习简介"><a href="#一、深度学习简介" class="headerlink" title="一、深度学习简介"></a>一、深度学习简介</h3><p>　　深度学习是机器学习的一个子领域，它使用多层非线性信息处理及抽象，用于监督和非监督的特征学习和表示、分类及模式识别。深度学习在不同的学习任务上都取得了很大的成功,特别是在Imagenet的图像分类任务和LFW的人脸识别任务上更是超过了人类水平。相比于传统方法，深度学习具有很多优势。首先深度学习通用性更强，而传统方法针对不同的任务需要设计不同的特征。深度学习相较于传统方法可以自动从原始数据中提取特征，所以同个算法可以应用到各种类似的任务中来，比如目标检测任务，它可以同时用于人脸检测，行人检测，一般物体检测任务上。其次深度学习学习的特征具有很强的迁移能力，比如它在ImagetNet分类任务上学习的特征，在目标检测任务上也可以获得非常好的效果。最后一个优势是它的工程开发、优化、维护成本较低。深度学习计算主要是卷积和矩阵乘，针对这种计算优化，所有的深度学习算法都可以提升性能；另外，通过组合现有的层，我们可以实现大量复杂网络结构和算法，开发维护成本也很低。随着深度学习的发展，当前也存在着很多种不同类型的网络体系结构，如深度自编码网络、卷积神经网络、时间递归神经网络等。其中应用最广泛的是卷积神经网络。<br><a id="more"></a></p><h3 id="二、卷积神经网络的发展过程"><a href="#二、卷积神经网络的发展过程" class="headerlink" title="二、卷积神经网络的发展过程"></a>二、卷积神经网络的发展过程</h3><p>　　卷积神经网络是一种深度学习网络结构，它的发明灵感来自于生物的自然视觉感知机制。在1990年，LeCun等人$^{[1]}$发表了开创性的论文建立了CNN的现代框架，并随后提出了对它的改进$^{[2]}$。他们开发了一个多层的人工神经网络叫做LeNet-5，用于识别手写体数字图片。与其它神经网络类似，LeNet-5具有多个网络层，并且可以通过反向传播算法$^{[3]}$来训练。它可以从输入图片中学习到有效的表示，这也让它可以直接通过原图来进行识别，而不需要先对图片提取特征。但是由于当时缺少大规模的训练数据以及受到计算能力的限制，他们的网络在更复杂的问题上(如大规模的图片和视频分类任务)无法表现得很好。<br>　　自2006年以来，很多的方法已经被开发出来用于改进卷积神经网络$^{[5-8]}$。其中最著名的是Krizhevsky等人提出的经典的CNN体系结构，AlexNet$^{[6]}$，该体系结构在图像分类任务上的性能大大超过了先前的方法，在图像分类任务上的top-1和top-5错误率分别从47.1%和28.2%降低到37.5%和17.0%。AlexNet在网络结构上与LeNet-5类似，但是网络结构比LeNet-5更加深。随着AlexNet的成功，很多的工作被提出来改进它的性能。其中，最有代表性的工作是ZFNet$^{[9]}$，VGGNet$^{[7]}$, GoogleNet$^{[8]}$,和ResNet$^{[10]}$。随着更多改进方法的提出，当前卷积神经网络在很多任务上都获得很高的性能，并且在某些任务中超过了人类水平。</p><h3 id="三、卷积神经网络的基本部件及训练方法简介"><a href="#三、卷积神经网络的基本部件及训练方法简介" class="headerlink" title="三、卷积神经网络的基本部件及训练方法简介　"></a>三、卷积神经网络的基本部件及训练方法简介　</h3><p>　　我们将输入数据输入到网络中，并经过卷积层、池化层、全连接层，最后到达输出层输出结果的过程称为前向传播。前向传播得到的估计值通常与实际结果会有误差，模型的训练过程就是不断缩小估计值与实际值之间的误差的过程。在这个过程中我们利用损失函数来衡量估计值和实际结果的误差带来的损失，并试图让该损失尽可能小，为此我们将误差带来的损失从输出层向隐含层传播，直至传播到输入层，这个过程称为反向传播。在传播过程中通过某种误差调整策略，如随机梯度下降，调整模型参数，使得模型误差减小。整个模型的训练过程就是不断迭代前向传播，反向传播并调整模型参数这个过程，直至最后模型收敛。在模型收敛之后，模型的每次预测过程就是一次前向传播过程。虽然当前存在很多不同的卷积神经网络体系结构，但是它们的基本部件是很相似的。如LeNet-5，它主要包含3种部件，卷积层、池化层和全连接层。下面给出了卷积神经网络的各个部件及其训练方法的简单介绍。</p><div style="width: 950px; margin: auto"><img src="/post/0/图片1.1.png"></div> <div align="center"> 图一.LeNet-5网络结构 </div><h4 id="1-卷积层"><a href="#1-卷积层" class="headerlink" title="1.卷积层"></a>1.卷积层</h4><p>　　卷积层的目的是学习输入的特征表示，如图一所示，卷积层主要由多个卷积核构成，每个卷积核被用于计算不同的特征图。图二给出了卷积计算的一个例子，其中图二(a)为卷积核，图二(b)中蓝色矩阵为输入的特征图，绿色矩阵为利用图二(a)中的卷积核对输入特征图进行卷积计算的结果。输出的矩阵中的每个神经元与输入特征图的局部区域相连，这个局部区域就是当前神经元在上一层的感受野。输出矩阵的计算方式是利用卷积核对每个局部区域进行卷积，即逐元素相乘再相加。在得到卷积结果之后，新的特征图可以通过对每个从局部区域卷积计算得到的神经元应用非线性激活函数得到。这里值得注意的是，同个特征图的生成过程中，输入的多个局部区域共享同个卷积核。最终多个特征图的生成，则是由多个不同的卷积核计算得到。我们把在第l层的第k个特征图中的第(i,j)个位置(这里(i,j)表示第i行第j列)的特征值表示为$z^l_{i,j,k}$，则：</p><div style="width: 200px; margin: auto"><img src="/post/0/图片3.2.png"></div>　　其中w表示第l层第k个滤波器的权重，b表示该滤波器对应的偏置项，x表示当前响应值对应的输入块，其中心位置对应特征图中的第(i,j)个位置。这里w为当前特征图中每个特征值所共享。这种权重共享方式具有多种优势，它可以降低模型复杂度，并且可以让网络更加容易训练。在卷积神经网络中引入非线性激活函数，可以让多层网络学习到非线性特征。用 $z^l_{i,j,k}$ 表示非线性激活函数，则卷积特征值的激活值$a^l_{i,j,k}$可以表示为：<div style="width: 200px; margin: auto"><img src="/post/0/图片6.png"></div>经典的激活函数是sigmoid、tanh、ReLU和Leaky ReLU，图三中给出了这四种常见的激活函数。图四中显示了两个卷积层从数字图片7中学习到的特征图，其中左边为第一层卷积层的特征图，大小为6X28X28，右边为第三层卷积层的特征图，大小为16X10X10。从特征图可以看出底层卷积核主要用于学习低层次的信息，如边缘和曲线，而更高层的卷积核则用于学习更加抽象的信息。通过叠加多个卷积层和池化层，我们可以逐渐提取到更加高层次的特征表示。<div style="width: 150px; margin: auto"><img src="/post/0/图片7.4.png"></div> <div align="center"> 图二 (a).卷积核示例</div><div style="width: 750px; margin: auto"><img src="/post/0/图片8.png"></div> <div align="center"> 图二(b).卷积计算示例</div><div style="width: 450px; margin: auto"><img src="/post/0/图片9.1.png"></div> <div align="center"> 图三.常用的激活函数 </div><div style="width: 450px; margin: auto"><img src="/post/0/图片10.1.png"></div> <div align="center"> 图四.从数字图片7中学到的特征</div><h4 id="2-池化层"><a href="#2-池化层" class="headerlink" title="2.池化层"></a>2.池化层</h4><p>　　池化层通过减少特征图分辨率的方式让网络具有平移和旋转不变性。它经常放在两个卷积层之间。典型的池化操作是平均池化和最大池化。图五给出了池化操作的例子，其中图五(a)为平均池化，图五(b)为最大池化。池化层的每个特征图与它前一层的卷积层对应的特征图相连接。用$pool(.)$表示池化函数，对于卷积层的每个特征图$a^l_{:,:,k}$，对应的池化层的特征值可以表示为：</p><div style="width: 350px; margin: auto"><img src="/post/0/图片13.png"></div>其中$R_{i,j}$是卷积层特征图中第(i,j)个位置周围的一个局部区域。平均池化取池化区域的平均值，而最大池化则是取池化区域的最大值。 <div style="width: 750px; margin: auto"><img src="/post/0/图片15.2.png"></div> <div align="center"> 图五(a) 平均池化示例</div><div style="width: 750px; margin: auto"><img src="/post/0/图片16.png"></div> <div align="center"> 图五(b) 最大池化示例</div><h4 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3.全连接层"></a>3.全连接层</h4><p>　　在经过多个卷积和池化层之后，可能会连接一个或者多个全连接层来进行高层次的推理。图六给出了全连接层的例子，它们将前一层的所有的神经元作为输入，并将它们与当前层的每个神经元相连接，通过这种方式来生成全局的语义信息。这里的全连接层并不是必须的，它有时候也可以用1X1的卷积层来替代。 </p><div style="width: 750px; margin: auto"><img src="/post/0/图片17.2.png"></div> <div align="center"> 图六.全链接层示例 </div><h4 id="4-输出层及损失函数"><a href="#4-输出层及损失函数" class="headerlink" title="4.输出层及损失函数"></a>4.输出层及损失函数</h4><p>　　卷积神经网络的最后一层是输出层。对于分类任务，最常用的是softmax操作。另一个比较常用的方法是svm，它可以结合卷积层的特征来处理多种不同的分类任务。用K表示类别数，则其对应的softmax函数可以表示为：<br><img src="/post/0/图片18.png"><br>其中$\sigma(z)_j）$表示第j个类别的概率。对于回归任务，输出层一般为普通神经元。<br>用$\theta$表示所有可学习的参数(比如卷积核的权重和偏置项)，对于具体任务的最优参数，可以通过优化其对应的损失函数来得到。假设我们有N个有标签的训练样本数据，将其表示为{(x(n),y(n))};nЄ{[1,…,N]}，这里x(n)表示第n个输入数据，y(n)是其对应的标签，用o(n)表示卷积神经网络的输出，则其损失函数可以表示为：</p><div style="width: 250px; margin: auto"><img src="/post/0/图片21.png"></div>对于分类任务，损失函数一般选择交叉熵损失函数；对于回归任务，则一般为均方误差。<div style="width: 350px; margin: auto"><img src="/post/0/图片9.png"></div> <div align="center"> 图七.带有一层隐含层的前馈神经网络 </div><h4 id="５-反向传播算法"><a href="#５-反向传播算法" class="headerlink" title="５.反向传播算法"></a>５.反向传播算法</h4><p>　　训练卷积神经网络是一个全局优化的问题，通过最小化损失函数，我们可以找到最合适的参数集。通常我们采用随机梯度下降方法来优化CNN网络。CNN网络中梯度的计算主要采用了反向传播算法，其中主要利用了链式法则来对不同网络层的权重进行求导。<br>下面给出反向传播的一个简单例子，假设我们的网络结构如图七所示并假设损失函数为<br>均方误差，则损失函数可以表示为：<br><img src="/post/0/图片23.png"><br>其中x是输入数据，y是其对应的标签；σ为激活函数，这里选取sigmoid函数为激活函数；W1，b1、W2，b2分别为第一层和第二层的权重和偏置项。我们将中间变量定义为：<br><img src="/post/0/图片24.png"><br>则其前向传播过程如下：<br><img src="/post/0/图片25.1.png"><br>对应的中间变量的梯度即反向传播如下:<br><img src="/post/0/图片26.png"><br>通过链式法则可以很容易计算到各个参数的梯度，如W2和b2的梯度：<br><img src="/post/0/图片27.png"><br><img src="/post/0/图片28.png"></p><p>[1] B. B. Le Cun, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, Handwritten digit recognition with a back-propagation network, in: Proceedings of the Advances in Neural Information Processing Systems (NIPS), 1989, pp. 396–404.<br>[2] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of IEEE 86 (11) (1998) 2278–2324.<br>[3] R. Hecht-Nielsen, Theory of the backpropagation neural network, Neural Networks 1 (Supplement-1) (1988) 445–448.<br>[4] W. Zhang, K. Itoh, J. Tanida, Y. Ichioka, Parallel distributed processing model with local space-invariant interconnections and its optical architecture, Applied optics 29 (32) (1990) 4790–4797.<br>[5] X.-X. Niu, C. Y. Suen, A novel hybrid cnn–svm classifier for recognizing handwritten digits, Pattern Recognition 45 (4) (2012) 1318–1325.<br>[6] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition challenge, International Journal of Conflict and Violence (IJCV) 115 (3) (2015) 211–252.<br>[7] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.<br>[8] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1–9.<br>[9] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: Proceedings of the European Conference on Computer Vision (ECCV), 2014, pp. 818–833.<br>[10] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.</p>]]></content>
      
      
      <categories>
          
          <category> 总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> convolution neural network </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018_ShuffleNet V2:Practical Guidelines for Efficient CNN Architecture Design_CVPR2018_MaN et al</title>
      <link href="/post/97be11b5.html"/>
      <url>/post/97be11b5.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>之前网络结构设计的时候采用的度量模型复杂度的方式是浮点运算数。这种方式不够直接，因为影响模型运算的因素除了浮点运算数还有其他方面。这里影响速度的因素除了浮点运算数之外，主要还有内存获取的代价(MAC)(某些操作的MAC会很大，比如组卷积)，以及模型的可并行程度；另外运行平台也会对模型的运行速度带来影响，因为不同平台会对不同操作做优化。为此作者直接在指定平台上度量模型运算速度。为了设计出高效的网络结构，作者通过一系列的控制实验，推导出4个高效网络设计的指导原则，并依据这些指导原则，设计了一个新的、适用于移动设备的高效网络结构，并将其命名为shuffleNet v2。最后作者通过实验验证了该网络结构在同时考虑速度和准确度的情况下能达到当前最先进水平。在模型复杂度为40M浮点数的情况下，ShuffleNet V2要比ShuffleNet V1性能提高3.2%，要比MobileNet V2性能提高3.7%。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ol><li>提出了设计高效网络的4个指导原则。<ul><li>a 输入和输出的channel相等的情况下可以让MAC最小 </li><li>b 要小心增加分组卷积的分组个数带来的速度减缓</li><li>c 要减少设计单元中分支的数量</li><li>d 要尽可能少的使用逐元素操作</li></ul></li><li>根据提出的指导原则，设计了高效的网络结构，ShuffleNet v2.</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-高效网络的设计原则"><a href="#3-1-高效网络的设计原则" class="headerlink" title="3.1 高效网络的设计原则"></a>3.1 高效网络的设计原则</h2><p>作者在2个不同的平台上对2个当前最流行的高效网络结构的运行速度进行分析，并推导出4个设计高效网络的指导原则。</p><ul><li>作者使用的GPU平台为单个GTX1080Ti，并使用CUDNN7.0加速库，并且把CUDNN的底层函数激活让它能够对不同的卷积操作，选择最快的算法；使用的ARM平台为Qualcomm Snapdragon810,并使用一个基于Neon的高度优化的库，且只用单个线程。其他设置包括：输入的图片大小为224X224，每个网络随机初始化，并评估100次取平均作为真是的运算速度。</li><li>作者使用的网络结构为ShuffleNet V1和MobileNet V2。<img src="/post/97be11b5/13F53406-6FF1-43D5-97FA-BEC47AB0C4D1.png"></li><li>作者发现模型在不同平台上的运行速度主要可以分解为4个部分，浮点运算数、数据IO、逐点操作数和数据的shuffle。</li><li>推导出的四个设计高效网络的指导原则<img src="/post/97be11b5/1C9B77A3-AFBE-411C-9BAE-5365DAE3D185.png"><strong>G1. 输入和输出的channel数相等的情况下可以让MAC最小</strong>。为了验证该结论，作者构建了具有有10个构建块，每个构建块包括两个卷积层，通道数分别为c1和c2的卷积网络。Table1给出了，在浮点运算数基本相同的情况下，c1和c2在不同比例下的速度。从Table1可以看出，当c1和c2比例为1:1时，速度是最快的。<img src="/post/97be11b5/4165806D-3FC7-4DE6-B0A4-899608689BF2.png"><strong>G2. 过多的分组卷积会增加MAC</strong>。为了验证该结论，作者构建了一个具有10个逐点分组卷积层的网络。Table2给出了，在浮点运算数相等的情况下，不同的分组个数下网络的运算速度。从Table2可以看出，随着分组个数的增加，运算速度在不断下降。<img src="/post/97be11b5/6EF2665E-88E0-4F57-A676-4226B7B8C4B0.png"><strong>G3. block中的碎片化层度（网络层数量）会影响并行度</strong>。为了验证该结论，作者构建了多个具有不同卷积层数及不同组合方式的构建块，并堆叠同个构建块构建10层的网络结构。Table3给出了在浮点运算数相同的情况下，不同构建块的运算速度。从Table3可以看出，随着碎片化程度的增加，网络的运行速度越来越慢，但在CPU上的运行速度的减缓较为缓慢。<img src="/post/97be11b5/4414D1F4-36C9-4BAB-B07A-5A3FAE538816.png"><strong>G4. 要尽可能少的使用逐元素操作</strong>。为了验证逐元素操作对模型运行速度的影响，作者对比了resnet的bottlenet形式以及移除掉shortcut和ReLU操作的模型运行速度。从Table4可以看出，在移除掉shortcut和ReLU操作后，不管是在GPU还是在CPU,模型的运行速度可以提高约20%。</li></ul><h2 id="3-2-ShuffleNet-V2"><a href="#3-2-ShuffleNet-V2" class="headerlink" title="3.2 ShuffleNet V2"></a>3.2 ShuffleNet V2</h2><img src="/post/97be11b5/43FB637A-5C5E-48FA-8EE9-7F9E73735D73.png"><p>目前提出的高效网络结构，都或多或少违背了上面的几个原则。如ShuffleNet V1严重依赖分组卷积，这与G2相违背，并且其类似bottlenet的构建块也违背了G1；MobileNet V2逆bottlenet形式也违背了G1，并且在小的feature map上使用逐深度卷积和ReLu也违背了G4；自动生成的结构则被严重碎片化，违背了G3。为此作者在ShuffleNet V1的基础上，对网络结构中违背提到的4个指导原则的部分结构进行调整，即关键在于保持较多通道数的情况下，更多使用密集卷积层而不是分组稀疏卷积层。为了达到该目的作者引入了一个新的操作叫做通道划分。如Figure3(a)(b)为ShuffleNet V1的两个构建块，Figure3(C)(d)为作者在ShuffleNet V1的基础上，提出的新的构建块。<br>在Figure3(c)中，作者首先利用通道划分（这里作者将通道划分成数量相等的两部分），将输入通道划分成两部分一部分不做任何操作，一部分则利用3层网络对其进行处理，这样可以减少碎片化程度，这个设计遵循了G3。使用的3层网络的输入和输出通道数也是一样的，这个设计遵循了G1。3层网络中1x1的卷积层不再是分组卷积了，这个设计遵循了G2。在卷积操作之后，两个分支被拼接在一起，使得输入和输出的通道数保持一致，这也遵循了G1。由于这里其实是分成了2个组，在拼接之后作者使用了channel shuffle来使各组信息相关联。这里三个连续的逐元素操作“Concat”，“Channel Shuffle”,“Channel Split”被合并成一个逐元素操作，这也遵循了G4.FIgure3(d)为feature map大小减小1倍而feature map的通道数量增加一倍的版本。这里通道划分被移除了，因此通道数量比原来增加了一倍。Table5为提到的网络结构，ShuffleNet V2。其中不同的输出通道数，给出了不同复杂度的网络。<br><img src="/post/97be11b5/68E57373-BA06-4038-B95B-B24C6C70CAF0.png"></p><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>作者在IamgeNet2012分类数据集上，验证了提到的网络结构的有效性。并对比了4种不同的模型复杂度下，不同网络结构的性能。超参的设置与ShuffleNet V1一致。</p><img src="/post/97be11b5/D913B5AF-DFD6-4412-AE53-206937F0009D.png"><ul><li>Table8给出了在模型复杂度为40，140，300和500+ MFLOPs下各个模型的运行速度和准确率。<strong>从Table8中可以看出在相同复杂度下，ShuffleNet V2要比其他网络结构准确率要提高很多</strong>。作者注意到在复杂度为40MFLOPs，输入大小为224x224的情况下，MobileNet V2的效果很差，这可能是因为通道数太少了，但是ShuffleNet V2则由于使用更多的通道数而不会出现这种情况。在Table8中作者也对比了其他最先进的网络结构如CondenseNet，IGCV2，IGCV3，在不同的复杂度下，提到的网络结构准确率都要比它们高。<img src="/post/97be11b5/0B6C53EF-01D4-4C7B-888F-58F7A1662CE3.png"></li><li>Figure1给出了在相同模型复杂度或者准确率下，不同模型的运行速度。<strong>从Figure1(c)(d)可以看出，在相同模型复杂度下，ShuffleNet V2在运行速度上优势也很明显。 从Figure1(a)(b)中可以看出，如果同时考虑准确率和速度，ShuffleNet V2是最优的</strong>。<img src="/post/97be11b5/0B7D3553-6E3B-453C-B8B5-4ABCE449CA08.png"></li><li>Table6对比了大模型情况下，ShuffleNet V2与其他先进的网络结构。<strong>从Table6可以看出在大模型的情况下，ShuffleNet V2也要比ShuffleNet V1和resnet50更好，并且模型复杂度要比resnet50少40%。ShuffleNet能够在模型复杂度远小于SENet的情况下，准确率超过SENet</strong>。<img src="/post/97be11b5/A4E0F7C4-0D80-40DE-BCAD-17004248E9CF.png"></li><li>Table7对比了在目标检测任务下，相同复杂度不同网络结构的性能。这里使用的目标检测方法是light-head RCNN，并将backbone网络替换为本论文提到的网络。<strong>从Table7可以看出，ShuffleNet V2在目标检测任务上也要由于其它网络结构，作者发现增加感受野能够提高目标检测的效果，为此作者在原本的构建块的第一个1x1卷积层之前加入了一个3x3的逐深度卷积，构建了ShuffleNet V2*网络结构，进一步提高了目标检测的效果</strong>。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ShuffleNet V2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018_[CGAN]CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation_MICCAI2018_Dakai Jin</title>
      <link href="/post/d5dc028e.html"/>
      <url>/post/d5dc028e.html</url>
      
        <content type="html"><![CDATA[<p>该论文发表于MICCAI2018</p><h1 id="一、动机"><a href="#一、动机" class="headerlink" title="一、动机"></a>一、动机</h1><p>   在医疗图像领域，数据难获取，而且类内存在着很大的差异。为了缓解数据问题，作者在这篇文章中调查了通过仿真出来的数据，是否能够改善P-HNN模型在肺结节分割任务上的效果。</p><h1 id="二、主要创新点"><a href="#二、主要创新点" class="headerlink" title="二、主要创新点"></a>二、主要创新点</h1><ol><li>为了生成仿真数据，作者使用了GAN模型，有效地学习肺结节在3D空间中的属性分布。</li><li>为了让仿真的结节能够更好的融入到背景中去，作者使用了真结节的背景信息，即把真结节从图片中抠走。</li><li>最后为了能够进一步让结节看起来更加的真实作者提出了一个新颖的multi-mask重构loss.<a id="more"></a></li></ol><h1 id="三、具体实现"><a href="#三、具体实现" class="headerlink" title="三、具体实现"></a>三、具体实现</h1><h2 id="3-1-3D-CGAN-网络结构"><a href="#3-1-3D-CGAN-网络结构" class="headerlink" title="3.1 3D CGAN 网络结构"></a>3.1 3D CGAN 网络结构</h2><img src="/post/d5dc028e/FD03C352-C78D-49B5-A5C6-A5A7659F125A.png"><ol><li>生成网络的输入是消除掉中心结节但是包含结节背景信息的图像。通过编码和解码过程，生成网络试图恢复消除掉的中心结节。</li><li>判别器的输入包括了扣掉中心结节的图片以及原始图片。</li><li>在生成网络的解码器的前两个卷积层作者加入了dropout.</li><li>作者在网络中利用strided卷积代替了pooling层。 包括生成网络的编码器，以及判别网络。并在strided 卷积后使用了Leaky relu激活函数。</li><li>在生成器的最后一层作者使用了Tanh激活函数</li></ol><h2 id="3-2-模型loss"><a href="#3-2-模型loss" class="headerlink" title="3.2 模型loss"></a>3.2 模型loss</h2><ol><li>CGAN loss<img src="/post/d5dc028e/1FB24B6C-CC1D-4979-8F83-CBA7348660AA.png"></li><li>multi-mask L1 loss<img src="/post/d5dc028e/5435D297-E905-4D08-9852-80300A39C660.png">这里的M表示消除掉的结节位置，N是对M进行膨胀后的结果。<br>这里的multi-mask是指对消除掉的结节位置的重构loss以及对其膨胀后膨胀位置像素的重构loss，分别赋予不同的loss 权重。比较合适的是3到6个。这里作者给前者赋予1.0，后者赋予5.0.</li><li>这个模型的loss由这两部分加权构成<img src="/post/d5dc028e/FE56BEC5-47FD-44B0-8FE2-D43C1D99BD8F.png">loss包括了两部分，一部分是普通的CGAN loss；另一部分是multi-mask loss。这两部分的权重分别为1.0和100.0.</li></ol><h2 id="3-3-相关参数"><a href="#3-3-相关参数" class="headerlink" title="3.3 相关参数"></a>3.3 相关参数</h2><ol><li>作者使用了标准的GAN训练方法，交替优化G和D。</li><li>在训练G的时候，作者通过去最大化logD(x,G(x)),而不是去最小化log(1-D(x,G(x)))。</li><li>作者使用了Adam优化方法，并设置初始学习率为0.0001，对于生成器动量参数设置为0.5，对于判别器动量参数设置为0.999。</li></ol><h2 id="3-4-数据和结果"><a href="#3-4-数据和结果" class="headerlink" title="3.4 数据和结果"></a>3.4 数据和结果</h2><h3 id="3-4-1-作者使用了LIDC数据集对GAN模型进行训练"><a href="#3-4-1-作者使用了LIDC数据集对GAN模型进行训练" class="headerlink" title="3.4.1 作者使用了LIDC数据集对GAN模型进行训练"></a>3.4.1 作者使用了LIDC数据集对GAN模型进行训练</h3><p>1.真实结节从ct图像中crop出来，crop的3个纬度的是结节直径的3到3.5倍大。最后将crop出来的图片缩放到64X64X64。<br>2.消除掉结节的输入图片，由真实结节图片消除掉中心直径为32的球体生成。<br>3.对于结节直接小于5mm的，不用于训练GAN.<br><img src="/post/d5dc028e/7CD4259F-D646-4E52-8EAD-9737CADCB5F3.png"><br>(c) 用全图的L1loss,不用判别网络<br>(d) 用了GAN和全图的L1 loss<br>(e) 用了GAN和中心消除位置的L1 loss<br>(f) 用了GAN和multi-mask L1 loss</p><h3 id="3-4-2-利用训练好的GAN模型生成一些仿真结节，用于fine-tune-P-HNN模型"><a href="#3-4-2-利用训练好的GAN模型生成一些仿真结节，用于fine-tune-P-HNN模型" class="headerlink" title="3.4.2 利用训练好的GAN模型生成一些仿真结节，用于fine-tune P-HNN模型"></a>3.4.2 利用训练好的GAN模型生成一些仿真结节，用于fine-tune P-HNN模型</h3><p>P-HNN模型在之前的实验中对于在肺壁的结节分割效果较差，主要原因是训练数据中缺少肺壁附近的结节。</p><ol><li>作者首先从LIDC数据集中挑选了34张图，了解数据中缺少的外围结节大致是如何的。</li><li>然后从相对健康的CT影像中挑选42个。并从其中随机挑选了30个类似的位置。每个位置距离肺壁距离在8到20mm之间。crop图片的大小为32到80mm之间。</li><li>接着将crop的图片缩放到64X64X64。生成仿真数据之后，再将其缩放回去，然后黏贴回原来的位置。</li><li>最后利用他们来fine-tune P-HNN模型。</li></ol><p>最后的结果显示，生成的结节确实可以提高分割效果。<br><img src="/post/d5dc028e/801E5093-B636-4793-B608-631A0D54D3E9.png"><br><img src="/post/d5dc028e/F57C46F9-E61A-47FD-A042-23872B64421B.png"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 医学图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> lung nodule </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018_An Analysis of Scale Invariance in Object Detection - SNIP_CVPR2018(oral)_SinghB_DavisL</title>
      <link href="/post/b4229735.html"/>
      <url>/post/b4229735.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>当前在ImageNet1000类分类任务上top5误差已经降低到2%；但是在COCO目标检测任务上，在overlap为50%的情况下mAP也只有60%。这个巨大的差距主要是由于COCO目标检测数据集中目标的尺度范围很大，并且存在大量的小目标。Fig1给出了COCO和ImageNet数据集中ROI区域相对于图像的比例，在整个数据集中的占比情况。从Fig1可以看出在COCO数据集中目标的尺度跨度很大，在COCO数据集中，有10%的ROI占图像的比例小于0.024，有90%的ROI占图像的比例小于0.472，而这中间目标尺度的跨度接近20倍，目标检测器想要对这么大的尺度范围的目标都有效是很困难的，特别是小目标；从Fig1中也可以看出ImageNet数据集和COCO数据集目标尺度的差异是很大的，这也会降低利用Imagenet进行预训练的效果。为了验证是否upsample能够更好的处理小目标及为了缓解尺度范围跨度较大的问题该如何选择不同的分辨率，不同的尺度范围来训练，作者进行了两个探究性试验。基于对这两个探究性实验的分析作者得出的结论是训练检测器的时候最好是使用特定的尺度范围内的目标来训练，但是要尽可能多的保留所有的目标外形和姿势等信息。为此作者提出了用于图像金字塔的尺度标准化训练框架(SNIP)。该框架使用多种分辨率的图像进行训练，并对于各个图像金字塔层只使用该金字塔层指定尺寸范围内的目标进行训练，忽略其它目标；在推断的时候也使用多种分辨率的图像进行测试，每种分辨率只保留指定尺度范围内的目标，然后将这些目标缩放到同个尺度，再利用soft-nms进行过滤。使用该方法，作者在单模型的情况下，在COCO目标检测任务上mAP达到了45.7%，在组合3个模型的情况下mAP达到了48.3%。并赢得了COCO2017的挑战赛。<br><a href="http://bit.ly/2yXVg4c" target="_blank" rel="noopener">代码：http://bit.ly/2yXVg4c</a><br><img src="/post/b4229735/BF8A9D5D-85C2-45F6-88D6-92750296BA56.png"></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-这里简单介绍通常处理尺度变化问题的一些解决思路"><a href="#2-1-这里简单介绍通常处理尺度变化问题的一些解决思路" class="headerlink" title="2.1 这里简单介绍通常处理尺度变化问题的一些解决思路"></a>2.1 这里简单介绍通常处理尺度变化问题的一些解决思路</h2><ol><li>通过dilated convolutions来增加特征图的分辨率的同时维持预训练网络的感受野，从而不会影响到大目标的检测效果</li><li>训练的时候对图片进行1.5倍或者2倍的upsample，测试的时候对图片进行2倍的upsample。</li><li>使用多个不同分辨率的卷积层来独立预测目标，如SDP，SSH，MS-CNN。</li><li>通过组合浅层和深层卷积层来预测目标，如FPN，Mask-RCNN，RetinaNet。</li></ol><h2 id="2-2-本文的贡献及创新点"><a href="#2-2-本文的贡献及创新点" class="headerlink" title="2.2 本文的贡献及创新点"></a>2.2 本文的贡献及创新点</h2><ol><li>通过第一个探究性的实验，揭露了在进行小目标分类的时候，将小目标upsample到预训练网络使用的训练图片尺度来finetune，效果要优于直接使用小尺度的图片来从头开始训练，所以对小目标进行upsample是很重要的。</li><li>通过第二个探究性的实验，揭露了特定尺度范围的目标检测器虽然可以解决训练和测试的时候尺度的差异带来的负面影响，但是分类器在训练的时候训练的目标大大减少，也会使性能降低；用所有尺度训练的目标检测器虽然训练的目标很多，但是目标跨尺度很大，也给目标检测器的训练带来了难度。</li><li>基于对两个探究性实验的分析作者得出的结论是训练检测器的时候最好是使用特定的尺度范围内的目标来训练，但是要尽可能多的保留所有的目标外形和姿势等信息。为此作者提出SNIP框架来实现这个目标。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-探究性实验为存在的一些问题寻求答案"><a href="#3-1-探究性实验为存在的一些问题寻求答案" class="headerlink" title="3.1 探究性实验为存在的一些问题寻求答案"></a>3.1 探究性实验为存在的一些问题寻求答案</h2><h3 id="3-1-1-两个存在的问题"><a href="#3-1-1-两个存在的问题" class="headerlink" title="3.1.1 两个存在的问题"></a>3.1.1 两个存在的问题</h3><ol><li>为了让目标检测器获得更好的性能，上采样图片是不是关键的呢？</li><li>在利用预训练的图像分类模型来进行finetune的时候，使用的目标实例的分辨率应该rescale到跟预训练的模型一致呢，还是只需要对图片进行上采样然后使用所有的目标来训练呢？</li></ol><h3 id="3-1-2-两个探究性实验"><a href="#3-1-2-两个探究性实验" class="headerlink" title="3.1.2 两个探究性实验"></a>3.1.2 两个探究性实验</h3><ol><li>通过改变输入图像的尺度，来探究训练和测试的时候尺度差异对分类模型的影响以及为了检测小目标对图像进行上采样好还是直接利用低分辨率来训练更好。<br>因为目标检测在训练的时候一般会把图像缩放到800 $\times$ 1200,测试的时候为了检测小目标则是把图像缩放到1400 $\times$ 1200，为此作者通过这个实验来探究这种分辨率的差异对性能有什么影响以及上采样对检测小目标是否更好。<img src="/post/b4229735/2A3400FB-E0E7-49DA-B935-E375B5791501.png"><img src="/post/b4229735/66EA5E47-51AD-4ABA-AB64-294C2A351FC9.png">作者设置了3个实验：</li></ol><ul><li>1）多尺度推理：如Fig3 CNN-B所示，该实验通过将原本ImageNet的图像缩放到 48 $\times$ 48，64 $\times$ 64，80 $\times$ 80，96 $\times$ 96，128 $\times$ 128，然后再将图片缩放到224 $\times$ 224，来对使用高分辨率训练的分类器进行多种尺度的测试。Fig4给出了CNN-B top-1的准确率。作者发现随着测试分辨率和训练分辨率差异的增加，性能会不断下降。因此训练和测试图片分辨率的差异确实不是最佳的选择，至少对于图像分类任务是这样。</li><li>2）使用小目标图像来训练和推理：由于训练和测试图像尺度的差异会影响性能，那么一个比较直接的方式，就是让训练和测试的时候图片的分辨率保持一致。如Fig3 CNN-S所示，作者通过修改ResNet-101网络结构来达到这个目的。作者给出了两种不同尺度下的效果，48 $\times$ 48和96 $\times$ 96。从Fig4可以看出，CNN-S的效果要远远优于CNN-B。因此为了让小目标得到更好的性能，测试和训练的时候尺度保持一致要更好。</li><li>3）在基于所有尺度训练的分类器思的基础上利用小目标图像来进行finetune：如Fig3 CNN-B-FT所示，作者将小目标图像缩放到预训练网络使用的训练目标的尺度，然后利用缩放后的图片来对预训练的网络进行finetune。如Fig4所示，通过对小目标进行上采样来finetune高分辨率的预训练网络得到的效果要优于直接使用低分辨率来训练和测试。</li></ul><p><strong>结论</strong>：作者通过第一个探究性试验揭露了通过对小目标进行上采样来finetune所有尺度范围目标训练的的预训练网络得到的效果要优于直接使用小目标图像来训练和测试，这也说明预训练对效果的提升有很大的帮助。</p><ol><li>通过测试使用不同尺度范围目标训练得到的目标检测器的性能，来探究在训练目标检测器的时候，为了让小目标的检出效果更好该如何选定目标的尺度。</li></ol><p>从第一个探究性实验可以看出，训练和测试的图像分辨率的差异会降低性能。而目前的目标检测方法由于受到GPU内存的限制，训练的时候使用了800 $\times$ 1200大小的图片，而测试的时候使用了1400 $\times$ 2000的图像，显然不符合最优的设置。那训练的时候使用什么样的分辨率会对小目标的检测最好呢？为了探究这个问题，作者使用了多种不同的设置来训练目标检测器，而使用分辨率为1400 $\times$ 2000的图片进行测试，来看看对小目标的效果如何(这里的小目标主要是像素小于32 $\times$ 32的目标)。由于Deformable-RFCN是COCO数据上最优的目标检测器，并且也开发了代码，所以在这里作者使用它来做相关的实验。在实验中proposal是利用了公共可获取的Deformable-RFCN检测器在800 $\times$ 1200分辨率下提取的。Deformable-RFCN检测器的RPN网络是使用ResNet101作为backbone，并在分辨率为800 $\times$ 1200的图像上，使用5种anchor尺度训练得到的。proposal的分类，作者使用了以ResNet50作为backbone的Deformable-RFCN，但是删除了Deformable RoIpooling层，改用双线性差值后的RoIPooling，这样做的目的在于减少最后一层滤波器的数量；最后作者使用阈值为0.3的NMS过滤掉冗余的目标。在训练的时候，作者并没有让两部分网络进行联合训练，而是只使用RPN网络的结果来训练ResNet-50，主要是为了减少训练时间和节省GPU内存。<br><img src="/post/b4229735/3E23E793-FE11-4DB6-8394-686FE6444D70.png"></p><p>作者在这里做了如下3个实验：</p><ul><li>1）使用不同的分辨率进行训练：在这个实验里，作者使用了两种不同的分辨率训练了2个检测器，一个使用了800 $\times$ 1400的图片，一个使用了1400 $\times$ 2000的图片，在这里分别记为$800_all$和$1400_all$。从Table1中可以看出，如所期望的一样，使用1400 $\times$ 2000的图片训练的检测器要更优。但是提高的性能是很小的。这主要是因为，在分辨率为1400 $\times$ 2000的情况下，大尺度的目标由于太大很难去分类，使得最终对于小目标的效果并没有那么明显。</li><li>2）特定尺度范围的检出器：为了消除过大目标的影响，作者在该实验里使用了1400 $\times$ 2000的图片，但是忽略了像素大于80 $\times$ 80的目标。从Table1中可以看出，最后的结果要比$800_all$差很多，这主要是忽略的中等和大的目标使得很多姿势和外形信息被丢弃了(大概30%的目标实例被丢弃)，从而影响到最终检出器的性能。</li><li>3）多尺度的训练(MST)：在这个实验中作者评估了在训练目标检测器时经常用到的多尺度的训练方式，该方式使用在多种分辨率中随机选择的图片来训练目标检测器，从而得到尺度不变的目标检测器。该训练方式虽然使用了多种不同的分辨率，但是同时也受到了极度大和极度小的目标的影响，使得最终的性能与$800_all$类似。</li></ul><p><strong>结论</strong>：作者通过第二个探究性的试验揭露了特定尺度范围的目标检测器虽然可以解决训练和测试的时候尺度的差异带来的负面影响，但是分类器在训练的时候训练的目标大大减少，也会使性能降低；用所有尺度训练的目标检测器虽然训练的目标很多，但是目标跨尺度很大，也给目标检测器的训练带来了难度。最后作者得出的结论是，训练检测器的时候最好是使用特定的尺度范围内的目标来训练，但是要尽可能多的保留所有的目标外形和姿势等信息。作者提到的训练框架也正是同时满足了这两点，并使得性能得到很大的提高。</p><h2 id="3-2-提到的训练框架"><a href="#3-2-提到的训练框架" class="headerlink" title="3.2 提到的训练框架"></a>3.2 提到的训练框架</h2><p>通过上面两个探究性的实验，作者提出了一个新颖的训练框架，用于图像金字塔的尺度标准化方法(SNIP)，该方法可以消除在使用分类网络进行finetune的时候，两个任务训练目标的尺度偏差，也可以在使用特定尺度范围内的目标来进行训练的时候，不会减少训练样本的数量。这里尺度的偏差通过图像金字塔的方式来解决而不是一个特定范围的目标检测器。通过图像金字塔的形式将目标尺度标准化到某个范围内，然后在训练的时候只使用与预训练的CNN的训练目标尺度在同个范围内的目标进行训练。通过这种方式可以有效的利用所有目标，该方法也可以应用到不同的任务上，如实例分割，姿势估计，时空活动识别等等，识别的目标的尺度有很大变化的各种问题上。作者通过该方式，将目标检测的性能提高了3.5%，与Deformable-RFCN集成更是将IOU50%的mAP提高7.4%。通过该方法也证明了，比起提供更多的各种尺度的数据来处理尺度跨越较大的问题，CNN更加能够处理特定尺度范围内的识别问题。</p><h3 id="3-2-1-SNIP"><a href="#3-2-1-SNIP" class="headerlink" title="3.2.1 SNIP"></a>3.2.1 SNIP</h3><p>SNIP是多尺度训练的改进版本，它只使用了与预训练的模型尺度一致的目标，典型的是224$\times$224。在图像金字塔中，在分辨率较高的金字塔层过大的目标很难训练，在分辨率较低的金字塔层过小的目标很难训练，但是这些过小过大的目标在某个金字塔层的尺度是相对合适的。所以SNIP主要是利用图像金字塔中落到指定范围内的目标来训练，而忽略不在该范围内的目标。由于落在图像金字塔中的同个目标，总有一个的尺度在指定的范围内，所以训练的时候不会损失外形和姿势信息，同时也减少了预训练模型训练图像中的目标尺度与目标检测任务中图像中的目标尺度的差异。从Table1中可以看出，SNIP要比其他训练方式要好很多。<br><img src="/post/b4229735/A5D91001-E996-4290-A780-1065E8437E1F.png"><br>Fig6给出了SNIP的整个框架。该方法在训练proposals分类器的时候，抛弃了在特定的分辨率下不在指定尺度范围内的proposal和grounth box。这里在指定范围内的被视为有效，不在指定范围内被视为无效。一般对于分辨率较高的图片，指定尺度范围为原分辨率图片中尺度较小的图片，而对于分辨率较低的图片，指定尺度范围为原分辨率图片中尺度较大的图片。类似的，在训练RPN网络的时候，作者使用了所有的grounth box，但是当anchor与无效的grounth box的IOU大于0.3的时候，该anchor被视为无效，从而不会用于训练。在推理的时候，对于不同分辨率的图像，首先使用RPN网络提取proposals；然后使用proposals分类器对目标进行精细分类和定位，并抛弃不在指定范围内的目标。最后将不同分辨率下的目标统一到统一的尺度下并使用soft-NMS过滤掉冗余的目标，得到最终的检出结果。</p><h3 id="3-2-2-对高分辨率图像进行子图采样"><a href="#3-2-2-对高分辨率图像进行子图采样" class="headerlink" title="3.2.2 对高分辨率图像进行子图采样"></a>3.2.2 对高分辨率图像进行子图采样</h3><p>由于在使用较为复杂的网络结构对高分辨率的图像进行训练的时候需要很大的显存，为了缓解显存消耗过大的问题，作者对高分辨率的图像进行随机裁剪，并选取包含目标最多那个图像用于训练。作者只对分辨率为1400 $\times$ 2000的金字塔层执行该操作，从中随机裁剪50个1000 $\times$ 1000的子图，然后选择包含最多目标的图像用于训练，对于不包含有效尺度范围的图片则不需要执行该操作，因为里面的所有目标都不用于训练。对于没有被采集到的有效目标，重复执行该操作直到所有有效的目标都被采集到。最后大约每张图片生成1.7张子图。这种方式只是为了在显存受限的情况下，能够支持复杂网络的训练，而不是为了获得性能的提高。作者使用resnet-50验证了使用子图和原图的效果，发现性能并没有变化。</p><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><p>作者在COCO数据集上验证了提到的方法的有效性。COCO数据集包含了123000张训练图片和20288张测试图片。由于测试图片的grounth truth并没有公开，所以为了更加快速地进行实验，作者只使用训练集中的118000张图片进行训练，然后使用剩下的5000张图片(minival-set)来测试。本文中作者提到的小目标是指像素小于32 $\times$ 32的目标，中等目标是指像素范围在32 $\times$ 32到96 $\times$ 96的目标，大目标是指像素大于96 $\times$ 96的目标。</p><h2 id="4-1-训练细节"><a href="#4-1-训练细节" class="headerlink" title="4.1 训练细节"></a>4.1 训练细节</h2><ul><li>作者使用Deformable-RFCN作为实验的检出器，并使用3个图像金字塔层，分辨率分别为(480,800)、(800,1200)、(1400,2000)。这里的尺度大小是由数据集决定的，480是最短的短边，2000是最长的长边。</li><li>作者在实验的时候并没有让RPN和RCN网络联合训练，为了能够更快地去尝试不同的分类网络结构，作者将他们分开训练。</li><li>分类器作者训练7个epcoh，RPN网络则训练6个epoch。</li><li>作者在前2000次迭代先使用学习率为0.0005进行warmup，然后再调整学习为0.005，并在分类网络和RPN网络分别训练到5.33个epoch和4.33个epoch的时候将学习率减半。</li><li>对于分类器，作者在图像分辨率为(1400,2000)的时候，将在原本图像分辨率下尺度在[0,80]范围内的目标认为是有效的目标；在图像分辨率为(800,1200)的时候，将在原本图像分辨率下尺度在[40,160]范围内的目标认为是有效的目标；在图像分辨率为(480,800)的时候，将在原本图像分辨率下尺度在[120,无穷大]范围内的目标认为是有效的目标。这些尺度范围主要是基于预训练模型的训练目标尺度来决定的，通过合适的选择可以消除预训练模型和finetune所使用的目标尺度的差异。</li><li>对于RPN网络，作者在分辨率为(800,1200)的时候，将在原本图像分辨率下尺度在[0,160]范围内的目标认为是有效的目标。其它分辨率的有效范围同分类器。</li><li>推断的时候，对于分类器，不同分辨率下的有效范围由minival set决定。</li><li>由于训练RPN网络的时候比较快，作者在第2个epoch的时候开始使用SNIP；而对于分类网络，使用SNIP后每个epoch会增加一倍的训练时间，所以作者在第4个epoch开始使用SNIP。</li></ul><h2 id="4-2-SNIP对RPN网络的改善"><a href="#4-2-SNIP对RPN网络的改善" class="headerlink" title="4.2 SNIP对RPN网络的改善"></a>4.2 SNIP对RPN网络的改善</h2><p>原本的RPN网络分配正样本的方式是，要么grouth truth与anchor的IOU大于0.7，要么grouth truth与anchor具有最大的IOU。作者发现在COCO数据集中，当使用图像分辨率为800 $\times$ 1200并使用15个anchors(5 scales - 32,64,128,256,512,stride16,3 aspect ratios)的时候，只有30%的grouth truth能够与anchor的IOU大于0.7。在将阈值降低到0.5的时候，也只有58%的grouth truth满足条件，因此有超过40%的grouth truth与anchor的IOU小于0.5被分配为正样本或者被忽略。SNIP由于将所有的目标rescale到一定的尺度范围，使得这个问题得到了一定的改善。另外作者也尝试使用更强的特征以及7个anchor尺度来获得更好的性能，该方式为下文提到的Improved PRN。</p><h2 id="4-3-实验结果"><a href="#4-3-实验结果" class="headerlink" title="4.3 实验结果"></a>4.3 实验结果</h2><h3 id="4-3-1-SNIP在分类网络上的效果"><a href="#4-3-1-SNIP在分类网络上的效果" class="headerlink" title="4.3.1 SNIP在分类网络上的效果"></a>4.3.1 SNIP在分类网络上的效果</h3><img src="/post/b4229735/62C0A778-A844-4F32-8025-74C5790FEC0D.png"><p>为了验证SNIP应用到分类网络上的效果，作者首先按照第二个探究性实验的网络设置方式，比较了单尺度(800 $\times$ 1200)训练单尺度测试、单尺度训练多尺度测试、多尺度训练多尺度测试及SNIP在COCO数据集上的检出性能。从Table2中可以看出，单尺度训练多尺度测试要比单尺度训练单尺度测试的mAP高出1.4%；多尺度训练多尺度测试相较于单尺度训练多尺度测试并没有提高反而略微下降，这是因为在高分辨率的情况下，网络的感受野不够大，无法去区分大目标；SNIP要比单尺度训练多尺度测试高出1.9%，并且作者只使用了单尺度下的proposals结果，然后将该尺度下的结果映射到其它多个尺度来进行分类。</p><h3 id="4-3-2-SNIP在RPN网络上的效果"><a href="#4-3-2-SNIP在RPN网络上的效果" class="headerlink" title="4.3.2 SNIP在RPN网络上的效果"></a>4.3.2 SNIP在RPN网络上的效果</h3><img src="/post/b4229735/49709B2F-3DD4-4ACD-971C-8FD6775508C8.png"><p>为了验证SNIP应用到RPN网络上的效果，作者使用ResNet-50的conv4特征图来训练RPN网络，并在3种分辨率下进行测试。在每种分辨率下，作者选择top300个proposals来计算平均召回率，3种分辨率共900个proposals。从table3可以看出，Improved版本(Improved版本是指使用更强的特征和更多的anchor尺度(7vs5))要比baseline的平均召回要好，这里大部分的提高来自于大尺度(&gt;100像素)目标召回的提高。但是对于IOU为0.5的平均召回率提高并不明显，IOU为0.5的目标可以通过下一步的分类来得到更加精确的分类和定位，所以IOU为0.5的平均召回率这个指标对两阶段的目标检测更加重要。相对于Improved版本，SNIP将IOU为0.5的平均召回提高了2.9%，小目标的平均召回更是提高6.3%。在使用更强的网络结构DPN-92的情况下，平均召回更是得到了进一步的提高。</p><h3 id="4-3-3-与最先进目标检测器的比较"><a href="#4-3-3-与最先进目标检测器的比较" class="headerlink" title="4.3.3 与最先进目标检测器的比较"></a>4.3.3 与最先进目标检测器的比较</h3><img src="/post/b4229735/AE8B0F1C-1322-4E34-8907-1A3D2195AB20.png"><p>Table4给出了在结合SNIP之后，最先进的目标检测方法的效果。从Table4可以看出在最先进的目标检测方法的基础上结合SNIP，整体的mAP得到了很大的提升。相较于以ResNet-101作为backbone，在单尺度下训练的D-RFCN，该方式为baseline(3尺度测试)，在使用SNIP进行训练和测试后，平均AP提高5%，小目标的AP提高8.7%。相较于使用6种尺度测试的baseline，平均AP提高2.5%，小目标的AP提高3.9%。在使用DPN-98作为backbone的情况下性能得到了更进一步的提高。在模型融合的时候，作者使用了backbone为DPN-92的RPN网络的候选结果，并将同个候选目标的预测结果进行求和取平均来得到最终的预测结果。D-RFCN+SNIP(backbone为DPN-98 with flip)这里则是对翻转和没翻转的结果进行求和取平均，来得到预测结果；最后作者使用soft-NMS过滤掉冗余的目标。所有的预训练模型都是在ImageNet-1000分类任务上进行预训练的，模型融合的时候没有用到Faster RCNN。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 尺度问题 </tag>
            
            <tag> SNIP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Deformable Convolutional Networks_ICCV2017(oral)_DaiJ et al</title>
      <link href="/post/992cacb1.html"/>
      <url>/post/992cacb1.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>视觉识别的一个关键的挑战是如何让模型去适应目标的几何变换，如尺度，姿势，视角以及局部的形变，从而让模型对这些信息不敏感。一般为了达到这个目的，有两种方式。一种是构建一个包含目标的各种变换的数据集，从而让模型从这些变换中学习到更加鲁棒的特征，这个通常通过数据增强来实现，但是这种方式通常会增加训练时间；第二种方式是使用对形变不敏感的特征，比如SIFT,然后在目标检测任务中会利用滑动窗口去检测目标。然而上面的这两种方式存在两个缺点，一个是不管是对数据进行增强，还是使用人工设计的特征，增强的方式以及设计特征的方式都考虑了有可能的几何变换形式，当这些方法应用到其它包含其它几何变换的任务的时候，不一定有效；另一个是，人工设计的对几何变换不敏感的特征，不可能十全十美。CNNs在视觉识别任务中取得了很大的成功，如图像分类、语义分割、目标检测等。但是CNNs仍然存在上面说的两个缺点，CNNs模型仍然是通过数据增强，复杂模型的强大表达能力，以及人工设计好的模块来建模目标的几何变换信息，maxpooling只能去适应小的几何变换。CNNs本身的结构限制了模型去适应那些大的未知的几何变换。这个主要是由于CNNs模块的几何结构是固定的：卷积单元每次都按照固定的位置去采样输入特征图；池化层每次都按照同等的比例去减少空间分辨率；RoI池化层每次都将一个RoI区域分离成固定大小的多个bin。这些固定的结构让模型缺少处理几何变换的内部机制。由于卷积神经网络的构建块的几何结构固定，限制了模型去建模目标几何信息变换的能力。而对于语义分割和目标检测任务，让模型去适应几何变换对性能的提高是很关键的。比如深层的语义特征会编码目标的尺度，形变等信息，但是由于每个位置的感受野都是固定的，无法根据目标类型去调整感受野，从而无法学习到更加精确的信息，这个对需要精细化定位的任务影响是比较大的，比如语义分割；另外对于目标检测，现在仍然是按照固定的方式从bbox提取预定义大小的多个bin，这对于非刚性目标来说，显然无法从精确的位置提取精确的特征。为了应对这个问题，作者提出了两个新的构建块，用于增强卷积神经网络学习几何变换的能力，这两个构建块分别为可变形的卷积层和可以变形的RoI池化层。这两个构建块都源自于同个想法，就是通过从任务中学习每个采样位置的偏置，这里不需要增加额外的监督信息，并利用该偏置重新调整采样的位置，使得采样位置从固定变成不固定，这也是为啥叫可变性卷积和可变性RoI池化的原因。由于新加的结构不会对原本的模型结构带来影响，所以可以对任意一个普通的卷积层或者RoI pooling层进行增强，并且可以进行端到端的训练，作者将使用提到的增强结构的网络，称为可变形的卷积神经网络。最后作者在语义分割和目标检测任务上验证了提到的方法的有效性。</p><img src="/post/992cacb1/B41FE037-A813-4CC1-BD83-FD3572113411.png"><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ol><li>为了解决CNNs模型结构固定而无法适应目标较大的几何变换的问题，作者对卷积层和ROI池化层进行增强，提出了可变形卷积和可变形的ROI池化层。</li><li>可变形卷积和可变形的ROI池化层，主要是通过学习各个采样位置的偏置信息，来让卷积层和ROI池化层的采样位置从原本固定的方式，变成自适应目标几何信息的形式，从而极大的提高对精细化的特征比较依赖的任务，如目标检测和语义分割。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-Deformable-Convolution"><a href="#3-1-Deformable-Convolution" class="headerlink" title="3.1 Deformable Convolution"></a>3.1 Deformable Convolution</h2><img src="/post/992cacb1/ABD1E492-507F-44D9-B6DB-D22E9B6D7DFF.png"><p>Deformable Convolution其实就是通过学习卷积核在输入特征图上各个采样位置的偏置，然后通过将学习到的偏置与原本采样位置相加来改变原本的采样位置，让采样位置可以适应目标的几何变换。学习到的每个输入位置的偏置一般都是浮点数类型，这样得到的新位置也是浮点数类型，得到新的采样位置一般没办法与输入特征图上的特征点相对应，所以这里作者采用双线性差值来得到新位置的特征点。如Fig2所示，偏置的获取主要是通过一个放置在输入特征图上的卷积层来得到，学习偏置的卷积层的卷积核与应用偏置的卷积层的卷积核的大小和dilation rate相同。学习偏置的卷积层输出的feature map的分辨率与输入的feature map一致，只是channel数扩大了一倍，因为每个位置对应x和y两个方向。不管是学习偏置的卷积层还是应用偏置的卷积层，都是通过将梯度反向传播到双线性差值来同时学习的。</p><h2 id="3-2-Deformable-ROI-Pooling"><a href="#3-2-Deformable-ROI-Pooling" class="headerlink" title="3.2 Deformable ROI Pooling"></a>3.2 Deformable ROI Pooling</h2><p>这里作者给出了普通ROI Pooling和PSRoI Pooling的Deformable形式。不管是哪种信息，都是可用通过反向传播进行端到端的训练。</p><h3 id="3-2-1-RoI-Pooling的Deformable形式"><a href="#3-2-1-RoI-Pooling的Deformable形式" class="headerlink" title="3.2.1 RoI Pooling的Deformable形式"></a>3.2.1 RoI Pooling的Deformable形式</h3><img src="/post/992cacb1/1D2105B6-4D48-4C31-A427-B3BDEA3EB88D.png"><p>类似于Deformable Convolution，Deformable RoI Pooling也是学习各个输入位置的偏置，但是与其不同的是它学习的是映射到每个bin上的各个特征块的偏置。由于偏置是浮点数，新的特征块的获取也是通过双线性插值来得到。如Fig3所示，标准化后的偏置信息是通过在原本RoI Pooling的feature map上应用一个fc层来得到的。在得到标准化后的偏置信息之后再让每个偏置的x，y方向分别乘以ROI的宽和高，然后再乘以一个预定义的尺度因子来得到最终的特征块的偏置，作者将尺度因子设置为0.1。这里通过学习标准化后的偏置信息而不是原始偏置信息是很有必要的，这样可以消除RoI大小的影响，使得学习更加容易。</p><h3 id="3-2-2-PsRoI-Pooling的Deformable形式"><a href="#3-2-2-PsRoI-Pooling的Deformable形式" class="headerlink" title="3.2.2 PsRoI Pooling的Deformable形式"></a>3.2.2 PsRoI Pooling的Deformable形式</h3><img src="/post/992cacb1/7D1D6BB1-3D9B-4181-B663-B38CEC7CA68E.png"><p>这里Deformable PsRoI Pooling与Deformable RoI Pooling是类似的，只是Deformable PsRoI Pooling的偏置是通过与PsRoI Pooling一致的全卷积形式学习到的。如Fig4所示，标准化后的偏置信息是通过与原本一致的位置敏感得分图来得到的，只是由于偏置有两个方向，为此这里的channel数是原本的2倍；并且每个位置的标准化的偏置信息来自于与之对应的score map。最后在得到标准化后的偏置信息之后，再按照Deformable RoI Pooling的方式，得到最终的偏置信息。</p><h2 id="3-3-Deformable-ConvNet"><a href="#3-3-Deformable-ConvNet" class="headerlink" title="3.3 Deformable ConvNet"></a>3.3 Deformable ConvNet</h2><p>这里不管是Deformable Convlution还是Deformable RoI Pooling都是在原本的基础上增加卷积层来学习偏置信息，没有改变原本的结构，输入输出也是与原来的一致，所以可以将任意的普通卷积层和RoI Pooling层替换为Deformable形式。训练的时候学习偏置信息的卷积层和fc层的相关权重都被初始化为0，学习率被设置为原本学习率的$\beta$倍，默认是1，在faster R-CNN框架下fc层则是被设置为0.01。替换了Deformable Convolution和Deformable RoIPooling的CNNs这里称为Deformable ConvNet。<br>为了将Deformable ConvNet应用到最先进的网络结构中，并让其适用不同的任务。作者对要使用的最先进的网络结构做了一些调整。作者发现不管是什么任务，都可以将整个网络划分成两个阶段，第一个阶段是一个层数很深的全卷积网络，用于生成特征；第二个阶段则是一个浅层的任务指向的网络，用于生成结果。为此作者分别就这两个阶段对网络进行调整。<br><strong>在第一阶段中</strong>作者使用两个最先进的网络结构用于特征提取，ResNet-101和一个修改版本的Inception-ResNet(因为之前Inception-ResNet用于图像分类，存在特征不对齐的问题，所以作者根据Aligned-Inception-Resnet进行更改)。这两个网络首先在ImageNet分类任务上进行预训练，然后后作者将最后的average Pooling层和fc层移除掉；最后加入一个随机初始化的1x1卷积层，将最后的卷积层压缩到1024个channel。为了增加最后卷积层的分辨率，作者将Conv5的第一个卷积层的stride从2变成1，从而将整体的stride从32变成16，为了补偿感受野的变小，Conv5中所有卷积核大于1x1的卷积层的dilation rate全部重1变成2。<strong>这里作者将deformable Convolution应用到最后几个卷积核大于1x1的卷积层上。</strong><br><strong>在第二阶段中</strong>语义分割作者采用DeepLab语义分割方法，目标检测作者采用了Category-Aware RPN，Faster R-CNN和R-FCN目标检测方法。</p><ul><li>为了应用DeepLab语义分割方法，作者在特征提取网络输出的feature map上加了一个1x1的卷积层，生成一个(C+1)的feature map，表示每个像素属于不同类别的得分，最后应用一个softmax层得到每个像素属于不同类别的概率。</li><li>Category-Aware RPN是与RPN类似的方法，与RPN网络的区别主要是最后输出的不是2分类，而是(C+1)个类别。</li><li>为了应用Faster R-CNN方法，作者将RPN网络加在conv4的顶端，而RoI Pooling层则是被加在了最后输出的feature map上，然后再接两个fc层，最后再bounding box回归器和分类器。</li><li>为了应用R-FCN方法，作者按照原先的实现方式，在最后生成一个位置敏感得分图。</li></ul><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><p>这里作者测试了提到的方法在语义分割任务和目标检测任务上的效果。语义分割任务主要使用了PASCAL VOC数据集和CityScapes数据集，并使用mIoU来评估分割的效果。目标检测任务主要使用了PASCAL VOC以及COCO数据集，并使用mAP来评估检出的效果。对于PASCAL VOC给出的是IOU阈值大于0.5和IOU阈值大于0.7的mAP，对于COCO则是给出IOU阈值大于0.5小于0.95的mAP。</p><h2 id="4-1-探究提到的两个模块对语义分割和目标检测的性能的影响"><a href="#4-1-探究提到的两个模块对语义分割和目标检测的性能的影响" class="headerlink" title="4.1 探究提到的两个模块对语义分割和目标检测的性能的影响"></a>4.1 探究提到的两个模块对语义分割和目标检测的性能的影响</h2><img src="/post/992cacb1/7ECBF28B-8324-4765-AF37-FD74E932A1D3.png"><p>Table1给出了将deformable Convolution应用在最后的1、2、3和6个卷积核大于1x1的卷积层上的效果。从Table1中可以看出，随着deformable convolution的数量不断提高，性能稳步提升。对于DeepLab来说当使用3个deformable Convolution的时候性能就饱和了，而对于其它任务使用6个则效果更好。在后面的实验作者都使用了3个deformable Convolution。<br><img src="/post/992cacb1/CFFE5ED4-B2B8-4304-85B9-D62154F3B412.png"><br>Table2给出了将使用3个Deformable Convolution的R-FCN网络应用到VOC2007测试集上，并将Deformable Convolution的filters根据ground truth bounding box的标注以及filters中心所在的位置分成4个类别，small、medium、large和background，给出了每个类别下filters的dilation的均值和方差。从Table2中可以看出：1）deformable convolution的filters对应的感受野大小与目标的大小是相关的。deformable convolution可以很好适应目标的几何变换；2）背景的dilation大小是介于medium和large目标的中间的，说明为了更好的识别背景，应该使用一个相对大的感受野。这个观察在不同层上的表现是一致的。<br><img src="/post/992cacb1/F08E20A3-61FC-43B2-9D4E-718909E90FB1.png"><br>Table3给出了对ResNet-101的最后3个3x3的卷积层使用不同dilation rate的效果，以及在dilation rate为2的情况下使用deformable convolution和deformable RoI pooling的效果。从Table3可以看出：1）随着dilation rate的增加，各个任务的性能不断提高，这也说明默认网络的有效感受野比较小；2）对于不同的任务来说，最优的dilation rate是不同的，对于DeepLab来说其最优的dilation rate是6，而对于Faster R-CNN则是3；3）使用deformable convolution效果是最好的，这说明让filter自适应去学习偏置是有效而且必要的；4）单独使用deformable RoI Pooling也带来了很大的提升，特别是mAP@0.7，当结合deformable Convolution的时候，性能得到了更进一步的提高。<br><img src="/post/992cacb1/70FED89A-583B-4ABA-B852-4D8505CAB2D7.png"><br>Table4给出了各个模型的deformable版本和原来版本的模型复杂度和运行时间。从Table4可以看出，deformable ConvNets相较于原来的版本只是增加了极少了模型参数和极少的计算量，这说明性能的提高不是因为模型复杂度的增加，而是来自于模型建模几何变换的能力的增加。<br><img src="/post/992cacb1/91866B7D-8C82-458F-920D-57F56F922252.png"><br><img src="/post/992cacb1/60AF8A22-9BF1-4F19-BC73-B41A7F2326DC.png"><br><img src="/post/992cacb1/137F399D-E0C8-4B84-B259-4FE05DD4077A.png"></p><p>Fig5、Fig6、Fig7给出了应用Deformable Convolution和Deformable RoI Pooling后的一个直观效果。从Fig5、Fig6、Fig7可以看出，Deformable Convolution和Deformable RoI Pooling可以更好地适应不同目标的大小和形变，从而去关注更加有效的信息。</p><h2 id="4-2-deformable-ConvNet在COCO目标检测任务的效果"><a href="#4-2-deformable-ConvNet在COCO目标检测任务的效果" class="headerlink" title="4.2 deformable ConvNet在COCO目标检测任务的效果"></a>4.2 deformable ConvNet在COCO目标检测任务的效果</h2><img src="/post/992cacb1/311A595F-E461-4977-A9D2-A3C5244D6A2E.png"><p>Table5对比了deformable ConvNet与原来版本在COCO目标检测任务上的效果。从Table5可以看出，1）不同的方法其deformable ConvNet版本要比原版本效果要好；2）在使用表达能力更加强的特征提取网络，以及多尺度测试和iterative bounding box average的情况下，效果得到了更进一步的提高。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deformable Convolution </tag>
            
            <tag> Deformable RoI Pooling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Aggregated Residual Transformations for Deep Neural Networks_CVPR2017_XieS et al</title>
      <link href="/post/e3ea9c19.html"/>
      <url>/post/e3ea9c19.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>深度学习的发展，使得计算机视觉从最开始的设计人工特征，发展到现在主要是设计网络结构。但是网络结构的超参很复杂，并不容易设计。当前比较成功的设计思想有两种方式，一种是类似于vgg-net和ResNet那样堆叠相同的构建块；另一种方式是类似Inception模型那样小心的设计网络拓扑结构，让网络在具有较强的表示能力的情况下，尽可能的较少模型的复杂度。虽然Inception模型在减少复杂度的情况下，能保持较高的性能，但是在设计网络结构的时候需要费很多心思，并且设计出来的网络结构可能会过于适应当前任务，导致对于其他任务表现不佳。在这篇文章中，作者采用VGG和ResNet的设计思想即堆叠基本的构建块，但是采用Inception的设计策略即采用多个不同的分支结构的形式，提出一个简单的网络结构。提出的模块，在执行一系列变换之后，将变换结果进行整合，即split-transformation-aggregate的方式。变换的方式可以多种多样，但这里作者采用了相同的变换方式，使得该思想具有很大的扩展性。作者将提到的网络结构命名为ResNetXt。该方法可以在维持一定复杂度的情况下,提高算法精度。作者通过实验证明,cardinality（变换的基数）是一个跟深度，和宽度同等重要的能提高算法性能的维度。甚至比增加宽度或者深度来得更加有效。该方法在ILSVRC2016分类任务上获得第二名。<br>代码<a href="https://github.com/facebookresearch/ResNeXt" target="_blank" rel="noopener">https://github.com/facebookresearch/ResNeXt</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-创新点"><a href="#2-1-创新点" class="headerlink" title="2.1 创新点"></a>2.1 创新点</h2><ol><li>引入一个新的维度cardinality，并提出了一个新的网络结构。</li><li>通过实验证明了该维度与深度和宽度同等重要，甚至对于性能的提高更加明显。</li></ol><h2 id="2-2-重要结论："><a href="#2-2-重要结论：" class="headerlink" title="2.2 重要结论："></a>2.2 重要结论：</h2><ol><li>cardinality（变换的基数）是一个跟深度，和宽度同等重要的一个能提高算法性能的一个维度。甚至比增加宽度或者深度来得更加有效。</li><li>只要在转换深度大于等于3的时候，这种变换才有意义。<img src="/post/e3ea9c19/CA1BC53B-653A-47EB-8AC9-1F9EA0A5CD7B.png"></li><li>在Mask R-CNN中也使用了ResNeXt，并在实例分割和目标检测上达到了最先进的结果。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-模块构建规则及构建的模块"><a href="#3-1-模块构建规则及构建的模块" class="headerlink" title="3.1 模块构建规则及构建的模块"></a>3.1 模块构建规则及构建的模块</h2><ol><li>模块构建规则<br>作者提出的网络结构主要采用与VGG/ResNets类似的高度模块化网络设计方式。由一系列residual模块构成。这些模块都具有相同的拓扑结构，并按照以下两种规则来构建：<br>1).feature map大小相同的block共享相同的超参(相同的宽度和滤波器大小)。<br>2).当feature map大小减少1倍的时候，blocks的宽度会增大一倍。</li><li>构建的网络模块<br>作者提出的构建块如下所示:<img src="/post/e3ea9c19/37D4102C-FD06-457E-900C-2DD75E644F8B.png"></li></ol><h2 id="3-2-可以将提出的构建块变成分组卷积的形式"><a href="#3-2-可以将提出的构建块变成分组卷积的形式" class="headerlink" title="3.2 可以将提出的构建块变成分组卷积的形式"></a>3.2 可以将提出的构建块变成分组卷积的形式</h2><img src="/post/e3ea9c19/72CD82A0-96BD-4DA3-950B-E7D7D8BF8154.png"><p>这使得提出构建块很容易实现。可以通过简单的分组卷积的方式来实现提到的构建块。</p><h2 id="3-3-网络拓扑结构"><a href="#3-3-网络拓扑结构" class="headerlink" title="3.3 网络拓扑结构"></a>3.3 网络拓扑结构</h2><img src="/post/e3ea9c19/B2BC1274-53B2-4717-8DA2-7962749B32A7.png"><h2 id="3-4-讨论"><a href="#3-4-讨论" class="headerlink" title="3.4 讨论"></a>3.4 讨论</h2><p>这里作者使用了相同的转换形式，使得提到的构建块能够通过group 卷积的形式来实现。但是这只是作者提到的构建方法的一个具体的实现。作者提到的构建方式是基于split-transformation-aggregate的方式。变换形式可以多种多样。</p><h1 id="四、实验结果及结论"><a href="#四、实验结果及结论" class="headerlink" title="四、实验结果及结论"></a>四、实验结果及结论</h1><h2 id="4-1-Imagenet-1k分类任务"><a href="#4-1-Imagenet-1k分类任务" class="headerlink" title="4.1 Imagenet-1k分类任务"></a>4.1 Imagenet-1k分类任务</h2><img src="/post/e3ea9c19/F01A4DBB-EE7D-462E-A9A0-8767B7B1E01C.png"><p>Table2给出了相同复杂度下不同组数对应的bottleneck的宽度。<br><img src="/post/e3ea9c19/84F3B697-5F62-468F-BA94-76F318AADC01.png"><br>从Table3可以看到在相同复杂度下，随着组数增多，性能也跟着提高。并且随着组数的增加，性能的提高也趋向饱和。<br><img src="/post/e3ea9c19/E8A79D3F-6B1A-47D5-9DCF-11B290E7FD84.png"><br>从Table4可以看到相同复杂度下提高深度或者宽度带来的性能提升并没有提高分组好；并且即使在增加的复杂度只有使用宽度和深度增加的复杂度的50%的时候，使用分组卷积仍然比他们好。<br><img src="/post/e3ea9c19/E43B0223-10A0-4095-AA3F-E44D8E935428.png"><br>Figure5为各个模型训练的误差曲线<br><img src="/post/e3ea9c19/4A0F5D52-582F-455B-9444-15B767D2F9FF.png"><br>在移除short cut的情况下，不使用分组比使用分组卷积性能下降更严重。<br><img src="/post/e3ea9c19/470A46E5-C2EC-4EE4-B378-982E09AC9A77.png"><br>Table5为ResNeXt与跟其它最先进方法的比较。</p><h2 id="4-2-ImageNet-5k分类任务"><a href="#4-2-ImageNet-5k分类任务" class="headerlink" title="4.2 ImageNet-5k分类任务"></a>4.2 ImageNet-5k分类任务</h2><p>该数据集是整个ImageNet-22k的一个子集，该数据集包括了ImageNet-1k和额外的4k个类别。整个数据集有680万张图片，大概是ImageNet-1k的5倍。由于没有官方的训练集和验证集的划分方式，作者使用了ImageNet-1k的验证集来验证。实验细节与ImageNet-1k一致。作者采用从头开始训练的方式。<br><img src="/post/e3ea9c19/79E3F82C-D2B2-4B44-810A-E79F02AD5A9A.png"><br>Figure6给出训练的误差曲线<br><img src="/post/e3ea9c19/F4AD2992-0A7F-4C9D-BADE-7B330716528C.png"><br>这里验证集采用将其它类别抛弃只对1k个类别使用softmax的方式。从Table6的结果可以看到尽管复杂度差不多，但是ResNeXt具有更强的模型表达能力，并且在验证集上达到更好的效果。</p><h2 id="4-3-CIFAR分类任务"><a href="#4-3-CIFAR分类任务" class="headerlink" title="4.3 CIFAR分类任务"></a>4.3 CIFAR分类任务</h2><img src="/post/e3ea9c19/39D8CF6F-9C21-414B-829D-D7477395ED02.png"><p>从Figure7可以看出增加组数比增加宽度对性能的提高更加明显。<br><img src="/post/e3ea9c19/74A745B4-CEF8-45A6-A034-F1B310BF56AC.png"><br>从Table7可以看出在同等复杂度下，ResNeXt要比Wide ResNet要好。</p><h2 id="4-4-coco目标检测任务"><a href="#4-4-coco目标检测任务" class="headerlink" title="4.4 coco目标检测任务"></a>4.4 coco目标检测任务</h2><p>作者使用COCO trainval-115k来训练，并使用val-5k来测试。评估方式为coco-style AP和AP@IOU=0.5。检出框架采用faster rcnn，backbone使用ResNet/ResNeXt。backbone先在ImageNet-1K分类任务上进行预训练，然后在检出数据集上进行finetune。<br><img src="/post/e3ea9c19/9E87493D-7C06-40E2-B756-4B006023C1A0.png"><br>Table8为ResNet和ResNeXt在COCO目标检测任务上的对比结果。从Table8可以看出ResNeXt要比ResNet性能更好。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Group Convolution </tag>
            
            <tag> ResNeXt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices_CVPR2017_ZhangX et al</title>
      <link href="/post/876b210.html"/>
      <url>/post/876b210.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>作者发现Xception和ResNeXt在网络比较小的情况下，由于存在大量的1x1卷积使得网络的效率不高。为此作者在这篇论文中提出使用1x1的分组卷积来降低1x1卷积的复杂度。为了消除分组卷积带来的负面影响，作者提出了一个新颖的处理方式，对分组卷积通道进行shuffle。基于这两个技术，作者提出了一个专门为移动端设计的CNN网络结构，ShuffleNet。这两个技术使得提到的网络结构可以在维持准确率的情况下极大的减少计算量。相较于其他同等复杂度的网络结构，shuffleNet可以使用更多的通道来编码更多有用的信息，这个对于非常小的网络的性能是很关键的。作者在ImageNet分类任务和COCO目标检测任务上验证了该方法的有效性。特别是在ImageNet分类任务上，在计算量为40MFLOPs的情况下top-1误差要比MobileNet低7.8%。在基于ARM的设备上，提到的网络结构在维持跟AlexNet同等准确率的情况下，速度要比AlexNet快大约13倍。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ol><li>提出使用1x1的分组卷积来降低1x1卷积的复杂度并通过对分组卷积通道进行shuffle来消除分组卷积带来的负面影响（即分组卷积组间信息的关系没能很好的学习到）。</li><li>提出了更适合于小网络的ShufflNet unit</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-Channel-Shuffle-for-Group-Convolutions"><a href="#3-1-Channel-Shuffle-for-Group-Convolutions" class="headerlink" title="3.1 Channel Shuffle for Group Convolutions"></a>3.1 Channel Shuffle for Group Convolutions</h2><img src="/post/876b210/E9E9A39C-A311-4C3A-A9EC-9BB039DCD835.png"><p>作者发现现在主流的网络结构，如Xception和ResNeXt，没有考虑对1x1卷积进行优化设计，但是1x1卷积在整个网络中占据的计算量是很大的。为此作者采用分组卷积对其进行优化，分组卷积可以大大降低1x1卷积的计算量。但是当多个分组卷积堆在一起的时候会出现一个问题，就是下个分组卷积中每个组有可能只用了上个分组卷积里面同个组产生的结果，Figure1(a)阐明了这种情况。为了避免这种情况，可以让下个分组卷积的输入来自于上个分组卷积的各个分组的结果，如Figure1(b)所示。这种方式可以通过通道shuffle操作来实现，如Figure1(c)。通道shuffle操作其实很简单，假设有$g$个分组的卷积层，则其输出的通道数为$g\times n$；那么通道shuffle操作是先将输出的通道维度reshape成$(g,n)$，然后将其转置再展平。这个操作在前后两个分组卷积的组数不同的情况下也是适用的，而且该操作也是可微的，能够进行端到端的训练。</p><h2 id="3-2-ShuffleNet-unit"><a href="#3-2-ShuffleNet-unit" class="headerlink" title="3.2 ShuffleNet unit"></a>3.2 ShuffleNet unit</h2><img src="/post/876b210/0571D1A1-FBFD-4C8D-AD15-CB3163EF7D1A.png"><p>Figure2给出了ShuffleNet unit结构，该结构主要是在resnet unit的bottlenet形式上加入了逐通道分离卷积、1x1分组卷积以及通道shuffle操作。Figure2(a)为加入逐通道分离卷积的bottleneck unit；Figure2(b)为ShuffleNet unit；Figure2(c)为stride=2的ShuffleNet unit。</p><h2 id="3-3-网络结构"><a href="#3-3-网络结构" class="headerlink" title="3.3 网络结构"></a>3.3 网络结构</h2><img src="/post/876b210/59132592-9306-4570-97CB-85C98E6645DB.png"><p>Table1给出了ShuffleNet的网络结构。这里每个ShuffleNet unit中bottleneck通道是输出通道的四分之一。stage2的第一个shuffleNet unit，作者没有使用1x1的分组卷积，因为网络的通道数比较小,而是适用了加入逐通道分离卷积的resnet的bottlenet形式。Table1给出的是复杂度为1的ShuffleNet版本，即为ShuffleNet 1x，其它复杂度可以通过减少通道数据来得到，比如shuffuleNet sx，可以将通道数乘以s倍。</p><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>作者使用ImageNet2012分类数据集来验证提到的网络结构的有效性。具体参数设置可以参看原文。</p><h2 id="4-1-Ablation-Study"><a href="#4-1-Ablation-Study" class="headerlink" title="4.1 Ablation Study"></a>4.1 Ablation Study</h2><h3 id="4-1-1-使用分组的1x1卷积对模型性能的影响"><a href="#4-1-1-使用分组的1x1卷积对模型性能的影响" class="headerlink" title="4.1.1 使用分组的1x1卷积对模型性能的影响"></a>4.1.1 使用分组的1x1卷积对模型性能的影响</h3><img src="/post/876b210/9A0F9763-80B5-4FFA-9C20-6EF7BB098042.png"><p>Table2对比了3个不同尺度，每个尺度不同分组的效果。g=1为不使用1x1的分组卷积的情况。从Table2中可以看出使用1x1分组卷积要比不使用分组卷积效果要好。另外由于组数越多可以产生的特征图也越多，也可以看出特征图越多的情况下效果越好。但是当分组达到一定程度的时候，由于输入的通道数变少也会影响效果。所以这两方面应该进行权衡。</p><h3 id="4-1-2-使用通道shuffle对性能的影响"><a href="#4-1-2-使用通道shuffle对性能的影响" class="headerlink" title="4.1.2 使用通道shuffle对性能的影响"></a>4.1.2 使用通道shuffle对性能的影响</h3><img src="/post/876b210/D157B820-0847-4A8A-9DD3-5A252A4A5F3D.png"><p>Table3对比了3个不同的复杂度下，使用通道shuffle和不使用通道shuffle的性能。从Table3可以看出使用通道shuffle要比不使用效果要好，特别是当组数比较大的情况下，效果更明显。</p><h2 id="4-2-不同网络结构单元的对比"><a href="#4-2-不同网络结构单元的对比" class="headerlink" title="4.2 不同网络结构单元的对比"></a>4.2 不同网络结构单元的对比</h2><img src="/post/876b210/D8243687-22C9-4E35-8060-D1502CFD6628.png"><p>作者按照VGG，ResNet，Xception，ResNeXt的设计思想，重新设计了对应的小网络版本。从Table4可以看出相同复杂度下，提到的网络结构要比其他设计单元要好。</p><h2 id="4-3-与MobileNets及其他框架进行对比"><a href="#4-3-与MobileNets及其他框架进行对比" class="headerlink" title="4.3 与MobileNets及其他框架进行对比"></a>4.3 与MobileNets及其他框架进行对比</h2><img src="/post/876b210/E9073E47-2611-4932-97B3-BE3C5F977BEF.png"><p>MobileNet是专门为移动设备设计的网络，他在小模型层面取得了最先进的结果。Table5给出了ShuffleNet和MobileNet在分类任务上的对比结果。可以看到在同等复杂下，ShuffleNet效果都比MobileNet要好。虽然ShuffleNet原本是设计成小模型的，但是它在网络复杂度较大的情况下也超过过了同等复杂度的MobileNet。另外作者也对比了ShuffleNet和MobileNet在同等深度下的结果，ShuffleNet仍然要比MobileNet好，说明增加的性能不是仅深度带来的，而更多是ShuffleNet Unit。<br><img src="/post/876b210/E4AB8BC1-63D1-439E-B534-1954CD36CE44.png"><br>Table6作者对比在相同的准确率下，主流框架与ShuffleNet的复杂度。可以看出在同等准确率下，ShuffleNet的复杂度要远远小于其他主流模型。<br>简单的网络结构设计也使得ShuffleNet很容易结合最新的一些成果，比如Squeeze-and-Excitation模块。从Table5中可以看出，通过组合SE模块，top-1误差降低到了24.7%。虽然SE模块增加了很小的网络复杂度，但是在移动端运行速度却要慢上25%到40%。这说明网络结构的设计对于移动端的速度是很关键的，不仅仅是模型的复杂度。</p><h2 id="4-4-模型在目标检测任务上的效果"><a href="#4-4-模型在目标检测任务上的效果" class="headerlink" title="4.4 模型在目标检测任务上的效果"></a>4.4 模型在目标检测任务上的效果</h2><img src="/post/876b210/7A28B9D7-64BE-484A-904C-F8E12D34E547.png"><p>作者采用faster rcnn框架去测试提到的模型在COCO目标检测任务的效果。作者使用COCO trainval-113k作为训练集，val-5k作为验证集。Table7给出了两个不同分辨率下MobileNet和ShuffleNet在COCO目标检测任务上的效果。ShuffleNet 2x与MobileNet具有相同的复杂度，从Table7可以看出ShuffleNet 2x在COCO目标检测任务上要比同等复杂度的MobileNet效果要好；而ShuffleNet 1x与MobileNet效果差不多，但是ShuffleNet 1x的复杂度要远小于MobileNet，差不多事MobileNet的四分之一。</p><h2 id="4-5-ARM平台上的真实速度评估"><a href="#4-5-ARM平台上的真实速度评估" class="headerlink" title="4.5 ARM平台上的真实速度评估"></a>4.5 ARM平台上的真实速度评估</h2><img src="/post/876b210/71B19513-CCA0-40AE-ACF2-F7172EA74FC1.png"><p>作者发现虽然g越大效果越好，但是也比较影响推断速度。而g=3是性能和速度两方面比较好的权衡。Table8给出了3中不同的分辨率下的测试结果。作者发现他们的实现方案，复杂度每降低4倍，速度可以提高2.6倍。最终与AlexNet相比，ShuffleNet 0.5x在准确率保持基本一致的情况下，可以带来13倍的加速。这个速度比同等准确率下的其它模型或者加速方法都要快。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ShuffleNet </tag>
            
            <tag> Mobile Device </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Light-Head R-CNN:In Defense of Two-Stage Object Detector_CVPR2017_LiZ et al</title>
      <link href="/post/f4b2034c.html"/>
      <url>/post/f4b2034c.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>在这篇论文中，作者调查了为什么两阶段的方法没法跟一阶段的方法一样快。作者发现主要原因是两阶段方法的检测部分太重了，很影响速度，即使把backbone换成轻量级的也无法提速。比如faster-rcnn使用了两个很大的全连接层去对每个区域的候选目标做进一步的筛选和定位。resnet backbone+faster-rcnn使用了整个resnet第五stage去对每个区域的候选目标做进一步的分类和定位，这些都是很耗时的；另外由于ROIpooling出来的特征也很大，所以在接入第一层全连接层的时候，运算量也会很大，这些都对速度带来很大的影响。为了解决逐区域预测带来的计算量的极大消耗，R-FCN使用了一个区域共享的全卷积神经网络去对每个ROI区域做进一步的筛选和定位，但是R-FCN使用了一个很大的位置敏感得分图，这个也很消耗内存和计算时间。为此作者提出了一个轻量级的ROI特征提取层及轻量级的检出头，构建了一个高效但是准确率很高的两阶段检测器。在使用resnet-101作为backbone，并加上提到的轻量级ROI特征提取层及轻量级检出头，并在保持时间效率的情况下，提到的方法可以超过在COCO数据集上最先进的检出方法。当将backbone替换为类xception的网络结构的时候，更是可以在COCO测试集上以每秒102帧的速度达到30.7mmAP的效果，在速度与准确率上远超YOLO和SSD.<br><img src="/post/f4b2034c/68995EB4-FDDF-48E3-AC69-D36F094432CC.png"><br>Figure 1为提到的方法在使用不同的backbone下，与最先进的目标检测方法在准确率和速度上的对比情况。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ol><li>提出了一个轻量级的ROI特征提取层及轻量级的检出头，构建了一个高效但是准确率很高的两阶段检测器。</li><li>高准确率版本的Light-Head R-CNN在COCO test-dev上与其他最先进的方法的对比要比最优方法高出1.4个点。</li><li>快速版本的Light-Head R-CNN与最先进的快速检测器做比较，在速度和准确率上都比最先进方法要好。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-Light-Head-R-CNN"><a href="#3-1-Light-Head-R-CNN" class="headerlink" title="3.1 Light-Head R-CNN"></a>3.1 Light-Head R-CNN</h2><img src="/post/f4b2034c/E34F9B44-25F5-446E-9A21-F2F69DE972E6.png"><p>传统的目标检测器的目标检出部分包括两个部分：一个是R-CNN子网，另一个是RoI warping。只考虑准确率的话，faster rcnn更加擅长对ROI进行分类，但是为了降低计算量，通常在R-CNN子网里面会使用一个全局的平均pooling层，这样往往会造成空间信息的缺失从而影响到定位效果；而RFCN则是直接使用位置敏感Pooling层去得到ROI的分类和定位结果，这种方式没有进行逐区域的计算，性能往往比faster rcnn差。只考虑速度的话，faster rcnn对每个区域都应用了一个RCNN子网速度较慢，特别是当候选目标比较多的时候，速度更加慢；RFCN没有对每个区域使用RCNN子网，但是它需要生成一个很大的位置敏感得分图，也很耗时。faster rcnn和RFCN网络整体结构如Figure2(a)和Figure2(b)所示。考虑到这些问题，作者使用一个简单的全连接层作为R-CNN子网，并减少用于R-CNN子网的特征维度，构建了一个Light-Head的R-CNN网络。构建的Light-head RCNN网络整体结构如Figure2(C)所示。</p><h3 id="3-1-1-Thin-Feature-maps-for-RoI-warping"><a href="#3-1-1-Thin-Feature-maps-for-RoI-warping" class="headerlink" title="3.1.1 Thin Feature maps for RoI warping"></a>3.1.1 Thin Feature maps for RoI warping</h3><p>在将候选的特征输入到R-CNN子网之前，会使用RoI warping将候选的特征固定在一定的维度。作者在提到的网络中，使用了一个小的特征图供RoI warping提取特征。作者发现小特征图不仅可以提高准确率而且还能够节省内存和计算时间。在使用小的特征图之后，R-CNN子网可以不用使用全局平均pooling，从而可以提高性能；R-FCN也可以在接一个子网去提升R-CNN的效果。</p><h3 id="3-1-2-Light-Head-R-CNN-for-Object-Detection"><a href="#3-1-2-Light-Head-R-CNN-for-Object-Detection" class="headerlink" title="3.1.2 Light-Head R-CNN for Object Detection"></a>3.1.2 Light-Head R-CNN for Object Detection</h3><img src="/post/f4b2034c/957507E4-3385-4CAD-A194-B383F21B0D78.png"><ul><li>backbone<br>作者使用了两种backbone，”L”为高准确率版本；”S”为高速度版本。”L”backbone作者使用了ResNet101；”S”backbone作者使用了类Xception的网络结构。为了产生小的特征图，作者在$C_5$卷积层上应用了一个大kernel的分解卷积层，结构如Figure3所示。这里k为15，对于”S”，$C_{mid}=64$；对于”L”，$C_{mid}=256$。作者也将$C_{out}$减小为$10\times p\times p$，相对于RFCN的位置敏感得分图要小很多。</li><li>R-CNN subnet<br>在每个ROI区域上作者使用单层全连接层(通道数为2048没有加入dropout)然后接入两个全连接层用于RoI的分类和回归，以此作为R-CNN子网。这里回归任务是类别无关的。</li><li>RPN<br>作者将RPN接在$C_4$层，anchor包括了5种尺度3种宽高比，尺寸为$\{32^2,64^2,128^2,256^2,512^2\}$，宽高比为$\{1:2,1:1,2:1\}$。在使用RPN得到候选之后，作者使用IoU阈值为0.3的NMS来过滤掉冗余的目标；然后根据候选目标与grouth truth bbox的IoU来为anchor分配标签。当候选与grouth truth的IoU最大或者IoU大于0.7则被分配为正样本。当候选与grouth truth的IoU小于0.3，则被分配为负样本，其它的被忽略。</li></ul><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>作者使用具有80个类别的COCO分类任务去评估提到的方法的有效性。并使用trainval-115k作为训练集，val-5k作为验证集。</p><h2 id="4-1-实现细节"><a href="#4-1-实现细节" class="headerlink" title="4.1 实现细节"></a>4.1 实现细节</h2><ul><li>使用8个Pascal TITAN XP GPU</li><li>使用同步的SGD</li><li>权重衰减系数为0.0001，动量系数为0.9</li><li>每个batch-size每个GPU2张图片，每张图片训练的时候选择2000个RoI候选，测试的时候选择1000个RoI候选</li><li>每个mini-batch将图片pad成一样大小，位置是在图片的右下角</li><li>前1.5M次迭代(这里的迭代是值1张图片)学习率为0.01，之后0.5M次迭代学习率为0.001</li><li>backbone为resnet的所有实验，在stage5都使用了孔洞卷积，并使用在线困难样本挖掘(OHEM)</li><li>backbone使用imagenet分类任务进行预训练。</li><li>roipooling大小为7x7</li><li>实验的时候作者固定了backbone的第1、第2个stage。</li><li>采用水平翻转数据增强方式</li></ul><h2 id="4-2-Ablation-Experiments"><a href="#4-2-Ablation-Experiments" class="headerlink" title="4.2 Ablation Experiments"></a>4.2 Ablation Experiments</h2><img src="/post/f4b2034c/8F747207-F641-4E4B-8F26-1C40EEBA8CEA.png"><p>Table1给出了两个baseline，一个是原本的R-FCN，另一个则是在原本的R-FCN上对参数进行调整得到的更好的结果。调整方式包括：1）将图片的短边缩放到800像素，并设置最长边不超过1200。2）将RPN的anchor的尺度变成5，为$\{32^2,64^2,128^2,256^2,512^2\}$.3)将回归任务的损失函数的权重翻倍。4）在线困难样本挖掘的时候使用top256个样本5）训练的时候使用2000个候选，测试的时候使用1000个候选。</p><h3 id="4-2-1-使用小feature-map对性能的影响"><a href="#4-2-1-使用小feature-map对性能的影响" class="headerlink" title="4.2.1 使用小feature map对性能的影响"></a>4.2.1 使用小feature map对性能的影响</h3><img src="/post/f4b2034c/5781DCF7-EEED-4A07-B33E-FA9CAAACB0E5.png"><p>如Figure4所示，作者对原本的R-FCN做了两个调整。1）减少了原本的R-FCN的敏感得分图的通道数，从$3969(81\times 7\times 7)$减少到$490(10\times7\times7)$;2)在RSRoI pooling层之后接入两个全连接层去执行分类和回归任务。<br><img src="/post/f4b2034c/5BD84478-A37C-4FF0-B369-DE9538611FFF.png"><br>从结果可以看出，虽然通道数减少了很多，但是结果下降并不多。另外通过使用小的feature map也可以在此结构上使用FPN网络来进一步提高性能。</p><h3 id="4-2-2-使用Large-separable-convolution对性能的影响"><a href="#4-2-2-使用Large-separable-convolution对性能的影响" class="headerlink" title="4.2.2 使用Large separable convolution对性能的影响"></a>4.2.2 使用Large separable convolution对性能的影响</h3><img src="/post/f4b2034c/249753BF-A4D3-4B86-9373-284D7615F90B.png"><p>原本为了减小feature map作者使用了1x1卷积，在使用由Large separabel convolution生成的小feature map去提取RoI区域特征之后，AP要比baselines高0.7个点。</p><h3 id="4-2-3-使用Light-Head-R-CNN对性能的影响"><a href="#4-2-3-使用Light-Head-R-CNN对性能的影响" class="headerlink" title="4.2.3 使用Light-Head R-CNN对性能的影响"></a>4.2.3 使用Light-Head R-CNN对性能的影响</h3><img src="/post/f4b2034c/AE67BB94-912D-49B6-B171-E8BB444626CD.png"><p>从结果可以看出在使用Large Kernel之后使用Light-Head R-CNN对效果提升明显。</p><h2 id="4-3-Light-Head-R-CNN-高准确率下的设置"><a href="#4-3-Light-Head-R-CNN-高准确率下的设置" class="headerlink" title="4.3 Light-Head R-CNN,高准确率下的设置"></a>4.3 Light-Head R-CNN,高准确率下的设置</h2><img src="/post/f4b2034c/458FE052-EEE3-43B5-8471-757CAC15F2B9.png"><p>为了获得更高的准确率，作者结合了最新的目标检测方法。如Table6所示。在使用”L”backbone的基础上，作者将RoIAlign提到的差值方法应用到了PSRoI pooling上，带来了1.3个点的提升。在此基础上，作者将nms抑制的阈值从原本的0.3，变成了0.5，带来了0.6个点的提升。紧接着作者使用了多尺度的训练方式，将图片随机reshape到{600,700,800,900,1000}然后将图片的短边reshape到训练对应的尺度上，带来了1个点的提高。<br><img src="/post/f4b2034c/E3FF9C91-AD19-449C-B72C-CBE5A798793F.png"><br>Tabel5给出了高准确率的Light-Head R-CNN在COCO test-dev上与其他最先进的方法的对比结果。从结果可以看出提到的方法要比最优方法高出2.4个点。</p><h2 id="4-4-Light-Head-R-CNN-高速度下的设置"><a href="#4-4-Light-Head-R-CNN-高速度下的设置" class="headerlink" title="4.4 Light-Head R-CNN,高速度下的设置"></a>4.4 Light-Head R-CNN,高速度下的设置</h2><img src="/post/f4b2034c/5EDECCFE-1BFA-456F-A765-23B869DB16CD.png"><p>为了让推断速度更快，作者对网络及相关设置做了一些调整。包括：1）利用xception提到的逐通道卷积替换掉resnet中的卷积层,并且由于网络比较浅作者没有使用pre-activation设计。具体的网络结构如Table7所示。2）弃用孔洞卷积。3）将RPN卷积的通道数设置为256，为原来的一半。4）应用一个$kernelsize=15$,$C_{min}=64$,$C_{out}=490$的large sparable convolution。5)使用对齐的PSPooling层。<br><img src="/post/f4b2034c/B3FB7DC1-B9C8-49B5-AFEC-3EC03D801E9C.png"><br>如Table8所示作者将快速版本的Light-Head R-CNN与最先进的快速检测器做比较，在速度和准确率上都比最先进方法要好。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Light-Head RCNN </tag>
            
            <tag> RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Xception:Deep learning with Depthwise Separable Convolutions_CVPR2017_CholletF</title>
      <link href="/post/d9144852.html"/>
      <url>/post/d9144852.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>普通的卷积可以理解为同时映射通道之间以及空间之间的关系，而Inception模块则是让这个过程更加简单，它首先通过一系列的1X1卷积映射通道之间的关系，将输入数据映射到3到4个分离的空间；然后再通过3X3或者5X5卷积在这些小的3D空间中，映射通道之间和空间之间的相互关系。逐深度分离卷积可以理解为最大化towers的Inception 模块。是不是跨通道的关系和跨空间的关系可以完全解耦，是不是利用逐深度离散卷积替换掉Inception模块效果更好？基于这样的假设作者提出了一个新的网络结构，主要是将Inception模块替换为逐深度分离卷积，作者将其称为Xception。最后作者通过了实验验证该结构的有效性，在同等网络复杂度的情况下，Xception网络在Imagenet数据集上稍微比Inception V3好，并且在更大的数据集上提升效果更加明显。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><img src="/post/d9144852/97D0ECC6-EB3E-4312-AD98-542B1F3FB7FB.png"><img src="/post/d9144852/4E0E0923-D614-4CAD-862E-087EF99E3356.png"><img src="/post/d9144852/8E1B8BF2-26E1-4E92-8188-33AE46ACB9DA.png"><img src="/post/d9144852/D858DB3C-425A-4E93-91C5-AC6350F8F04E.png"><p>考虑最简单版本的Inception模块，只是用卷积tower,不使用平均tower，如figure2。这样就可以将其看做是一个大的1x1卷积在加上一个group卷积，如figure3。如果这个group达到极限，如figure4，也即每个group只有一个通道，那就是它的极限形式。<br>这个形式与逐深度离散卷积基本等同。它与逐深度离散卷积的不同之处在于1）计算顺序不同，逐深度离散卷积是先计算逐通道卷积，再利用1X1卷积去计算跨通道卷积。2）逐深度离散卷积在计算逐通道卷积的时候一般不使用非线性函数。第一点一般没有影响，第二点可能对性能有所影响，所以后面作者对此进行了研究。通过这种变换关系，是不是利用逐深度离散卷积替换掉Inception模块更好呢，为此有了这个新的网络结构Xception.在这篇论文中作者只是研究了这种极限形式，而普通卷积和逐通道离散卷积的中间形式是否更好作者并没有进行研究。</p><h1 id="三、实现细节"><a href="#三、实现细节" class="headerlink" title="三、实现细节"></a>三、实现细节</h1><h2 id="3-1-网络结构"><a href="#3-1-网络结构" class="headerlink" title="3.1 网络结构"></a>3.1 网络结构</h2><img src="/post/d9144852/09528774-B069-4EFE-90A8-DD6633A3D3D6.png"><p>整体的网络结构如figure5所示，整个特征提取网络有36层，最后加入一个逻辑回归分类器。这36层被划分成14个模块，除了第一个和最后一个，每一个都有residual connections。<br>代码<a href="https://keras.io/applications/#xception" target="_blank" rel="noopener">https://keras.io/applications/#xception</a></p><h1 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h1><h2 id="4-1-模型及数据集说明"><a href="#4-1-模型及数据集说明" class="headerlink" title="4.1 模型及数据集说明"></a>4.1 模型及数据集说明</h2><p>由于Inception V3和提到的Xception模型复杂度差不多，所以作者选择了Inception V3来做对比实验。作者在两个任务上做对比试验，一个是ImageNet分类任务，该任务是包含1000个类别的单标签分类任务；另一个是JFT分类任务，该任务是有17000个类别的多标签分类任务。JFT数据集包括了350million张高分辨率的图片，总共类别数为17000个。为了评估模型在JFT上训练的效果，作者使用了另一个辅助数据集FastEval14k来进行评估。该数据集包括14000张图片，共6000个类别，平均每张图片标签数为36.5个。评估方式是使用带权重的MAP@100,这个score主要来自于该图片在社交媒体上出现的频率。</p><h2 id="4-2-参数设置"><a href="#4-2-参数设置" class="headerlink" title="4.2 参数设置"></a>4.2 参数设置</h2><p>On ImageNet: </p><ul><li>Optimizer: SGD </li><li>Momentum: 0.9 </li><li>Initial learning rate: 0.045</li><li>Learning rate decay: decay of rate 0.94 every 2 epochs </li></ul><p>On JFT: </p><ul><li>Optimizer: RMSprop</li><li>Momentum: 0.9 </li><li>Initial learning rate: 0.001</li><li>Learning rate decay: decay of rate 0.9 every 3,000,000 samples </li></ul><p>两个模型在这两个任务上都使用了相同的参数配置，而且这些参数配置是适配于Inception V3的，这就撇开了故意去选择对于Xception更好的参数</p><p>正则化:</p><ul><li>weight decay :<br>Inception V3:4e-5<br>xception: 1e-5 </li><li>Dropout:<br>Imagenet:0.5 both<br>JFT: none both</li><li>Auxiliary loss:<br>None</li></ul><p>训练平台为60块 K80 GPU</p><ul><li>Imagenet采用同步梯度下降，跑3d</li><li>JFT采用异步梯度下降，跑1个月没完全拟合，完全拟合需要3个月。</li></ul><h2 id="4-3-结果比较"><a href="#4-3-结果比较" class="headerlink" title="4.3 结果比较"></a>4.3 结果比较</h2><h3 id="4-3-1-分类性能的比较"><a href="#4-3-1-分类性能的比较" class="headerlink" title="4.3.1 分类性能的比较"></a>4.3.1 分类性能的比较</h3><img src="/post/d9144852/3C35F990-0DF0-4CB4-9324-ECF3FACF17FC.png"><img src="/post/d9144852/C6E06219-A0C9-4789-82A3-98C588FFF0F9.png"><img src="/post/d9144852/DD7D2CD5-656D-4045-872D-81575AB9E70C.png"><img src="/post/d9144852/09ABF202-40A4-46B9-9D25-4360C72D79E3.png"><img src="/post/d9144852/5563DDE7-4F93-4573-BDF8-CF3DD824B6DD.png"><p>所有的评估都是用单模型单尺度，并且Imagenet任务都是验证集上的结果，JFT的结果是运行了1个月后的结果不是完全拟合的结果。<br>从结果上可以看到，在这两个任务上，Xception都要优于Inception V3。并且在JFT上，提升更为明显，作者认为这是因为Inception V3的参数更加适配Imagenet分类任务导致的，如果Xception也进行参数的调优，提升效果会更加明显。</p><h3 id="4-3-2-模型大小和速度的比较"><a href="#4-3-2-模型大小和速度的比较" class="headerlink" title="4.3.2 模型大小和速度的比较"></a>4.3.2 模型大小和速度的比较</h3><img src="/post/d9144852/8E90AFD6-CA69-4DEC-A9BD-F2D01F5090D0.png"><p>从Table3中可以看出两个模型的大小相似，但是Xception要稍微比Inception V3慢。从模型大小可以看出Xception性能的提升不是来自于模型表达能力的增加，而是来自于逐深度离散卷积带来的性能提升。</p><h3 id="4-3-3-residual-connetions的影响"><a href="#4-3-3-residual-connetions的影响" class="headerlink" title="4.3.3 residual connetions的影响"></a>4.3.3 residual connetions的影响</h3><img src="/post/d9144852/8A44D215-035D-460A-939F-7016D4C68951.png"><p>从结果可以看出residual connection确实有助于模型的拟合，不管在速度或者是模型性能，都带来了提升。residual connections只是对于这种网络结构很重要，但是并不是说主要的提升是它。作者也试了在vggnet中加入逐深度离散卷积，在参数数与InceptionV3接近的情况下，在JFT任务上也超过了Inception V3。说明该结构确实有用。</p><h3 id="4-3-4-逐通道卷积操作后加入激活函数对结果的影响"><a href="#4-3-4-逐通道卷积操作后加入激活函数对结果的影响" class="headerlink" title="4.3.4 逐通道卷积操作后加入激活函数对结果的影响"></a>4.3.4 逐通道卷积操作后加入激活函数对结果的影响</h3><img src="/post/d9144852/9F8DB3A1-A6D1-4015-8F8E-0C04EF5355D1.png"><p>从结果可以看出，在逐通道卷积操作之后不使用任何激活函数，效果是最好的，不管是拟合速度还是分类性能都更好。这可能是对于多通道的卷积使用激活函数更有用，而单通道的卷积不使用激活函数更加有用。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> xception </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Focal Loss for Dense Object Detection(ICCV2017)Tsung-Yi Lin</title>
      <link href="/post/eb34d552.html"/>
      <url>/post/eb34d552.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>目前准确率最高的目标检测方法主要是基于两阶段的方法，第一阶段筛选候选目标，第二阶段对候选目标做进一步筛选，包括分类及精确的定位。而一阶段的目标检测方法则是在密集的候选目标上做分类和定位。虽然一阶段的方法会更快更简单，但是准确率与两阶段的方法比还是差很远。在这篇论文中，作者探究了为啥会出现这种情况。作者发现，目标检测方法在训练的时候，前景和背景的数量极度的不平衡，造成了训练上的困难。为了应对这种困难，在两阶段的方法中，首先在第一阶段的时候通过RPN网络将大部分的背景去除掉，然后在第二阶段的时候再通过采样策略，如控制前景背景的比例为1：3或者是使用OHEM，从而大大减轻了样本的不平衡；而对于一阶段方法，候选目标很多，容易区分的样本占比更大，类似的采样策略比如bootstrapping和HEM作用没那么大。为此作者针对这种情况，对交叉熵损失函数进行改进，提出了在训练过程中能够自动关注困难样本的损失函数Focal Loss，Focal Loss其实就是可以根据样本难度自动分配权重的损失函数，越容易区分的样本权重越小,相对的困难样本权重越大。最后作者设计了一个单阶段的目标检测模型RetinaNet，并采用Focal Loss对其进行训练。该模型速度上跟一阶段模型一致，但是能达到比当前最先进的两阶段方法更高的准确率。<br>代码<a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">https://github.com/facebookresearch/Detectron</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><p>作者针对一阶段的目标检测方法在训练过程中出现的极度的正负样本不平衡问题，对交叉熵损失函数进行改进，提出了在训练过程中能够自动关注困难样本的损失函数focal loss，focal loss其实就是可以根据样本难度自动分配权重的损失函数，越容易区分的样本权重越小，相对的困难样本权重越大。</p><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-Focal-Loss"><a href="#3-1-Focal-Loss" class="headerlink" title="3.1 Focal Loss"></a>3.1 Focal Loss</h2><img src="/post/eb34d552/4287D108-7CE9-4DD4-9A55-CC75C4E562CC.png"><p>Foacl Loss是在交叉熵loss的基础上引入了根据样本困难程度对样本权重进行自动变更的机制，所以这里先从普通交叉熵loss开始逐步引入focal loss。</p><h3 id="3-1-1-普通交叉熵loss"><a href="#3-1-1-普通交叉熵loss" class="headerlink" title="3.1.1 普通交叉熵loss"></a>3.1.1 普通交叉熵loss</h3><p>普通的二值交叉熵loss可以表示为：<br><img src="/post/eb34d552/E99730D9-192D-47B6-B22A-DDA0589CB150.png"><br>这里的类别标签为+1和-1，$p$为类别$y=+1$的概率值，为了表示方便，作者定义$p_t$为：<br><img src="/post/eb34d552/8F9B61E4-6237-408E-824B-0EC940C573CC.png"><br>则交叉熵loss可以简单写为：<br><img src="/post/eb34d552/D829EC0A-EB2C-4412-B474-74CD68461F3C.png"><br>在Figure1中蓝色曲线是普通的交叉熵loss，可以看出对于容易区分的样本，产生的loss还是比较大的，如果这样的样本很多，则整体的loss会被大量这种容易区分的样本所占据。</p><h3 id="3-1-2-加权交叉熵loss"><a href="#3-1-2-加权交叉熵loss" class="headerlink" title="3.1.2 加权交叉熵loss"></a>3.1.2 加权交叉熵loss</h3><p>为了处理类别不平衡的问题，通常会在交叉熵loss中引入一个权重因子，该权重因子$a$数值在0到1之间，这里类别+1分配的权重为$a$,则类别-1分配的权重为$1-a$。一般情况下在正样本偏少时$a$则相对要选择大一些。这里为了方便表示，类似$p_t$，也引入一个标记$a_t$。因此，$a-CE$ Loss可以表示为：<br><img src="/post/eb34d552/BFF8D0EC-5B2B-4C29-BDF8-4E6B675C298D.png"></p><h3 id="3-1-3-Focal-Loss"><a href="#3-1-3-Focal-Loss" class="headerlink" title="3.1.3 Focal Loss"></a>3.1.3 Focal Loss</h3><p>在类别不平衡的情况下，普通的交叉熵loss会被大量容易区分的样本所占据。而加权交叉熵loss虽然可以处理类别不平衡问题，但是它没办法区分困难样本和容易区分的样本。而Focal Loss则可以根据样本的困难程度调整对应的权重。为了达到这个目的，作者在交叉熵loss上加入了一个调节因子$(1-p_t)^r,r&gt;=0$,这里的r是可以调整的,因此Focal<br> Loss可以表示为：<br><img src="/post/eb34d552/24E5F91E-B5B8-4FD7-A25B-32062EA6B7AF.png"><br> Figure1给出了不同$r$下各个概率值下的Focal Loss的变化情况，从中可以看出Focal Loss有两个性质：</p><ul><li><p>1）当样本被误分类并且$p_t$较小的情况下，这个调节因子会接近1,这时候loss并没有受到影响，而当$p_t$逐渐接近1的时候，这个调制因子会接近于0，这时候容易区分的样本的重要性会逐渐降低。</p></li><li><p>2）随着$r$的增大，调节因子的影响会越来越大，会有更大范围的概率值接收一个较小的调节因子，这样容易区分的样本权重就越小，相对的困难样本的重要程度不断的提升。</p></li></ul><p>在实际应用中，更多会使用如下的加权Focal Loss形式。<br><img src="/post/eb34d552/4A7ADA5F-7843-4517-AA5E-E5B4F3004B86.png"><br>加权的Focal  Loss会比不加权的情况下效果要好一些。最后作者发现在实现的时候，组合sigmoid去计算$p_t$可以在计算focal loss的时候，数值更加稳定。</p><h3 id="3-1-4-类别不平衡情况下模型的初始化"><a href="#3-1-4-类别不平衡情况下模型的初始化" class="headerlink" title="3.1.4 类别不平衡情况下模型的初始化"></a>3.1.4 类别不平衡情况下模型的初始化</h3><p>通过将最后一层卷积层的bias设置为$-log((1-\pi)/(\pi))$，其输入到sigmoid函数后会得到$\pi$，这个$\pi$即为开始的时候稀有类的概率，通过设置$\pi$为较小的值，可以让模型在开始训练的时候更关注稀有类。这种设置对于在开始训练的时候存在大量负样本占据主导作用情况下很有帮助，可以加快模型的训练。本文作者将其设置为0.01。</p><h2 id="3-2-RetinaNet-Detector"><a href="#3-2-RetinaNet-Detector" class="headerlink" title="3.2 RetinaNet Detector"></a>3.2 RetinaNet Detector</h2><img src="/post/eb34d552/12739FBE-97E0-4787-850A-318EEF33B473.png"><h3 id="3-2-1-网络结构设置"><a href="#3-2-1-网络结构设置" class="headerlink" title="3.2.1 网络结构设置"></a>3.2.1 网络结构设置</h3><ul><li>backbone<br>使用FPN作为backbone，并构建从$P_3$到$P_7$的金字塔层，这里$P_l$层金字塔的分辨率是输入的$1/P_l$倍，每个金字塔层的channel为256。这里作者采用FPN作为backbone，主要是因为只用最后一层特征效果较差。</li><li>anchor设置<br>anchor大小为$32^2$到$512^2$对应$P_3$到$P_7$层金字塔，每一层金字塔层anchor的宽高比为$\{1:2,1:1,2:1 \}$，为了达到更加准确的定位，作者在每一层金字塔上将原来的anchor数增加到3个，大小设置为$\{ 2^0 ,2^{1/3}, 2^{2/3} \}$。所以在每一层金字塔层上共有A=9个anchors，所有的anchor跨过尺度范围为32到813。每个anchor被分配了一个长度为K的one-hot向量，用于目标的分类和一个4维向量用于目标的定位。当anchor与ground-truth的IOU大于0.5时，该anchor为正样本，当IOU在0到0.4之间时，该anchor为负样本，当IOU在0.4到0.5之间则被忽略。</li><li>分类子网<br>分类子网是一个FCN网络并且被所有金字塔层所共享。该FCN包括4个3x3通道数为256的卷积层，每个卷积层的激活函数都为ReLU；然后接一个3X3，通道数为KA的卷积层。K为目标类别数，A为anchor个数；最后再接一个sigmoid激活函数。</li><li>回归子网<br>回归子网与分类子网类似，只是在最后的卷积层只使用了4A个channel，并且不加sigmoid激活函数。这里作者使用的是类别无关的bbox回归器。这里分类子网和回归子网权重不共享。</li></ul><h3 id="3-2-2-训练和测试的参数设置"><a href="#3-2-2-训练和测试的参数设置" class="headerlink" title="3.2.2 训练和测试的参数设置"></a>3.2.2 训练和测试的参数设置</h3><ul><li>测试过程<br>整个模型由ResNet-FPN backbone，分类子网和bbox回归子网构成。推断过程只需要一个简单的前向传播过程。为了提高速度，作者只对每个金字塔层预测的置信度大于0.05的前1000个bbox进行解码；然后将所有的候选合并并进行阈值为0.5的nms抑制，得到最终的结果。</li><li>focal loss<br>在训练分类子网的时候，作者使用了focal loss。作者发现在实际应用中$r=2$可以取得比较好的结果，并且当$r=[0.5,5]$区间时，效果比较稳定。在训练过程中focal loss被应用到图片中的所有约100k个anchors中,这与启发式采样和OHEM不同，它们只是选择了一部分数据用于训练。最终的loss为所有anchor的focal loss之和，除以被分配为正样本的anchor数。这里作者没有除以所有的anchor，主要是因为很多的负样本都很容易区分，focal loss几乎可以忽略。作者发现将r设置为2，将a 设置为0.25表现效果是比较好的。</li><li>Initialization<br>作者先使用ImageNet1k数据对ResNet-50-FPN和ResNet-101-FPN进行预训练；然后对于新加入的卷积层，除了分类子网的最后一层，都使用0均值标准差为0.01的高斯分布进行初始化，并将bias设置为0；对于分类子网最后的卷积层作者将bias设置为$b=-log((1-\pi)/\pi)$，则在接入一个sigmoid操作后，输出的概率就为$\pi$,这里的$\pi$表示的是在开始训练的时候，每个anchor都有pi的置信度被标记为前景,这里作者将pi设置为0.01，这样对于大量的负样本带来的影响就比较小，loss会更加关注正样本。这个设置方式有助于前几个epoch训练的稳定。</li><li>Optimization<br>在8GPU上使用同步SGD进行训练。每个GPU2张图片，每个minibatch16张图片。所有的模型训练90k个迭代，起始学习率为0.01，在第60k次迭代变为0.001，第80k次迭代变为0.0001。数据增强只使用水平翻转。权重衰减系数为0.0001，动量为0.9.训练的loss为focal loss和smooth L1 loss。训练时间再10到35个小时之间。</li></ul><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>使用COCOtrainval-135k数据集进行训练，minival-5k数据集进行验证。测试指标主要是COCO-AP。</p><h2 id="4-1-验证不同设置的影响"><a href="#4-1-验证不同设置的影响" class="headerlink" title="4.1 验证不同设置的影响"></a>4.1 验证不同设置的影响</h2><img src="/post/eb34d552/2C98FE21-6BD8-4170-ACB8-0E7EBE3FAA48.png"><img src="/post/eb34d552/6B3D6F6F-3837-406A-802B-456EB72E3B81.png"><p>从上图可以看出随着r越来越大，训练的时候会越来越关注困难负样本，而对于正样本的影响则是比较小的。</p><h2 id="4-2-与当前最先进方法的对比"><a href="#4-2-与当前最先进方法的对比" class="headerlink" title="4.2 与当前最先进方法的对比"></a>4.2 与当前最先进方法的对比</h2><img src="/post/eb34d552/4549D529-66D8-4A0C-9AD7-5B86DA957EC6.png"><img src="/post/eb34d552/C3F81C5E-7CF1-49E4-97AA-71B23E8689A5.png">]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Focal Loss </tag>
            
            <tag> RetinaNet </tag>
            
            <tag> one-stage Detector </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Mask R-CNN(ICCV2017)Kaiming He</title>
      <link href="/post/f80ddd8e.html"/>
      <url>/post/f80ddd8e.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>在这篇论文中，作者对faster rcnn进行扩展，通过在faster rcnn的ROI区域上加多一个分支来做实例分割。加多的一个分支是一个轻量级的分支，使得mask rcnn每秒能跑5帧，而且mask rcnn可以很容易泛化到其他任务，比如人体关键点识别。作者利用mask rcnn在coco的实例分割，目标检测和行人关键点识别3个任务上都取得了最好的结果，在单模型情况下都比其他最先进的模型要好。mask rcnn可以作为实例识别的一个基本框架，在目标检测之后对每个实例进行进一步的识别。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ol><li>之前faster rcnn没有设计成像素对齐的方式，主要是RoIPool层只是对ROI区域进行粗糙的空间量化来提取特征，没有精确对齐ROI区域，这样对实例分割结果影响很大。为此作者提出了一个像素对齐的RoIAlign层来保留精确的空间位置信息。虽然这只是一个小的改动，但是对于实例分割的效果影响却很大。</li><li>作者发现对分类预测和实例分割两个任务进行解耦这一步很关键。作者独立地为每一类目标预测一个2值mask，然后将类别预测放到了RoI分类那个分支。</li></ol><h2 id="重要结论"><a href="#重要结论" class="headerlink" title="重要结论"></a>重要结论</h2><ol><li>更优的网络可以带来最终效果的提高（并不是说所有的任务都可以直接从更优的网络结构中获益的，但是该任务可以）见Table2a；</li><li>每个类别使用一个2值mask要比多类别mask要好,见Table2b。只使用一个类别无关的mask与每个类别使用一个2值mask差异不大；</li><li>RoIAlign对实例分割具有很大的提升，见Table2c。特别是当使用的特征的stride更大时，更加明显见Table2d。</li><li>mask采用卷积的形式要比采用MLP的形式要好Table2e。</li><li>RoIAlign也有助于目标检测；</li><li>多任务学习提高了目标检测的效果；</li><li>Mask R-CNN拉近了目标检测和实力分割的性能差异。</li><li>多任务学习可以提高关键的估计的性能。但是会略微影响实例分割和目标检测的效果。</li><li>ROIAlign对于关键点识别更加重要。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><img src="/post/f80ddd8e/B79D828B-9DDD-4BB6-8FF2-977F53BBB504.png"><p>mask R-CNN也采用与Faster R-CNN类似的两阶段方式。第一阶段利用RPN网络预测候选目标；第二阶段并行预测ROI区域的类别、精确的位置以及二值mask。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>这里的Loss包括3部分，分别是分类loss、回归loss和二值mask loss。</p><script type="math/tex; mode=display">L = L_{cls}+L_{box}+L_{mask}</script><p>分类loss和回归loss与Faster R-CNN一样。这里输出的mask是每个类别对应一个二值mask，每个mask的大小为$m \times m$，所以每个ROI区域对应$Km^2$维输出，然后再接一个逐像素的sigmoid，最终的mask loss就是平均的二值交叉熵 loss。在训练的时候，只有grouth truth类别对应的mask，才用来训练，其它mask不用于训练。而在预测的时候，则是根据分类分支的类别来选择对应的mask。这种方式与通常的语义分割不同，通常的语义分割使用softmax和多分类的交叉熵去预测每个像素是属于哪个类别的，而这种方式则是将类别与分割预测进行了解耦，作者也通过实验验证了这种方式在实例分割任务中表现更好。</p><h2 id="mask的表示"><a href="#mask的表示" class="headerlink" title="mask的表示"></a>mask的表示</h2><p>这里作者采用的是卷积的形式，最终输出为$m \times m$的卷积特征图，而不是采用全连接的形式。作者也通过实验验证了通过卷积的形式要比全连接的形式要好。</p><h2 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h2><p>原本的ROIPool是利用RPN网络的预测结果，将预测的位置映射到要提取特征的特征图上，然后再根据输出大小，将特征图分成nxn份，最后在每个bin上进行maxpool或者avepool。这里映射的方式只是粗略的进行取整计算，不够精确，这种方式对于分类可能没有影响，因为分类对于这种小的偏移不敏感，但是对于分割，则会很大影响分割的精度，特别是在特征图缩减较大的时候，在特征图上小的偏移会在原图上产生一个较大的偏移。为此作者提出了RoIAlign层，将ROI的位置精确的映射到要提取的特征图上。</p><ul><li>首先作者将原本取整的运算变成了精确的浮点运算，来找到ROI在特征图上的精确位置；</li><li>然后根据输出大小将提取的特征图精确地划分成nxn份；</li><li>接着根据采样个数，bin中的像素点，利用双线性差值得到采样点的值；</li><li>最后再利用maxPool或者avePool得到每个bin的值。</li></ul><p>最后作者通过实验证明了RoIAlign确实要比RoIWarp和RoIPool好。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><img src="/post/f80ddd8e/367F0D86-96FF-4D79-95E1-D90FD96D38BF.png"><p>backbone作者主要使用了ResNet-50-C4和ResNet-FPN。这两个backbone对应的head见Figure3.</p><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><h3 id="Trainnig："><a href="#Trainnig：" class="headerlink" title="Trainnig："></a>Trainnig：</h3><ul><li>训练mask RCNN(非RPN)的时候，如果候选框与ground-truth的IOU大于0.5则为正样本，否则为负样本。</li><li>$L_{mask}$只定义在正样本上，<strong>mask目标是RoI区域和grouth truth交叉的部分</strong></li><li>超出图像的候选目标都不用于训练</li><li>将图像的短边resize到800</li><li>每个mini-batch每个GPU包括2张图片，每张图片有N个候选框，正负样本的比例为1：3。对于ResNet-50-C4backbone作者使用N=64，对于ResNet-FPN作者使用N=512。总共使用8个GPU，因此每个mini-batch有8张图片。</li><li>总共跑160k个迭代，其实学习率为0.02，到120k个迭代后降为0.002.</li><li>权重衰减系数为0.0001，动量为0.9</li><li>RPN anchors包括5个尺度，3个宽高比。</li><li>这里为了方便比较，<strong>作者并没有让RPN和mask RCNN共享权重</strong>，但是他们是可以共享的。</li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>测试的时候对于backbone为ResNet-50-C4的网络，proposals数是300，对于FPN,proposals数为1000。接着再进行NMS操作；</li><li>然后再取前100个目标，得到它们的mask，这里只取了预测类别对应的mask；</li><li>最后mask被resize到RoI区域的大小，并根据阈值为0.5，进行二值化。</li><li>由于这里只取top100来计算mask，所以只增加了一点点时间，约占对应的faster rcnn模型的20%</li></ul><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><h2 id="4-1-实例分割和目标检测结果"><a href="#4-1-实例分割和目标检测结果" class="headerlink" title="4.1 实例分割和目标检测结果"></a>4.1 实例分割和目标检测结果</h2><h3 id="4-1-1-实例分割"><a href="#4-1-1-实例分割" class="headerlink" title="4.1.1 实例分割"></a>4.1.1 实例分割</h3><p>数据集主要使用了COCO数据集，使用trainval-135k作为训练集，minival-5k作为验证集。评价指标包括$AP$, $AP_{0.5}$, $AP_0.75$, $AP_S$, $AP_M$, $AP_L$. 这里AP的是mask IoU。<br><img src="/post/f80ddd8e/9CF11881-8091-4F11-A7CC-C5150A6214E0.png"><br><img src="/post/f80ddd8e/FC559A91-401A-4E07-8BD9-6EC3F596617B.png"><br><img src="/post/f80ddd8e/2DD58C70-B281-4C7B-A187-988F518DCA68.png"><br><img src="/post/f80ddd8e/B20C1AED-F7F3-482A-A81C-DFED5EA2C9D6.png"><br>从结果来看mask R-CNN在实例分割任务上要比最先进的方法好很多。<br><img src="/post/f80ddd8e/Snipaste_2020-01-15_22-07-57.jpg"></p><h4 id="重要结论-1"><a href="#重要结论-1" class="headerlink" title="重要结论"></a>重要结论</h4><ol><li>更优的网络可以带来最终效果的提高（并不是说所有的任务都可以直接从更优的网络结构中获益的，但是该任务可以）见Table2a；</li><li>每个类别使用一个2值mask要比多类别mask要好,见Table2b。只使用一个类别无关的mask与每个类别使用一个2值mask差异不大；</li><li>RoIAlign对实例分割具有很大的提升，见Table2c。特别是当使用的特征的stride更大时，更加明显见Table2d。</li><li>mask采用卷积的形式要比采用MLP的形式要好Table2e。</li></ol><h3 id="4-1-2-目标检测"><a href="#4-1-2-目标检测" class="headerlink" title="4.1.2 目标检测"></a>4.1.2 目标检测</h3><img src="/post/f80ddd8e/B6A9ECA3-FDDE-4CEB-A94C-AF3D5C4DBBEA.png"><p>从结果来看mask R-CNN在目标检测任务上要比最先进的方法好很多。</p><h4 id="重要结论-2"><a href="#重要结论-2" class="headerlink" title="重要结论"></a>重要结论</h4><ol><li>RoIAlign也有助于目标检测；</li><li>多任务学习提高了目标检测的效果；</li><li>Mask R-CNN拉近了目标检测和实力分割的性能差异。</li></ol><h3 id="4-1-3-运行时间"><a href="#4-1-3-运行时间" class="headerlink" title="4.1.3 运行时间"></a>4.1.3 运行时间</h3><h4 id="Inference-1"><a href="#Inference-1" class="headerlink" title="Inference"></a>Inference</h4><p>作者将RPN和mask R-CNN进行特征共享，并按照faster RCNN的方式进行训练。发现最后的结果与不共享基本一样。并且在使用ResNet-101-FPN作为backbone时推断时间为195ms（Tesla M40 GPU)+15ms（cpu）；使用ResNet-50-C4则约为400ms。显然ResNet-101-FPN更优更快。</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>在COCO, trainval-135k数据集上，使用8个GPU(每个mini-batch16张图片0.72s),ResNet-50-FPN只需要32个小时；ResNet-101-FPN只需要44个小时。</p><h2 id="4-2-人的关键点估计"><a href="#4-2-人的关键点估计" class="headerlink" title="4.2 人的关键点估计"></a>4.2 人的关键点估计</h2><h3 id="实现细节-1"><a href="#实现细节-1" class="headerlink" title="实现细节"></a>实现细节</h3><p>作者将关键点位置以 one-hot的形式表示，并使用mask R-CNN去预测K个masks,每个mask表示一种关键点类型（比如左肩膀，右手肘等），即训练的目标是一个one-hot的$m\times m$的二值mask,里面只有一个位置被标记为前景，其它都为背景。训练时使用的loss,是K维的softmax with loss（使用softmax可以确保一个位置只有一种类型的关键点）,每个ROI共$m^2$个样本。</p><p>关键点识别的Head architecture与以FPN作为backbone的实例分割任务的Head architecture类似。它主要由8个核为3x3通道为512的卷积层和一个转置卷积层以及一个2x的双线性差值层构成，最后的输出为56X56。作者发现对于关键点识别，最后输出的分辨率对关键点定位影响很大。</p><p>模型主要使用了COCO的trainval-135k数据集中包含关键点的数据来训练，由于数据集比较小，所以作者使用了尺度增强方式，将图片随机缩放到[640,800]，测试的时候则使用尺度为800来测试。前60k次学习率为0.02，60k到80k学习率为0.002，80k后学习率为0.0002，共训练90k次迭代。在使用nms的时候作者使用的IOU阈值为0.5，其它细节与mask-rcnn一样。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><img src="/post/f80ddd8e/CC1B34DF-4CFF-40C2-8845-08589077EF34.png"><p>从结果来看mask R-CNN在关键点估计任务上要比最先进的方法好很多。</p><h3 id="重要结论-3"><a href="#重要结论-3" class="headerlink" title="重要结论"></a>重要结论</h3><img src="/post/f80ddd8e/56AC9B7F-F94E-444C-92FB-44C2B1F58CEF.png"><ol><li>多任务学习可以提高关键的估计的性能。但是会略微影响实例分割和目标检测的效果。<img src="/post/f80ddd8e/EB9D2388-AFA7-493F-B875-520EF80D6C95.png"></li><li>ROIAlign对于关键点识别更加重要。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 实例分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mask-RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017_Feature Pyramid Networks for object detection(CVPR2017)Tsung-Yi Lin.pdf</title>
      <link href="/post/c8c7a90a.html"/>
      <url>/post/c8c7a90a.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>识别大范围尺度的目标，是计算机视觉的一个基本挑战。特征金字塔是目标检测为了能够适应不同的尺度，常用的一个基本组件。图像金字塔的优势是特征化每一层图像，生成一个多尺度的特征表示，并且每一层特征表示都具有很强的语义信息。深度卷积神经网络对于尺度的变换更加鲁棒，但是并不是说它就不需要使用图像金字塔，在coco比赛上面取的最好效果的几个目标检测结果，都是采用了多尺度的测试方式。但是在深度卷积神经网络中对每一层图像进行特征化具有明显的局限性。这样会使推断时间增加，使得很难应用到实际场景中，并且训练过程中采用特征金字塔也是不可取的，大的金字塔层需要很大的显存。  图像金字塔并不是唯一的方式去计算多尺度强语义的特征表示。深度卷积网络逐层去计算特征层次，并且使用降采样层，这些特征层本身就具有多尺度金字塔结构。这些网络结构中的特征层提供了不同空间分辨率的特征图，但是由于处在不同的深度，它们表达的语义强度不同。高分辨率的特征图只能表达更低层的特征，语义强度不够，从而影响到目标检测的效果。在这篇论文中，作者使用了一个嵌入在深度卷积神经网络中的金字塔结构，能够利用多种分辨率的强语义特征来对目标进行识别并且能够在引入特征金字塔结构的情况下，不会带来过多额外的计算复杂度和内存的消耗。该结构采用自顶向下的方式并引入了skip connections，用于为各个尺度构建深层语义特征图。作者将该结构称为特征金字塔网络（FPN），这个体系结构作为通用的特征提取器在多个应用中带来了性能的提升，如目标检测和实例分割。作者将FPN结构嵌入到基本的faster rcnn中，在coco检出数据集上取得了单模型最好的结果，超过了所有现存的其它模型。另外作者提到的方法，在GPU上每秒可跑5帧，是多尺度目标检测在应用中的一个比较好的解决方案。<br>代码<a href="https://github.com/caffe2/caffe2" target="_blank" rel="noopener">https://github.com/caffe2/caffe2</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-创新点"><a href="#2-1-创新点" class="headerlink" title="2.1 创新点"></a>2.1 创新点</h2><p>作者利用卷积网络金字塔形状式的特征层来构建一个在各个尺度上都有强语义的特征金字塔。为了达到这个目的，作者使用了top-down 和skip connection结构，组合了低分辨率，语义强的特征和高分辨率语义弱的特征，使得每一层特征图都具有强的语义表达能力。作者通过该方式，在不牺牲表达能力，速度和内存的情况下，构建了一个网络内的特征金字塔，替换掉图像金字塔，并在多个任务上准确率和速度都得到了提高。</p><h2 id="2-2-提到的方法与先前方法的区别"><a href="#2-2-提到的方法与先前方法的区别" class="headerlink" title="2.2 提到的方法与先前方法的区别"></a>2.2 提到的方法与先前方法的区别</h2><p>下图C是SSD采用的方式，虽然它采用了多种分辨率的特征，但是它使用的是较为底层的特征，表达的语义信息不够强。并且弃用了更高层的语义信息作者通过实验验证高层语义信息对于检测小目标是很重要的。如下图d作者提到的方法，正是利用skip connection和top down结构，组合了低分辨率，语义强的特征和高分辨率语义弱的特征。该结构在每一层都具有丰富的语义信息，并且只需单尺度输入，训练和测试速度很快，不需要消耗额外的显存。<br><img src="/post/c8c7a90a/CEAE39B6-CF62-4B87-BE61-E9A52575BED0.png"><br>下图与作者提出的结构类似，但是该结构只是生成单个更细粒度的高层特征图，作者也通过实验验证了提到的方式更加优越。<br><img src="/post/c8c7a90a/64AB35B7-8160-42CA-A80C-ECB41B3A26DC.png"></p><h2 id="2-3-重要结论"><a href="#2-3-重要结论" class="headerlink" title="2.3 重要结论"></a>2.3 重要结论</h2><ol><li>尽管深度学习具有很强的表达能力，而且对尺度变化较为鲁棒，但是显式引入金字塔表示，对于处理多尺度问题仍然是很关键的。</li><li>高层语义信息和分辨率都对结果有影响。</li><li>使用bottom-up 金字塔效果比top-down要差很多，因为低层特征图语义信息不够强。</li><li>通过skip connetion将细节信息与抽象信息结合带来的效果提升明显。</li><li>对每层金字塔进行预测要比单层预测效果好，并且单单增加anchor个数，并不能够带来性能的提高。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><p>提到的结构主要包括了：bottom-up pathway，top-down pathway和skip connetions</p><ul><li>bottom-up pathway<br>这里的bottom-up pathway其实就是前向传播，也就是模型使用的backbone，主要是计算由feature map构成的特征层，这些特征层的尺度一般是2倍差距。这里作者把feature map大小一致的层构成的网络看做是一个网络阶段，也即一个金字塔层；然后选择每一个金字塔层最后的feature map作为该金字塔层的特征图，最后采用skip connection 与高层 feature map进行连接。<br>在这里作者使用的backbone是ResNets，并将每个阶段的最后的residual block作为每个金字塔层的特征，并将它们定义为{C2,C3,C4,C5}，对应的stride分别为{4,8,16,32}。由于考虑到内存占用的问题，这里作者没有使用stride为2的金字塔层。</li><li>Top-down pathway and lateral connetions<br>如下图，top-down pathway主要通过将语义信息更强但是分别率较低的层进行upsample来得到分别率更大语义信息也更强的金字塔层，并将得到的特征与bottom-up pathway对应分别率的金字塔层通过skip connection进行组合，得到语义信息更加强的特征。bottom-up特征图虽然语义信息不够强，但是它保留了更多的位置信息。bottom-up特征图在与高层特征图组合的时候采用了1x1的卷积层，来使两个feature map大小一致，然后再进行逐元素相加，最后在得到的特征图上，在使用一个3X3的卷积层得到最终的特征图。这个最终的feature map集和作者将其表示为{P2,P3,P4,P5}与{C2,C3,C4,C5}相对应。最后的回归器和分类器是各个金字塔层共享的，所以最后作者将每一层金字塔层的feature map设置为256。<img src="/post/c8c7a90a/4CA517D7-123D-41B9-B1DF-A76B193A5AA2.png"></li></ul><h2 id="3-1-在RPN网络中应用特征金字塔"><a href="#3-1-在RPN网络中应用特征金字塔" class="headerlink" title="3.1 在RPN网络中应用特征金字塔"></a>3.1 在RPN网络中应用特征金字塔</h2><p>作者将RPN网络（3x3卷积层+2个分开的1X1卷积层，一个用于分类，一个用于回归）应用到各个金字塔层。由于RPN网络是各个金字塔层共享的，它会对所有金字塔层的各个位置进行识别，所以作者对每个金字塔层只使用了一个尺度的anchor，但是使用了3种宽高比。<br>当目标与anchor具有最大的IOU,或者IOU大于0.7时，该anchor被分配为正样本，当与所有目标的IOU小于0.3时，该anchor被分配为负样本。 其它配置与原本faster rcnn一致。<br>作者也尝试过让RPN网络不在多个金字塔层共享，但是得到的结果与共享是类似的。这说明各个金字塔层消除了尺度的影响，有了类似的语义信息。</p><h2 id="3-2-在fast-R-CNN中应用特征金字塔"><a href="#3-2-在fast-R-CNN中应用特征金字塔" class="headerlink" title="3.2 在fast R-CNN中应用特征金字塔"></a>3.2 在fast R-CNN中应用特征金字塔</h2><p>在利用RPN网络获取到候选目标之后，到哪一层金字塔层去提取特征呢？对于在图像上宽为w，高为h的目标，这里作者采用了如下方式选择金字塔层。<br><img src="/post/c8c7a90a/3E7B54A1-4315-4C8C-86A4-A1A0572378DD.png"><br>这里$k_0$是一个超参，表示$w\times h=224^2$大小的ROI应该在哪一层提取特征。这里作者将$K_0$设置为4。<br>fast rcnn的头部是所有特征层共享的，这里作者使用ROIpooling从每个金字塔层中提取7X7大小的特征，然后再接两个1024的全连接层。</p><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><p>作者在包含80个类别的COCO检出数据上进行试验。训练的时候使用了80K的训练集和35K的验证集的并集进行训练，并在5K的小验证集上进行测试，作者也在test-std上进行了测试，该测试集没有公开标签。<br>所有的网络的backbones采用Imagenet-1k进行预训练。这里作者使用了预训练的ResNet-50和ResNet-101</p><h2 id="4-1-目标检测"><a href="#4-1-目标检测" class="headerlink" title="4.1 目标检测"></a>4.1 目标检测</h2><h3 id="4-1-1-RPN"><a href="#4-1-1-RPN" class="headerlink" title="4.1.1 RPN"></a>4.1.1 RPN</h3><h4 id="4-1-1-1-评估方式和参数设置"><a href="#4-1-1-1-评估方式和参数设置" class="headerlink" title="4.1.1.1 评估方式和参数设置"></a>4.1.1.1 评估方式和参数设置</h4><p>这里作者采用的评价指标是COCO的评估方式即AR(Average Recall)和小，中，大目标的AR指标。也评估每张图片取100个proposals和1000个proposals的结果。</p><ul><li>预处理：将所有输入图像resize成短边为800像素的图片。</li><li>minibatch：使用8张卡，每个gpu一个minibath跑2张图片,每张图片256个样本。</li><li>优化器：采用同步SGD进行优化。</li><li>正则化：权重衰减为0.0001，动量为0.9。</li><li>学习率调整策略：在开始的30k个mini-batch学习率设置为0.02，剩余的10kmini-batch设置为0.002。</li><li>对于所有的RPN实验，作者将超出图像的anchor也一起训练。</li></ul><h4 id="4-1-1-2-实验结果"><a href="#4-1-1-2-实验结果" class="headerlink" title="4.1.1.2 实验结果"></a>4.1.1.2 实验结果</h4><img src="/post/c8c7a90a/9242A3E6-05D9-400E-A263-DE9B1E8D8087.png"><p>这里a是使用了$C_4$的特征，b是使用了$C_5$的特征，所有的都使用了5种尺度的anchor $[32^2,64^2,128^2,256^2,512^2]$，这里$AR^{1k}$表示测试的时候每个图片proposals数为1000。</p><ul><li>与baseline相比<br>从a和b的结果来看，单个高层的特征图并不足够，因为b虽然语义更强，但是分辨率也更低了。<br>从a跟c的结果可以看出，使用FPN后效果提升明显，特别对小目标。</li><li>top-down结构的影响<br>从d中结果可以看到，效果比FPN差距很大，这可能是低层特征层没有足够的语义信息。另外共享head和不共享效果差不多。</li><li>skip connection的影响<br>通过skip connetciont将细节信息与抽象信息结合带来的效果的提升明显。</li><li>对每层金字塔层进行预测的影响<br>对每层金字塔进行预测要比单层预测效果好，而且使用最后一层金字塔层也增加了anchor数，说明通过增加anchor个数，并不能够带来性能的提高。</li></ul><h3 id="4-1-2-fast-faster-R-CNN"><a href="#4-1-2-fast-faster-R-CNN" class="headerlink" title="4.1.2 fast/faster R-CNN"></a>4.1.2 fast/faster R-CNN</h3><h4 id="4-1-2-1-评估方式和参数设置"><a href="#4-1-2-1-评估方式和参数设置" class="headerlink" title="4.1.2.1 评估方式和参数设置"></a>4.1.2.1 评估方式和参数设置</h4><p>这里作者采用的评价指标是COCO-style的评估方式即AR(Average Recall)，小，中，大目标的AR指标以及PASCAL-style的评估方式，即在IOU的阈值为0.5的情况下的AP。测试的时候每张图片1000个proposals。</p><ul><li>预处理：将所有输入图像resize成短边为800像素的图片。</li><li>minibatch：使用8张卡，每个gpu一个minibath跑2张图片,每张图片512个样本。</li><li>优化器：采用同步SGD进行优化。</li><li>正则化：权重衰减为0.0001，动量为0.9。</li><li>学习率调整策略：在开始的60k个mini-batch学习率设置为0.02，剩余的20kmini-batch设置为0.002。</li><li>对于所有的RPN实验，作者将超出图像的anchor也一起训练。</li></ul><h4 id="4-1-2-2-实验结果"><a href="#4-1-2-2-实验结果" class="headerlink" title="4.1.2.2 实验结果"></a>4.1.2.2 实验结果</h4><h5 id="RPN与fast-RCNN不共享backbone"><a href="#RPN与fast-RCNN不共享backbone" class="headerlink" title="RPN与fast RCNN不共享backbone"></a>RPN与fast RCNN不共享backbone</h5><img src="/post/c8c7a90a/B95F5BF3-C31F-4C70-ABFF-EAACD62BB886.png"><p>作者为了更好比较先前方法与当前提到的FPN网络在fast RCNN上的效果，将fast RCNN和RPN网络独立开来，不进行特征共享，并使用了基于FPN的RPN网络的候选结果进行训练。<br>从实验结果可以看到，FPN相较于之前resnet+faster rcnn AP提高了2个点。d和e表明移除top-down connetions或者移除skip connection带来的影响，fast rcnn与RPN网络具有相同的结论。f是只用最后一层特征来预测，从结果来看使用多层金字塔特征会比只用最后一层要好，但是优势并不明显，这里可能的原因是ROIpool对于区域的尺度不敏感。</p><h5 id="RPN与fast-RCNN共享backbone"><a href="#RPN与fast-RCNN共享backbone" class="headerlink" title="RPN与fast RCNN共享backbone"></a>RPN与fast RCNN共享backbone</h5><img src="/post/c8c7a90a/F01F6F27-6B29-4032-A4C6-F9070165FCAB.png"><p>Table3中* 为使用resnet101+faster rcnn的结果。a也是相同的模型，但是使用的配置参数不一样。主要的不同有1）原本输入的图片尺度为600，现在作者将其变成800 2）原本训练的时候，每张图片只使用了64个ROI,现在训练的时候每张图片使用了512个ROIs 3）原本anchor尺度只有4个，现在作者改为了5个<br> 3）在测试的时候每张图片只有300个proposals，现在每张图片有1000个proposals。通过设置上的变动，AP比原本提高了7.6个点；AP@0.5比原本提高了9.6个点。</p><h5 id="作者也比较了共享特征和不共享特征对结果的影响"><a href="#作者也比较了共享特征和不共享特征对结果的影响" class="headerlink" title="作者也比较了共享特征和不共享特征对结果的影响"></a>作者也比较了共享特征和不共享特征对结果的影响</h5><img src="/post/c8c7a90a/27E8311D-3222-49E8-A242-55045649D8FB.png"><p>从结果可以发现共享确实能够带来性能的提升，而且也大大提高运行速度。在特征共享的基础上，基于FPN的faster RCNN，使用resnet50作为baakbone推断的时候每张图片需要跑0.165秒，使用Resnet101则需要0.19秒。而当尺度的resnet50,如Table3(a)则需要跑0.32秒。</p><h5 id="与COCO比赛的冠军进行对比"><a href="#与COCO比赛的冠军进行对比" class="headerlink" title="与COCO比赛的冠军进行对比"></a>与COCO比赛的冠军进行对比</h5><img src="/post/c8c7a90a/4CA6FF30-EB76-474F-9C18-1A6F59A659E7.png"><p>从结果可以看到作者提到的方法具有很大的优势。并且作者也没有使用最近提出的其它改进的方法。</p><h2 id="4-2-实例分割"><a href="#4-2-实例分割" class="headerlink" title="4.2 实例分割"></a>4.2 实例分割</h2><img src="/post/c8c7a90a/E34FCFA0-6598-4C78-B742-98E5C26729F9.png"><p>这里作者沿用了DeepMask和sharpMask的框架。<br><img src="/post/c8c7a90a/E5134226-1F6F-42E5-92B2-B5CD354DC538.png"><br>从结果可以看出速度和准确率都有极大提升。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FPN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016_Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks(CVPR2016)shaoqing ren</title>
      <link href="/post/dcec0426.html"/>
      <url>/post/dcec0426.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>fast R-CNN将R-CNN中影响速度的因素基本上都解决了，使得检出速度得到了很大提升。但是仍然存在一个步骤影响检出速度，那就是region proposal。为此在这篇论文中，作者提出了RPN网络来解决region proposal速度慢的问题，该网络可以与检测网络共享特征，使得region proposals基本不耗费时间，平均每张图片5ms。RPN网络是一个全卷积神经网络，它同时预测当前候选框是否为目标以及对预定义的候选框位置进行修正得到更加精准的候选框。RPN可以进行端到端的训练去生成高质量的候选框。作者使用交替训练的方式，让RPN和fast RCNN共享特征，得到faster rcnn模型。最后作者使用faster rcnn达到更好的性能，并且在速度上有了极大的提升。faster rcnn在PASCAL VOC2007上mAP达到73.2%，在PASCAL VOC2012上mAP达到70.4%并且在此基础上能在GPU上每秒跑5帧。<br><a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="noopener">代码：https://github.com/ShaoqingRen/faster_rcnn</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ol><li>提出了RPN网络用于region proposals，并与fast rcnn共享网络结构，得到faster rcnn网络模型，使得目标检测的性能及速度大大提高。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-RPN网络"><a href="#3-1-RPN网络" class="headerlink" title="3.1 RPN网络"></a>3.1 RPN网络</h2><h3 id="3-1-1-RPN网络结构"><a href="#3-1-1-RPN网络结构" class="headerlink" title="3.1.1 RPN网络结构"></a>3.1.1 RPN网络结构</h3><img src="/post/dcec0426/DCA5A725-FA69-46D3-A1DA-9EEA96978446.png"><p>RPN网络的输入是任意尺度的图片，输出是一系列的候选目标，每个候选目标有对应的置信度。作者使用一个全卷积神经网络来建模这个过程。为了与Fast R-CNN共享特征，RPN的网络结构作者使用了与Fast R-CNN backbone网络相同的网络结构，并共享了底层的部分卷积层。ZF网络共享了前5层卷积层，VGG网络共享了前13层卷积层。<br>为了生成候选目标，作者在最后一层共享卷积层上，滑动一个小网络为每个位置生成候选目标。这个小网络全连接到nxn的滑动窗口，将每个滑动窗口映射到一个低维向量，ZF模型为256维，VGG模型为512维；然后再连接到两个全连接层，一个用于分类目标，一个用于定位目标。在论文中作者使用的滑动窗口大小为3。Figure1给出了RPN网络的示意图。这里全连接层在每个位置进行共享，可以很自然地通过使用一个nxn的卷积层然后再接入两个1x1的卷积层来实现，nxn的卷积层作者使用了ReLU激活函数。</p><h3 id="3-1-2-RPN网络使用的Anchors"><a href="#3-1-2-RPN网络使用的Anchors" class="headerlink" title="3.1.2 RPN网络使用的Anchors"></a>3.1.2 RPN网络使用的Anchors</h3><p>在RPN网络中每个滑动窗口预测k个候选框，每个候选框预测其是否包括目标还是只有背景，并定位其位置。因此每个位置分类分支预测2k个得分，回归分支预测4k个得分。预测的候选框是预定义的候选框(anchors)的一个相对偏移,每个anchor的中心位置与滑动窗口的中心相对。每个滑动窗口对应的anchors具有3个尺度和3个宽高比，共k=9个。对于大小为$W\times H$的feature map，其总共有$W\times H\times k$个anchors。</p><h3 id="3-1-3-RPN网络的Loss-Function"><a href="#3-1-3-RPN网络的Loss-Function" class="headerlink" title="3.1.3 RPN网络的Loss Function"></a>3.1.3 RPN网络的Loss Function</h3><p>RPN网络的分类任务，正样本有两类anchors：a)跟grounth-truth具有最大IOU的anchor；b)跟gronth-truth的IOU大于0.7的anchor。这里单个ground-truth可能会为多个anchor分配正标签。负样本为IOU小于0.3的非正例。大于0.3小于0.7的anchors则被忽略。则loss函数可定义为：<br><img src="/post/dcec0426/0DD257A4-0890-4667-8603-13D75150DAF5.png"><br>其中i表示一个mini-batch中的一个anchor，$p_i$是预测第i个anchor概率。${p_i}^·$是真实的标签，1为正样本，0为负样本。$t_i$是预测的参数化的bbox坐标，${t_i}^·$真实的正样本的bbox的参数化坐标。分类loss使用交叉熵loss；回归loss采用smooth L_1。这里${p_i}^·L_{reg}$表示回归loss只使用正样本忽略负样本。作者通过以下方式对坐标进行参数化得到回归使用的grouth truth：<br><img src="/post/dcec0426/41A3DE35-A1D6-4343-B79C-BA8A2C850469.png"><br>这里$x,y,w,h$表示两个中心坐标和宽高。这里$x,x_a,x^·$分别代表预测的box，anchor box和grouth truth box。</p><h3 id="3-1-4-RPN网络的训练"><a href="#3-1-4-RPN网络的训练" class="headerlink" title="3.1.4 RPN网络的训练"></a>3.1.4 RPN网络的训练</h3><ul><li>作者随机从每张图片中采样256个样本，其中正负样本的比例为1:1。如果正样本小于128个，则用负样本补充。</li><li>新的网络层权重，使用0均值0.01标准差的高斯分布进行随机初始化。其它层则使用在Imagenet上进行预训练的权重进行初始化。</li><li>对ZFnet的所有层进行训练，但是只对VGGNet conv3_1以上的层进行训练。</li><li>在PASCAL数据集上，前60k个mini-batch的学习率为0.001，后20k个mini-batch的学习率为0.0001。</li><li>动量参数为0.9，权重衰减系数为0.0005</li></ul><h2 id="3-2-通过4个步骤让RPN与Fast-RCNN共享权重"><a href="#3-2-通过4个步骤让RPN与Fast-RCNN共享权重" class="headerlink" title="3.2 通过4个步骤让RPN与Fast RCNN共享权重"></a>3.2 通过4个步骤让RPN与Fast RCNN共享权重</h2><p>这里作者并没有使用联和训练方式，而是使用了交替训练方式来让RPN和fast RCNN共享卷积层特征。<br>1.先利用ImageNet预训练模型来初始化RPN，然后fine-tune该网络用于region proposals任务。<br>2.使用ImageNet预训练模型来初始化RCNN，然后利用之前训练好的RPN网络预测得到的候选框口来构建样本，训练fast RCNN模型。<br>3.使用fast RCNN网络来初始化RPN，同时固定卷积层并fine-tune RPN网络的全连接层。到这里两个网络就同时共享了卷积层。<br>4.最后让卷基层的特征保持固定的情况下，来fine-tune Fast-Rcnn的全链接层。</p><h2 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h2><ul><li>训练和测试都使用短边re-scale为600的单尺度图片。</li><li>anchors采用3个尺度，$128^2，256^2，512^2$和3个宽高比，$1:1,1:2,2:1$。</li><li>跨过边界的anchor作者将他们忽略，不做训练。对于大小为1000X600的图片，所有的anchor约有20k个，将跨边界的去除掉后大概剩下6k个。如果跨边界的anchor不去掉，会引入很多困难的样本，使得训练无法拟合。但是测试的时候会使用整个RPN网络进行测试，这样的话会使用到跨边界的anchor，也会生成跨边界的候选目标，这些候选目标会被clip到边界。</li><li>在通过RPN网络得到候选之后，会使用NMS去过滤掉IOU高于0.7的候选,最后保留得分最高的2k个候选。最后使用这2k个候选来训练Fast R-CNN。但是测试的时候则不是使用2k个。</li></ul><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><p>作者在PASCAL VOC2007和PASCAL VOC2012数据集上对提到的方法的有效性进行验证。PASCAL VOC2007包括了5k张trainval图像和5k张test图像，共有20个类别。ZFnet选用5层卷积3层全连接的版本，VGG选用13层卷积和3层全连接的版本。评价指标为mAP。<br><img src="/post/dcec0426/273C5A9E-3213-4576-A7C9-DA1936209E08.png"><br>Table1给出了在不同proposal方法基础上使用Fast R-CNN来进行目标检测的性能。从Table1可以看出，RPN+ZF可以在只给出较少候选目标的情况下，达到比SS和EB更好的效果。在Table1中，作者也比较了不同变量对于目标检测的影响。作者发现：</p><ol><li>RPN和Fast RCNN共享特征会比不共享特征更好.</li><li>使用RPN网络产生的更多的候选目标可以得到更好的性能，但是当候选目标数量达到一定量之后，性能会趋于饱和。</li><li>使用nms抑制不会影响效果，反而会过滤掉一些假阳。</li><li>RPN的分类任务可以用于挑选高质量的候选目标，使得用于下阶段的候选目标可以大大减少。</li><li>RPN的回归任务对结果的影响很大.</li><li>提高RPN网络的性能可以提高最终整体的性能。</li></ol><img src="/post/dcec0426/FBADBB22-C38F-4180-94CF-36F93F0271A6.png"><p>Table2和Table3给出了backbone使用VGG-16的Faster RCNN在PASCAL VOC 2007和PASCAL VOC 2012目标检测任务上的检出性能。从Table2和Table3中可以看出，在共享特征并只使用300个候选的情况下，backbone为VGG-16的RPN+Faster RCNN在PASCAL VOC2007和PASCAL VOC 2012目标检测任务上取得了比SS+Fast RCNN更好的结果，运行速度也远远比SS+Fast RCNN更快。<br><img src="/post/dcec0426/C612DC4F-6F19-4C28-85EC-E3875A7F5C35.png"><br>Table4对比了SS+Fast R-CNN和RPN+Fast R-CNN整个目标检测系统的运行速度。从Table4可以看出在使用RPN进行proposal后，整个系统的运行速度得到了极大的提升。<br><img src="/post/dcec0426/FA827482-6937-49C6-B9BA-A8EE92250953.png"><br>Figure2给出了不同的proposal方法，在不同IoU的情况下的召回率。从Figure2可以看出，RPN对于不同的proposals数量具有很强的稳定性，即使proposals数从2k降到300，对最终结果的影响也不大；而其它的proposal方法则对结果影响很大。<br><img src="/post/dcec0426/3E070A0A-68F8-444D-8A8A-DAC1ACF1DE4C.png"><br>Table5对比了一阶段的目标检测和两阶段的目标检测方法的性能。从Table5可以看出Tow-Stage目标检测方法要比one-stage目标检测方法要好很多，在PASCAL VOC 2007上提升了约5个点。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Faster RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016_R-FCN_Object Detection via Region-based Fully Convolutional Networks(NIPS2016)Jifeng Dai</title>
      <link href="/post/fe660c1.html"/>
      <url>/post/fe660c1.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>先前主流的目标检测方法，都是使用了一个全图片共享计算的全卷积子网和一个没有共享计算的ROI分类子网。这样的话没有共享计算的ROI分类子网往往比较耗时。最近最新的分类网络都被设计为全卷积网络的形式,比如resnet。那是不是在目标检测任务中也可以只使用全卷积网络进行共享计算来进行检测呢？作者发现如果直接运用全卷积网络来进行检测效果并不理想，这是因为分类任务是希望变化不敏感的，而检测任务则是希望变换敏感的，直接应用分类网络来进行检测效果不大理想。之前resnet作者为了获得比较好的准确率将ROIpooling插入到cov4和conv5卷积层之间，这样将网络变成了变换不敏感的全卷积子网和变换敏感的ROI子网，这样仍然与之前做法没区别，ROI子网仍然无法共享计算 。为此在这篇论文中，作者提出了基于区域的全卷积神经网络，用于更加准确，高效的目标检测任务。跟faster rcnn相比，RFCN不需要在每个区域运用一个子网络来对目标进行进一步的筛选，而是直接运用一个全图共享的全卷积神经网络直接得到结果，因此可以更快。为了达到这个目标，作者提出了一个位置敏感得分图，去解决在目标检测任务中，需要对变换敏感，而在分类任务中需要对变换不敏感这样一个两难的问题。该方法可以很自然地使用全卷积分类网络作为backbone。作者使用了resnet101，在PASCAL VOC数据集上达到了比较好的结果（在2007数据集上mAP为83.6%，在2012数据集上mAP为82.0%），并且速度比faster rcnn+resnet101快了2.5-20倍，平均每张图片只需170ms。</p><p>代码:<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="noopener">https://github.com/daijifeng001/r-fcn</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><img src="/post/fe660c1/64285B25-5335-40DD-83AB-91208765B8E3.png"><p>为了解决分类网络对变换不敏感而检出任务则需要对变换敏感这个两难问题，作者使用一个指定通道数的卷积层作为全卷积网络的输出，该卷积层中每个通道都编码进了不同类型目标的位置信息，整个卷积层构成了位置敏感得分图。之后作者在该卷积层上面放置了位置敏感的池化层（RsROI pooling）去整合各个类型目标得分图的信息，这个操作并没有引入额外的参数。整个过程都是可以端到端训练并且是全卷积可共享计算的。<br>如Figure1所示，C为类别数，k为空间划分的粒度，也即将目标所在区域划分成$k \times k$的网格，则每个目标将会有$k^2$个得分图，每个得分图对目标的不同位置做出响应，最终将会有$(C+1) \times k \times k $个得分图，多加的一类为背景。最后在RPN网络获取到候选目标位置的时候，会利用RsROI pooling将各个类别的各个位置的得分进行融合，作者采用取平均的方式，最后再使用softmax得到最终目标的分类置信度。对于目标的定位，作者也采用类似的方式，加多一个得分图分支，不过这里作者采用的是类别无关的方式，也即得分图通道数为$4 \times k \times k$。</p><h1 id="三、实现细节"><a href="#三、实现细节" class="headerlink" title="三、实现细节"></a>三、实现细节</h1><img src="/post/fe660c1/6B5EDDF9-3BD6-4F59-9799-972F05198BAB.png"><p>作者采用了2阶段的目标检测策略，先利用RPN网络得到候选框，然后再利用R-FCN对候选目标进一步的筛选和定位，整个检出系统如Figure2所示。<br>给定一个候选的ROI区域，R-FCN主要去区分该区域目标的类别以及精确定位目标。在R-FCN中，所有的可学习到的权重层都是卷积层。最后的卷积层为每个类别生成一个k平方channel的位置敏感得分图。这个得分图编码了位置信息进去。在位置敏感得分图之后，作者使用了一个RsROI pooling层去整合各个类别的各个位置的得分信息，最后再使用softmax得到每个类别的置信度。</p><h2 id="3-1-Backbone-architeture"><a href="#3-1-Backbone-architeture" class="headerlink" title="3.1 Backbone architeture"></a>3.1 Backbone architeture</h2><p>作者采用ResNet-101作为backbone。首先利用Imagenet对其进行预训练，然后将最后的global average pooling和1000类的fc层移除，并在最后一层2048-d卷积层后放置一个通道数为1024-d的1x1卷积层对最后一层进行降维，最后再放置$(c+1) \times k \times k$-d的得分图。</p><h2 id="3-2-Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling"><a href="#3-2-Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling" class="headerlink" title="3.2 Position-sensitive score maps &amp; Position-sensitive RoI pooling"></a>3.2 Position-sensitive score maps &amp; Position-sensitive RoI pooling</h2><p>为了编码位置信息到每个ROI区域，作者将ROI区域划分为$k x k$个网格。对于大小为$w x h$的ROI区域，每个网格大小约为为$(w/k) \times (h/k)$。对于每个类别的每个网格信息，作者通过以下的RsPooling操作得到其得分：<br><img src="/post/fe660c1/78ECCD99-5E37-45D5-9309-1B02EF7BAA97.png"><br>这里$Z_{i,j,c}$是每个channel的响应值，$x_0$和$y_0$是ROI区域左上角的位置。每个类别每个位置$(i，j)$的得分，是通过将该类别的对应位置的channel取出来，然后将与该位置相关的响应值求和取平均得到。在得到每个位置的得分之后，最后的结果通过对所有位置求平均得到。然后得到每个类别的score之后，再利用softmax得到最后每个类别的概率。loss采用交叉熵loss。<br>对于回归任务的处理，作者也采用了类似的方式，但是使用了一个类别无关的回归策略，当然也可以使用一个类别相关的。为此接入的一个卷积层的channel数为4乘以k的平方。然后通过聚合每个位置每个坐标的得分，得到一个4维的向量。这里位置的变换参考R. Girshick. Fast R-CNN. In ICCV, 2015.</p><h2 id="3-3-孔洞卷积"><a href="#3-3-孔洞卷积" class="headerlink" title="3.3 孔洞卷积"></a>3.3 孔洞卷积</h2><img src="/post/fe660c1/6B2A747A-6998-4C59-BAF7-51090E4AD8FF.png"><p>在FCN中使用孔洞卷积可以提高语义分割的效果。由于现在模型也是全卷积的形式，也可以利用孔洞卷积带来的增益。另外作者也将resnet101的stride从原来的32变成16，以此来增加得分图的分辨率。在conv4 stage及之前的卷积作者保持不变，而conv5的第一个block作者将stride从原来的2调整为1，并将conv5 stage的所有的卷积滤波器变成了孔洞卷积。作者将RPN放在了conv4 stage上，这样做也方便与之前resnet101+faster rcnn比较。最后作者通过实验发现使用孔洞卷积，mAP可以提高2.6个点。</p><h2 id="3-4-可视化"><a href="#3-4-可视化" class="headerlink" title="3.4 可视化"></a>3.4 可视化</h2><img src="/post/fe660c1/B0105F1E-D372-4A99-BC0A-51768AB0A681.png"><p>Figure3和Figure4给出了当目标在ROI区域内和不在ROI区域内得分图的情况.可以看到，当目标在ROI区域内时各个位置得分都比较高，最后融合后最终的得分相应的也高；而当目标偏离ROI区域的时候，有些位置的得分就偏低，最后融合后最终的得分相应的也低。</p><h1 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h1><h2 id="4-1-PASCAL-VOC目标检测任务"><a href="#4-1-PASCAL-VOC目标检测任务" class="headerlink" title="4.1 PASCAL VOC目标检测任务"></a>4.1 PASCAL VOC目标检测任务</h2><p>训练相关设置：</p><ul><li>目标函数：<img src="/post/fe660c1/F7562CD0-3CE5-4329-AC8D-01C2CA85C09E.png"><br>这里$c $ 为类别标签,$ c =0 $表示背景; $ t^* $为grouth truth box。分类损失函数为交叉熵；回归才是smooth-L1。</li><li>k * k = 7 x 7</li><li>正负样本构建准则:<br>正样本为与ground-truth box的IOU大于0.5的侯选框，其它为负样本。训练的时候选择128个候选框进行训练。</li><li>OHEM方式训练：训练的时候采用在线困难样本挖掘进行训练。该方法有个特别好的优势就是在困难样本挖掘的时候，基本不需要什么计算量。</li><li>正则化：权重衰减系数为0.0005，动量为0.9</li><li>预处理：将图片短边缩放为600，采用单尺度训练。多尺度训练使用的尺度为{400,500,600,700,800}</li><li>学习率：在voc上，前20k次迭代学习率为0.001，后10k次迭代学习率为0.0001.</li><li>batch-size：8个gpu每个gpu一张图片</li><li>采用faster rcnn提到的4-step交替训练，训练RPN和R-FCN。</li></ul><p>推断相关设置：</p><ul><li>在短边为600的图片上获取300个候选，并使用IOU为0.3的NMS进行过滤</li></ul><h3 id="4-1-1-多种全卷积目标检测策略的比较"><a href="#4-1-1-多种全卷积目标检测策略的比较" class="headerlink" title="4.1.1 多种全卷积目标检测策略的比较"></a>4.1.1 多种全卷积目标检测策略的比较</h3><img src="/post/fe660c1/3B88DC08-51CB-40A9-B92D-5745A0A326BA.png"><p>作者比较了多种全卷积的目标检测策略，发现提到的方法要远远优于其他策略。</p><h3 id="4-1-2-faster-R-CNN-Resnet101和RFCN-Resnet101的比较"><a href="#4-1-2-faster-R-CNN-Resnet101和RFCN-Resnet101的比较" class="headerlink" title="4.1.2 faster R-CNN+Resnet101和RFCN+Resnet101的比较"></a>4.1.2 faster R-CNN+Resnet101和RFCN+Resnet101的比较</h3><img src="/post/fe660c1/D7D26C5B-F54F-4627-80BE-E294CA3C9B23.png"><p>从结果可以看出，R-FCN与faster rcnn效果差不多，但是速度要快上很多。</p><h2 id="4-2-MS-COCO目标检测任务"><a href="#4-2-MS-COCO目标检测任务" class="headerlink" title="4.2 MS COCO目标检测任务"></a>4.2 MS COCO目标检测任务</h2><img src="/post/fe660c1/B1031FFD-2625-47F6-8CE6-3CBA7679B247.png"><ul><li>数据集：80k训练集，40k验证集，20k测试集</li><li>学习率：前90k次迭代为0.001，后30k次迭代为0.0001</li><li>batchsize:8</li><li>交替训练方式：参考faster-rcnn</li></ul><p>从结果可以看出，R-FCN与faster rcnn效果差不多，但是速度要快上很多。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> two-stage detector </tag>
            
            <tag> RFCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016_Rethinking the Inception Architecture for Computer Vision(CVPR2016)_SzegedyC et al</title>
      <link href="/post/15020972.html"/>
      <url>/post/15020972.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>虽然在有足够标签数据的前提下，通过增加模型的大小和计算代价可以马上带来性能的提升，但是足够快的计算效率和更小的模型仍然对于某些场景很重要，比如移动端的视觉识别任务和大数据场景。Inception网络结构的提出正是为了能够找到一个运行效率高、性能高且模型复杂度较小的网络结构，但是Inception体系结构的复杂性使得在该网络上再做一些优化变得很困难，因为改变里面的一些结构有可能会带来性能的快速下降。之前也没有解释网络结构中哪些构建原则对网络性能影响比较大，这样更加难以对其进行调整。在这篇论文中，作者首先提出了一些关于如何有效去扩大网络结构来获得更多性能提升的通用原则；然后作者根据这些原则通过<strong>适当的卷积分解</strong>和<strong>有效的正则化</strong>来尽可能有效地增加模型表达能力，并以此按比例增加网络，来找到一个更加精简有效的网络结构。最后作者在ILSVRC2012分类比赛的验证集上对模型进行测试，并达到了最先进水平。在单模型单帧测试的情况下，该模型仅以50亿次加乘操作的代价，达到了top-1误差为21.2%，top-5误差为5.6%。模型仅有两千五百万参数。通过4个模型以及multi-crop的方式进行测试，更是将top-5误差减少到3.5%，top-1误差减少到17.3%。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-创新点"><a href="#2-1-创新点" class="headerlink" title="2.1 创新点"></a>2.1 创新点</h2><ol><li>提出了网络结构通用的设计准则（这些原则的应用并不十分明确，只是指明了一个方向）</li></ol><ul><li>a. 应该避免表示瓶颈，特别是在网络前几层。网络的表达能力不能一下子被过度压缩，而应该从输入到输出逐渐地减小。而且理论上，模型的表达能力不能仅仅通过维度来衡量，其它因素如相关的结构之类也是很重要的因素，维度只是提供了表达能力的粗略估计。</li><li>b. 在网络较深层应该利用更多的feature map，有利于容纳更多的不相关的特征。这样可以加速训练。</li><li>c.大的卷积，可以通过多个小卷积的聚合来表示，即把卷积稀疏化，并且不会降低表达能力。</li><li>d.要平衡网络的宽度和深度。同时增加网络的深度和宽度可以带来准确率的增加，但是适当的比例可以带来最大的提升。</li></ul><ol><li>提出了对卷积进行分解但不降低表达能力的方法。</li><li>提出了有效的正则化方式。</li><li>结合分解卷积及有效的正则化，找到精简但是性能强大的网络结构。</li></ol><h2 id="2-2-重要结论"><a href="#2-2-重要结论" class="headerlink" title="2.2 重要结论"></a>2.2 重要结论</h2><ol><li>可以将卷积分解为非对称卷积，而不降低表达能力。</li><li>在feature map大小为12到20之间的卷积层使用1x7和7x1的对称卷积，效果非常好。</li><li>辅助分类器不能辅助模型拟合，更多是作为一个正则项的功能。</li><li>应对低分辨率的情况，最好是通过调整stride和maxpool来让网络适应低分辨率，而不是使用小网络。</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><h2 id="3-1-卷积分解"><a href="#3-1-卷积分解" class="headerlink" title="3.1. 卷积分解"></a>3.1. 卷积分解</h2><h3 id="3-1-1-利用两个3X3卷积来替代一个5x5的卷积"><a href="#3-1-1-利用两个3X3卷积来替代一个5x5的卷积" class="headerlink" title="3.1.1 利用两个3X3卷积来替代一个5x5的卷积"></a>3.1.1 利用两个3X3卷积来替代一个5x5的卷积</h3><img src="/post/15020972/5FF96E53-BD3B-4A8C-A5BE-0E3D0C9EC348.png"><p>这种替代方式在保持感受野不变的情况下，带来了计算量以及参数量的减少。<br><img src="/post/15020972/7F8573DA-18FD-4A3B-9C09-1710EAC6A785.png"><br>这种替代方式使用线性激活函数应该更加吻合，但是作者做了对比试验发现，Relu要比线性激活函数效果要好。<strong>使用Relu增加了模型的表达能力。</strong><br><img src="/post/15020972/979671C2-F29C-43EB-8CEA-7D012B789662.png"><br><img src="/post/15020972/AEBD4F1F-88BF-4485-B340-D5E049383640.png"></p><p>Figure 4和Figure 5给出了利用两个3X3卷积来替代一个5x5的卷积的例子。</p><h3 id="3-1-2-将卷积分解为非对称卷积"><a href="#3-1-2-将卷积分解为非对称卷积" class="headerlink" title="3.1.2 将卷积分解为非对称卷积"></a>3.1.2 将卷积分解为非对称卷积</h3><img src="/post/15020972/9B7D0FF8-5A31-485A-9743-6E47EC2E70D0.png"><p>可以用两个3x1和1x3的不对称卷积来替代3x3卷积，进一步降低参数量及计算量，同时也增加了非线性性。<br><img src="/post/15020972/51518A57-8127-4DB1-B02A-C1263BEA71A5.png"><br>理论上可以用nx1和1xn来替代任何大小的nxn卷积，如Figure 6所示，但是作者发现实际应用中，在浅层卷积层使用效果并不好，但是在中等大小的feature map（在feature map大小为12到20之间的卷积层）上使用效果比较好。在feature map大小为12到20之间的卷积层使用1x7和7x1的对称卷积，效果非常好。在作者提到的网络结构中，作者在17X17大小的feature map上，使用了1x7和7x1分解卷积。<br><img src="/post/15020972/D1F7A09B-1F34-4D64-95E1-66FEB1386FF5.png"><br>Figure 7给出了增大网络宽度的一种方式。在作者提到的网络结构中，作者将其应用到了feature map大小为8x8的卷积层，为了增加高纬的特征表示，这个也是原则2所建议的方式。</p><h2 id="3-2-辅助分类器的使用"><a href="#3-2-辅助分类器的使用" class="headerlink" title="3.2 辅助分类器的使用"></a>3.2 辅助分类器的使用</h2><img src="/post/15020972/A2CD5386-14E9-4BC4-A36D-4CADBDC73DA4.png"><p>辅助分类器是在《Going deeper with convolutions.》中提出来的，作者说使用辅助分类器可以加速拟合，缓解梯度消失。但是本文作者发现辅助分类器并没有这个效果。作者发现：</p><ol><li>在模型没有达到完全拟合之前，使用辅助分类器或者不使用辅助分类器拟合速度都是差不多的。只有在模型接近拟合的时候，使用辅助分类器会比没有使用辅助分类器模型的准确率会更高一些。</li><li>在《Going deeper with convolutions.》中作者使用了2个辅助分类器，但是本文作者发现，将底层的辅助分类器移除掉，对模型最终的效果并没有影响。</li></ol><p>所以先前说辅助分类器可以辅助模型拟合的说法是有问题的，在这里辅助分类器更多是作为一个正则项的功能。这个也有一些证据的支持，就是在辅助分类器使用bn，或者dropout时，模型表现会更好。</p><h2 id="3-3-feature-map大小的有效减少"><a href="#3-3-feature-map大小的有效减少" class="headerlink" title="3.3 feature map大小的有效减少"></a>3.3 feature map大小的有效减少</h2><p>为了降低Feature map的大小，通常都会使用pooling操作。为了避免表示瓶颈，在pooling之前会先用卷积对filter进行扩展，然后再应用max或者avg pooling操作。比如想要从大小为dxd，通道数为k的feature map过度到大小为(d/2)x(d/2),通道数为2k的feature map，一种没有表示瓶颈的操作方式是先利用stride为1的卷积层，先得到一个通道数为2k的feature map，然后再应用pooling操作，但是这种方式会增加计算代价。<br><img src="/post/15020972/9D9D1F28-2A33-4FF4-A01F-3BFFA423DED0.png"><br>如Figure9左边所示，先pooling再卷积会带来表示瓶颈；如Figure 9右边所示，先卷积再pooling会增加计算代价。</p><img src="/post/15020972/7C593166-E803-4C7E-A2D5-AEC00A1C9A03.png"><p>为了既能解决表示瓶颈又不会增加计算量，作者提出了一个新的构建模块。使用两种stride为2的并行分支，一种是先进行pooling，一种是进行卷积，然后再将两者拼接。如Figure 10所示，这种方式能在减少feature map大小的同时增加filter大小且不会带来表示瓶颈也不会增大计算代价。</p><h2 id="3-4-Inception-v3"><a href="#3-4-Inception-v3" class="headerlink" title="3.4 Inception-v3"></a>3.4 Inception-v3</h2><img src="/post/15020972/13DD350F-C555-4136-9674-770D9B710B9D.png"><p>作者根据提到的原则构建了网络模块，并使用这些原则下的构建块提出了一个新的网络结构。详细的网络结构设置如Table1所示。改进的Inception结构计算代价仅比GoogleLeNet大2.5倍，但是仍然比VGGNet代价小。</p><h2 id="3-5-通过标签平滑进行模型正则化"><a href="#3-5-通过标签平滑进行模型正则化" class="headerlink" title="3.5 通过标签平滑进行模型正则化"></a>3.5 通过标签平滑进行模型正则化</h2><p>这里作者对标签的分布进行平滑，以此让模型预测出来的概率差异不至于太大。如果预测出来的概率在真实标签位置接近于1，而其它标签位置接近于0，这样的话很容易过拟合而导致泛化能力较差；另外当预测出来的真实标签的概率远远大于其它标签概率的时候，这样产生的梯度就比较小而使模型的适应能力差，最终导致模型泛化能力较差。作者提出的方法是为标签加入一个平滑正则项（LSR），该平滑正则项为均匀分布，直觉上是让模型预测出来的概率值更偏向于均匀分布一些。加入正则项的ground truth可表示为：<br><img src="/post/15020972/6BFAF4E4-9C40-4DE2-B674-44E39679DF87.png"><br>这里K为类别数，而$\epsilon$为权重。通过加入该正则项，最终作者提到的模型在ILSVRC2012分类任务上，top1-error和top5-error性能都提高了0.2%</p><h2 id="3-6-训练细节"><a href="#3-6-训练细节" class="headerlink" title="3.6 训练细节"></a>3.6 训练细节</h2><ol><li>使用随机梯度下降，50张Kepler GPU，每张卡batch size 32，跑100个epoch.</li><li>使用RMSProp优化方法，decay 为0.9，$\epsilon$为1.0.</li><li>初始学习率为0.045，使用指数衰减方式，每2个epoch衰减一次，底数为0.94.</li><li>作者发现使用梯度截断能够让训练更加稳定，这里作者使用的阈值为2.0.</li></ol><h2 id="3-7-分辨率对性能的影响"><a href="#3-7-分辨率对性能的影响" class="headerlink" title="3.7 分辨率对性能的影响"></a>3.7 分辨率对性能的影响</h2><img src="/post/15020972/DFAA3F44-BF9B-4C32-BB13-75621A8A6BE9.png"><p>作者在保持计算代价的情况下，比较了不同分辨率输入的效果。发现低分辨率会略微影响性能，但是影响不大。如果在低分辨率的情况下也减少模型复杂度，那性能会降得更多。所以最好是通过调整stride和pooling来让模型适应低分辨率的情况。</p><h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><img src="/post/15020972/E3C2388A-EAFA-4235-A0E8-C2EB9979DEEF.png"><p>Table3为单模型单crop下的结果。<br><img src="/post/15020972/06F655E7-FCEC-4F49-B1F5-A743B94E00DE.png"><br>Table4为单模型多crop下的结果。<br><img src="/post/15020972/D729DD3A-96E3-420A-8F96-D742EB928978.png"><br>Table5为多模型多crop下的结果。<br>​</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Inception </tag>
            
            <tag> Inception-v3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016_Deep Residual Learning for Image Recognition_CVPR2016_HeK et al</title>
      <link href="/post/92de5aab.html"/>
      <url>/post/92de5aab.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>网络深度是深度神经网络性能提高的一个重要因素，但是在模型深度达到一定程度时，会出现梯度消失或者梯度爆炸的问题而很难训练。BN的出现极大程度解决了梯度消失或者爆炸带来的问题，使得训练更深层的网络更加容易。但是却带来了另一个问题，退化问题。随着网络深度的增加，模型在训练过程中准确率的增加会慢慢饱和，随后准确率反而开始下降。这个问题的出现表明并不是所有网络结构都很容易优化。为了解决深度神经网络网络层数太深而难以训练的问题，作者提出了残差学习框架。使用该框架可以解决由于网络层数太深而难以训练的问题。在此基础上作者提出resnet网络结构，该网络结构可以构建深度比vgg深很多倍，但是模型复杂度比vgg小的网络结构，并且效果要比vgg更好。使用提到的网络结构作者在ILSVRC分类任务以及COCO目标检测和分割任务中都取得了第一名。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><img src="/post/92de5aab/7293B65F-03B6-4F67-A2E1-1B77B7ABCF0B.jpg"><p>作者通过引入深度残差学习框架来解决退化问题。跟原本让堆叠的多层网络去学习一个潜在的映射不同，作者强制让他们去学习一个残差映射。假设原本学习的潜在映射是H，那现在堆叠的多层网络则是学习映射F，这里F=H-x。原本的映射则为H=F+x。作者认为这种方式要比原本的更加容易优化，因为在极端的情况下，当模型饱和的时候，输入端可以通过shortcut connection直接跳过非线性层。残差模块的构建也很简单，只需要通过shortcut connnection将输入端恒等映射到输出并加到原本堆叠的多层网络的输出上。<br><strong>这种做法的动机</strong>：为什么使用残差学习可以解决退化问题呢？作者给出了一个解释：模型通过多个非线性层来学习恒等映射是比较困难的。但是通过残差学习的形式，让多层非线性层去学习接近于0的输出从而让整体输出接近恒等映射则要容易很多。当网络达到一定复杂度的时候，很多时候会需要让多余的表达能力去拟合一个恒等映射，通过残差表示可以让模型更容易学习恒等映射，从而可以消除退化问题。<br><strong>最后作者通过实验证明了该方法十分有效。不仅很容易优化深层网络，而且也不会出现退化问题，反而能够很有效的利用深度的增加带来的增益</strong>。</p><h1 id="三、实现细节"><a href="#三、实现细节" class="headerlink" title="三、实现细节"></a>三、实现细节</h1><h2 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1.网络结构"></a>1.网络结构</h2><img src="/post/92de5aab/205C4314-502B-47E8-B471-3D2A8DC98F55.png"><p>这里作者按照vgg网络的构建思路（a.相同的feature map大小，具有相同数量的filter b.当feature map大小减半的时候，对应的filter数量增加一倍。）构建了一个新网络，并在此基础上对其加入shortcuts连接，变成对应的resnet版本。构建的网络相较于vgg网络，具有更低的复杂度，约为VGG的18%。构建的resnet网络，当输入和输出维度相同的时候（如上图实线），使用恒等映射进行连接。当维度增加的时候（如上图虚线）作者考虑了两种操作。A.使用zero padding进行补齐 B.使用1X1的卷积增加维度，这两种操作stride都为2。</p><h2 id="2-实现细节"><a href="#2-实现细节" class="headerlink" title="2.实现细节"></a>2.实现细节</h2><h3 id="2-1-ImageNet分类任务"><a href="#2-1-ImageNet分类任务" class="headerlink" title="2.1 ImageNet分类任务"></a>2.1 ImageNet分类任务</h3><p>训练时参数设置：</p><ul><li>预处理：减去均值图片</li><li>数据增强：1）将图片的短边随机缩放到[246，480]之间进行尺度增强。2）从缩放后的图片中随机crop224 * 224大小的图片 3)对图片进行随机水平翻转。4）进行颜色增强。</li><li>在每个卷积和激活函数中间加入BN层。</li><li>正则化：权重衰减系数为0.0001；动量参数为0.9；没使用dropout。</li><li>优化方法：SGD;batch-size大小为256；起始学习率为0.1;学习率衰减方式为当误差无法下降的时候将学习率除以10;迭代次数为60* 10^4以上。</li><li>参数的初始化方式参考：K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. </li></ul><p>测试时参数设置：</p><ul><li>采用多种尺度的全卷积形式，然后将得分取平均。尺度包括将短边resize为{224,256,384,480,640}</li></ul><h1 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h1><img src="/post/92de5aab/C183D00E-CC2C-4412-B521-C224E7FB4821.png"><h2 id="4-1-Imagenet-分类任务（试验用的模型结构如上图）"><a href="#4-1-Imagenet-分类任务（试验用的模型结构如上图）" class="headerlink" title="4.1 Imagenet 分类任务（试验用的模型结构如上图）"></a>4.1 Imagenet 分类任务（试验用的模型结构如上图）</h2><h3 id="4-1-1-Plain-Networks-vs-Residual-Networks"><a href="#4-1-1-Plain-Networks-vs-Residual-Networks" class="headerlink" title="4.1.1 Plain Networks vs Residual Networks"></a>4.1.1 Plain Networks vs Residual Networks</h3><img src="/post/92de5aab/88873603-E682-4EA0-9405-99DA5375A339.png"><img src="/post/92de5aab/C9094336-A10C-41EC-AFAB-39A68260C3A0.png"><p><strong>从Figure4和Table2的结果可得出以下几个结论：</strong></p><ol><li>没加入残差结构的时候，普通的18层网络反而比普通的34层网络具有更大的训练误差和测试误差，出现了退化问题。当加入残差结构的时候，退化问题消失了，并且模型可以从增加的深度获取增益。</li><li>从Figure4中可以看到，34层的resnet相较于普通的网络训练误差低很多，可以看出resnet可以有效用于训练很深的网络。</li><li>虽然18层的resnet和普通版本虽然具有相似的准确率，但是相较于普通版本resnet版本拟合得更快。</li></ol><h3 id="4-1-2-Identity-vs-Projection-Shortcuts"><a href="#4-1-2-Identity-vs-Projection-Shortcuts" class="headerlink" title="4.1.2 Identity vs Projection Shortcuts"></a>4.1.2 Identity vs Projection Shortcuts</h3><img src="/post/92de5aab/C83A0CE4-8DDD-4F1A-9838-081BC727EED4.png"><p>作者调查了3种形式的shortcut结构：A)zero-padding shortcuts用于增加维度，其它时候使用Identity shortcuts B）projection shortcuts用于增加维度，其它时候使用Identity shortcuts C）所有都是用projection shortcuts。发现projection shortcuts并不是处理退化问题的关键，shortcuts才是关键。</p><h3 id="4-1-3-Bottlenet-Architectures"><a href="#4-1-3-Bottlenet-Architectures" class="headerlink" title="4.1.3 Bottlenet Architectures"></a>4.1.3 Bottlenet Architectures</h3><img src="/post/92de5aab/F0414911-B5D1-4C06-A699-0B87FB087DB8.png"><img src="/post/92de5aab/60C0CFF2-04AE-4489-B3B6-43F0EA9C582B.png"><p>为了在控制的训练时间内，探索残差模块对训练更深网络的效用，作者使用了bottlenet结构，将每个残差模块中的2层卷积替换为3层卷积。先使用1x1卷积进行降维，之后再使用1X1卷积扩张维度。这样使得复杂度与原本2层结构相似。构建的网络结构如table1中50层resnet,101层resnet和152层resnet。从table3和table4中可以看出更深的网络效果更好，而且不会出现退化问题。table4为提到的方法与最先进的几个方法的对比，从结果可以看出resnet效果具有很大的提升。</p><h2 id="4-2-CIFAR-10-分类任务"><a href="#4-2-CIFAR-10-分类任务" class="headerlink" title="4.2 CIFAR-10 分类任务"></a>4.2 CIFAR-10 分类任务</h2><p><strong>作者在CIFAR-10数据集上也得出了类似的结论另外作者也对响应值情况和是否能够训练更深的网络进行了探索:</strong><br><img src="/post/92de5aab/36D99014-228F-4EF3-8F67-20713F09DD73.png"></p><ol><li>作者也对卷积层的响应情况进行分析（Figure7），并得出以下结论：<br>1）相较于普通网络，resnet网络的响应值更加小。<br>2）层数更多的resnet，会产生更多小响应值。这两个结论与原本的动机想符合。</li><li>作者也尝试将网络层数增加到1000层，发现网络依然能够正常训练，说明残差结构对于训练深层网络是很有用的。</li></ol><h2 id="4-3-PASCAL-和-MSCOCO目标检测任务。"><a href="#4-3-PASCAL-和-MSCOCO目标检测任务。" class="headerlink" title="4.3 PASCAL 和 MSCOCO目标检测任务。"></a>4.3 PASCAL 和 MSCOCO目标检测任务。</h2><img src="/post/92de5aab/29CE9F3E-7B73-4BF3-981B-A287779C46D0.png"><p>在目标检测任务中，作者简单的将vgg网络替换为res101就获得了很大的增益，说明该方法对于其他识别任务也同样有效。并以此网络结构作者赢得了ILSVRC和COCO2015各项比赛的第一，其中包括Imagenet 检测，Imagenet定位，COCO检测和COCO分割任务。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> resnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016_Identity Mappings in Deep Residual Networks_ECCV2016_HeK et al</title>
      <link href="/post/96c98dd3.html"/>
      <url>/post/96c98dd3.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>在这篇论文中，作者通过分析残差构建块的前向和反向信号传播机制，发现当使用恒等映射作为skip connection，并在残差函数与shortcut相加之后也使用恒等映射的时候，前向和反向信号可以直接从某个构建块，传播到任意一个构建块。并根据这两个条件提出了一个新的残差模块，使网络更加容易训练，并且泛化能力更好。最后作者在CIFAR-10和CIFAR-100数据集上使用1001层的resnet，以及在Imagenet数据集上使用200层的resnet都取得了较好的结果。<br>代码<a href="https://github.com/KaimingHe/resnet-1k-layers" target="_blank" rel="noopener">https://github.com/KaimingHe/resnet-1k-layers</a></p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="2-1-对深度残差网络的分析"><a href="#2-1-对深度残差网络的分析" class="headerlink" title="2.1 对深度残差网络的分析"></a>2.1 对深度残差网络的分析</h2><img src="/post/96c98dd3/4531CFD3-8404-493D-B4C0-EB9F9D219E99.png"><p>如上图所示为之前提出的残差块，其中$x_l$为第$l$层的输入特征，$x_{l+1}$为对应的输出；$W_l=\{W_{l,k}|_{1&lt;=k&lt;=K}\}$为该残差单元的权重和偏置；$K$为层数；$F$表示残差函数；$f$为逐点元素加操作之后的操作，比如在原先的残差单元是ReLU；$h(x_l)$为恒等映射，即$h(x_l)=x_l$。<br>如果$f$也是恒等映射即$x_{l+1}=y_l$,那么等式（2）就可以表示为：</p><img src="/post/96c98dd3/35504E40-8466-4E2D-A2EF-0742EEDD3066.png"><p>将$x_l$不断进行展开，则可以得到：<br><img src="/post/96c98dd3/FFE28798-3EAF-4739-8119-AEEDAB2AAE6B.png"></p><p>从等式（4）可以看出，对于任意深的模块$L$和对于任意浅的模块$l$，任意深的单元都可以表示为任意浅的单元加上一个残差函数，这就可以把任意两个单元之间的所有单元，看做是一个残差块。等式（4）也带来了非常棒的反向传播属性，根据链式法则，loss函数对任意一层的求导可以表示为：<br><img src="/post/96c98dd3/FFA72D68-6557-429A-A23C-C16A05EFE679.png"></p><p>从等式(5)可以看出，loss对任意一层的导数可以分解成两部分，一部分是信息直接传播到任一层不需要涉及到任何权重的，另一部分则是涉及到权重层。这种方式也使得传递到$l$层的梯度不会消失。等式（4）和等式（5）说明信号可以直接从任意单元直接传播到另一个单元。为达到这个目的需要满足两个条件1）$h(x_l)=x_l$ 2) $f(y_l)=y_l$</p><h2 id="2-2-提出的新的残差模块"><a href="#2-2-提出的新的残差模块" class="headerlink" title="2.2 提出的新的残差模块"></a>2.2 提出的新的残差模块</h2><img src="/post/96c98dd3/9173E43B-C089-4C53-AF7F-FC8435659DCF.png"><p>这里将卷积、BN和RELU的位置做了调换，使得该构建块满足提到的两个条件，即$h(x_l)$和$f(y_l)$都是恒等映射的。可以看到通过这种变换，模型拟合得更好，loss更低。</p><h1 id="三、对提到的两个条件的重要性进行验证"><a href="#三、对提到的两个条件的重要性进行验证" class="headerlink" title="三、对提到的两个条件的重要性进行验证"></a>三、对提到的两个条件的重要性进行验证</h1><h2 id="4-1-Identity-skip-connections的重要性。"><a href="#4-1-Identity-skip-connections的重要性。" class="headerlink" title="4.1 Identity skip connections的重要性。"></a>4.1 Identity skip connections的重要性。</h2><p>作者做了简单的修改，通过设置$h(x_l)=\lambda_lx_l$打破了恒等shortcut，即为：<br><img src="/post/96c98dd3/1D6D6054-837C-438E-B1C1-A1C9F3937D5A.png"><br>则前向传播展开后为：<br><img src="/post/96c98dd3/67DF6235-93B4-41F8-A6DB-569F75C66409.png"><br>对应的反向传播为：<br><img src="/post/96c98dd3/E18F98E7-D9DD-4620-AA47-74C38107C398.png"><br>从等式(8)可以看出当对于所有的$i$，$\lambda_i&gt;1$时，很容易发生梯度爆炸；而当$\lambda_i&lt;1$时则很容易发生梯度消失，这些都带来了优化困难。</p><img src="/post/96c98dd3/F8180146-1018-4EC8-B736-3863DB128AB6.png"><img src="/post/96c98dd3/52EA80EE-FFCD-45AB-81D7-77CF31FBC8BA.png"><p>上面的结果是同个结构跑5次，取平均。从结果来看，恒等映射是最好的。</p><h2 id="4-2-激活函数的使用"><a href="#4-2-激活函数的使用" class="headerlink" title="4.2 激活函数的使用"></a>4.2 激活函数的使用</h2><p>原本的残差单元$f=ReLU$并不是恒等映射，为了调查$f$为恒等映射时对结果的影响。作者调查了了以下形式的残差单元。<br><img src="/post/96c98dd3/8BA2F9D7-A774-4D5D-9DCD-C786B7C2C3A4.png"><br><img src="/post/96c98dd3/97A85F90-BA0C-4637-A2F3-2A28E32BB5F9.png"><br><img src="/post/96c98dd3/A9AA6A7F-34B2-403B-AB56-0287F0A5D8E5.png"><br>从Table2和Table3看出提出的构建块比其他形式更好。</p><h2 id="4-3-结果分析"><a href="#4-3-结果分析" class="headerlink" title="4.3 结果分析"></a>4.3 结果分析</h2><img src="/post/96c98dd3/828FDE42-BF42-4BC6-B0ED-8843EB67FFF8.png"><p>通过pre-activation的方式，带来了两个好处：</p><ol><li>使得优化更加容易。<br>从图1可以看出利用提到的构建块构建的1001层的resnet loss降得很快，也达到了当前所有模型中能达到的最低loss。另外作者也发现当层数不多的时候，f=ReLU并不会对训练带来特别严重的影响，从Figure6（right）中可以看出，loss与提到的构建块差不多。</li><li>提高了泛化能力。<br>从Figure6(right)中可以看出，虽然在较少层的情况下，f=ReLU并没有带来很严重的影响。但是，从测试误差可以看到，本论文提到的构建块泛化能力更强。从Figure4(a)和Figure4（e）可以看出原本的构建块shortcut部分在与残差部分相加之后并没有进行标准化操作，而本论文提到的构建块两边都进行了标准化。</li></ol><h1 id="四、实验结果（与最新方法的比较）"><a href="#四、实验结果（与最新方法的比较）" class="headerlink" title="四、实验结果（与最新方法的比较）"></a>四、实验结果（与最新方法的比较）</h1><img src="/post/96c98dd3/EBE0B2C3-8283-45FE-87F0-960104466D60.png"><img src="/post/96c98dd3/4E69C3AF-1503-4839-B279-2A7A755108BF.png"><p>从结果可以看出，提出的残差块具有很大的优势。详细的参数设置可以查看原文。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 模型结构优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> resnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>object detection</title>
      <link href="/post/4ec81054.html"/>
      <url>/post/4ec81054.html</url>
      
        <content type="html"><![CDATA[<h2 id="object-detection（rcnn-the-way-to-endtoend）"><a href="#object-detection（rcnn-the-way-to-endtoend）" class="headerlink" title="object detection（rcnn,the way to endtoend）"></a>object detection（rcnn,the way to endtoend）</h2><p>本文主要简单介绍了rcnn如何一步步改进到支持端到端训练的faster-rcnn。<br><a id="more"></a></p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ol><li>mAP（mean average precision）</li></ol><ul><li>precision：检测出相关的内容占检测出的内容的比例。</li><li>average precision：举个例子，比如说当前文档相关的主题有5个，检测出了3个，其中3个相关的主题rank分别为2,3,4，则这个文档的average precision就是(1/2+2/3+3/4+0+0)/5 。 </li><li>mAP:则是每个文档的average precision的平均值。<br>这里的文档对应于目标检测的每张图片，而相关主题则对应到图片中目标的个数</li></ul><ol><li>IoU（交并比）</li></ol><ul><li>两个区域的交集比上两个区域的并集</li></ul><ol><li>non maxinum supression</li></ol><ul><li>与得分最高的区域的IOU高于某个阈值时，将其排除掉</li></ul><h3 id="基于region-proposeal加classification这一框架的几个重要工作"><a href="#基于region-proposeal加classification这一框架的几个重要工作" class="headerlink" title="基于region proposeal加classification这一框架的几个重要工作"></a>基于region proposeal加classification这一框架的几个重要工作</h3><h4 id="1-rcnn"><a href="#1-rcnn" class="headerlink" title="1.rcnn"></a>1.rcnn</h4><ol><li>几个重要的创新</li></ol><ul><li>selection search提取候选区域</li><li>利用cnn 提取重要的特征（需要将特征保存到硬盘中，用于svm的训练）</li><li>利用svm对不同的候选框进行分类</li><li>利用回归来获取更加准确的位置</li></ul><ol><li>缺点：</li></ol><ul><li>训练需要分成多个阶段完成</li><li>训练和存储代价大</li><li>检测速度缓慢</li></ul><h4 id="2-spp-net"><a href="#2-spp-net" class="headerlink" title="2.spp-net"></a>2.spp-net</h4><ol><li>几个重要的贡献</li></ol><ul><li>rcnn速度缓慢的原因是它需要分别对每个proposal提取特征，而没有共享计算</li><li>输入整张图片，然后利用spp层提取不同尺度大小的featuremap，最后将它们级联起来，最后输出一个特定维度的特征向量。</li><li>spp-net使得rcnn在检测的时候提升了10到100倍。训练速度提升了3倍。</li></ul><ol><li>缺点</li></ol><ul><li>训练仍然需要多个阶段。仍然需要提取特征训练svm； 最后仍然需要利用回归模型来获取更加精确的定位。</li><li>需要将特征保存到硬盘中。</li><li>网络训练的时候梯度没法穿过spp层。</li></ul><h4 id="3-fast-rcnn"><a href="#3-fast-rcnn" class="headerlink" title="3.fast rcnn"></a>3.fast rcnn</h4><ul><li>解决需要将同张图的每个候选区域分别输入网络提取特征的问题<br>直接输入原图，并利用cnn以及roipooling来提取每个roi区域的特征，并利用cnn对每个roi区域进行分类以及定位</li><li>roipooling层：单尺度的ssp层，输出特定大小的特征向量，类似于根据情况调整pooling层的kenal size。</li><li>多任务的loss：softmax层以及bounding box 回归</li></ul><ol><li>几个重要的贡献</li></ol><ul><li>提升了训练以及检测的速度。</li><li>训练只需要单个阶段就可以完成。使用了多任务的loss层。</li><li>可以调整网络的所有层。</li><li>不需要将特征保存到硬盘中</li></ul><ol><li>缺点：</li></ol><ul><li>仍然需要利用selection search</li></ul><h4 id="4-faster-rcnn"><a href="#4-faster-rcnn" class="headerlink" title="4.faster rcnn"></a>4.faster rcnn</h4><ul><li>提出了利用PRN来提取候选区域，并让PRN与fast rcnn共享网络。使得目标检测成为真正的一种endtoend的方式，并将速度进一步提升到可以进行实时检测的目的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object detection </tag>
            
            <tag> rcnn </tag>
            
            <tag> spp-net </tag>
            
            <tag> fast rcnn </tag>
            
            <tag> faster rcnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2015_Fast R-CNN(ICCV2015)Ross Girshick</title>
      <link href="/post/a58d66d1.html"/>
      <url>/post/a58d66d1.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>R-CNN虽然效果好，但是存在很多缺点：1）训练过程需要多个阶段。2）训练时间缓慢而且需要较大的存储空间。3）测试时间较慢。虽然SPPnet对R-CNN做了些许改进（针对R-CNN对于每个proposal都需要提取特征的情况进行优化，让每张图片只需要提取一次特征，作者主要是在特征提取和分类器之间最后的pooling层替换为一个空间金字塔pooling层），但是仍然存在很多缺点没有改进。针对R-CNN的这些缺点，作者提出一个更加快速，有效的目标检测方法叫Fast R-CNN。该方法在训练和测试速度以及性能方面，都要优于先前方法。该方法训练要比R-CNN快9倍，测试要快213倍；相较于SPPnet，训练快了3倍，测试快了10倍，并且都更加准确。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol><li>提出fast R-CNN框架，训练速度，测试速度以及准确率都比R-CNN和SPPnet要好</li><li>训练过程不需要多个阶段，并且使用了多任务loss（回归和分类放在一起训练）。</li><li>采用端到端的方式，训练可以更新所有网络层。（主要采用了ROI pooling层）</li><li>不需要把特征预先存储起来。</li></ol><h2 id="重要结论"><a href="#重要结论" class="headerlink" title="重要结论"></a>重要结论</h2><ol><li>多任务有助于提高性能。</li><li>多尺度虽然可以提升效果，但是提升的幅度并没有使用大模型并使用单尺度的高，反而会增加测试时间。所以权衡下，用单尺度，大模型会更好。</li><li>更多的数据更多的迭代次数效果更好。</li><li>softmax与svm效果差不多，但是使用softmax速度更快</li><li>更多的候选框，无法达到更好的效果</li></ol><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><img src="/post/a58d66d1/98AEEC50-F474-448E-96D8-B6C25A52CB90.png"><h2 id="3-1-网络的框架和训练"><a href="#3-1-网络的框架和训练" class="headerlink" title="3.1 网络的框架和训练"></a>3.1 网络的框架和训练</h2><ol><li>fast rcnn的输入是整一张图片</li><li>在最后的卷基层使用了ROIpooling从对应的proposal提取特征送入全连接层。</li><li>使用两个分支，1个预测类别，一个用来修正当前类别的bounding box的位置。</li></ol><h2 id="3-2-RoI-pooling-层："><a href="#3-2-RoI-pooling-层：" class="headerlink" title="3.2 RoI pooling 层："></a>3.2 RoI pooling 层：</h2><p>ROI pooling层是单层的空间金字塔pooling层。具体的会根据输入的卷积feature map的大小，以及输出的feature map的大小，计算每个子窗口的大小。然后对每个子窗口利用max pooling操作得到响应值。</p><h2 id="3-3-对预训练网络的修改："><a href="#3-3-对预训练网络的修改：" class="headerlink" title="3.3 对预训练网络的修改："></a>3.3 对预训练网络的修改：</h2><ol><li>最后的maxpooling层改为使用ROIpooling，H=W=7</li><li>最后的全连接层和softmax层改为两个任务，一个是全连接层和softmax，一个是全连接层和类别相关的bounding-box回归。</li><li>网络修改为2个数据的输入，一个是图像，一个是图像的ROI。</li></ol><h2 id="3-4-利用检测数据来fine-tune整个网络："><a href="#3-4-利用检测数据来fine-tune整个网络：" class="headerlink" title="3.4 利用检测数据来fine-tune整个网络："></a>3.4 利用检测数据来fine-tune整个网络：</h2><p>对于RCNN和SPPnet来说，fine-tune整个网络效率是很慢的，因为它们的采用策略是从每个图片中提取一个ROI用来训练。为此作者提出一个更加有效的训练方式，假如训练的mini-batch的ROI数为R，图片的候选数为N，则使用提取R/N张图片用于训练。采用这种方式的一个问题可能是过拟合问题，但是从结果来看并不会这样。</p><ol><li>在fine-tune的时候，fast rcnn联和优化了两个目标，分类与回归。对于分类，采用了一个softmax with loss 来训练，对于回归作者采用了smooth L1 loss，回归的目标使用了ground truth 和proposal 的一个变换，使其对尺度不敏感。两个任务的权重都为1.0</li><li>每个minibatch的采样方式：从两张图片中采128个样本，每个图片64个样本。其中取1/4为正样本，即IOU大于0.5的proposal。3/4负样本，为IOU在0.1到0.5之间的proposal。数据增强方式只是按0.5的概率进行水平翻转。</li><li>梯度传播方式：在ROIpooling层会记录每个输出来自于哪一个输入，然后在方向传播时，将相同输入的梯度累加起来，继续往下传播。</li><li>用于分类和回归的全链接层分别采用标准差为0.01和0.001，0均值的高斯分布来进行初始化。bias初始化为0.所有的层的权重使用per-layer学习率为1.0，bias使用per-layer学习率为2.0。在VOC07和VOC12数据集上，头30k个迭代使用一个全局的学习率为0.001，后10k次迭代使用一个全局学习率为0.0001.当训练集更大的时候，会使用更多次迭代。动量设置为0.9，权重衰减系数设置为0.0005.</li></ol><h2 id="3-5-测试："><a href="#3-5-测试：" class="headerlink" title="3.5 测试："></a>3.5 测试：</h2><p>测试的时候，首先提取proposal，然后利用fast rcnn得到每个proposal属于每个类别的得分，以及对应的bounding box 偏移。然后利用每个类别的得分以及非最大化抑制得到最后结果。<br>使用truncated SVD可以加快测试时间</p><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><ol><li>多任务学习提高mAP<img src="/post/a58d66d1/67E41B1D-BCAE-4672-909E-D284FEE52A63.png"></li><li>多尺度虽然可以提升效果，但是提升的幅度并没有使用大模型并使用单尺度的高，反而会增加测试时间。所以权衡下，用单尺度，大模型会更好。<img src="/post/a58d66d1/F183E894-1A17-4631-A977-B7F39448565A.png"></li><li>更多的数据更多的迭代次数效果更好。</li><li>softmax与svm效果差不多，但是使用softmax速度更快<img src="/post/a58d66d1/D0D5812D-BE0C-4B25-9443-CE8582DE0CA2.png"></li><li>更多的候选框，无法达到更好的效果<img src="/post/a58d66d1/B09A28CB-199A-46AC-85F5-18231242B63F.png"></li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Fast-RCNN </tag>
            
            <tag> two-stage detector </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sublime基础用法汇总</title>
      <link href="/post/f7b78535.html"/>
      <url>/post/f7b78535.html</url>
      
        <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sublime由于支持多种插件的功能扩展，以及其优秀的编辑能力，得到了越来越多人特别是linux平台下的程序开发人员的青睐。但是想要发挥他强大的功能优势，还是需要学习以及熟悉这些功能，在这里整理了网上关于sublime插件基础用法的相关资料，供需要的人查看。<br><a href="http://blog.saymagic.cn/2015/01/01/sublime_text_concise_course.html" target="_blank" rel="noopener">sublime简明教程</a><br><a href="http://www.jianshu.com/p/d1b9a64e2e37" target="_blank" rel="noopener">简书上有人整理的sublime学习资源</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>利用git来管理各种文档</title>
      <link href="/post/6de20b39.html"/>
      <url>/post/6de20b39.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 工作环境 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>terminator+tmux打造超级终端</title>
      <link href="/post/12fd4503.html"/>
      <url>/post/12fd4503.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>利用sublime文本编辑工具和markdown标记语言写印象笔记</title>
      <link href="/post/78cc5777.html"/>
      <url>/post/78cc5777.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 工作环境 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建属于你自己的工作环境</title>
      <link href="/post/d9ff40b9.html"/>
      <url>/post/d9ff40b9.html</url>
      
        <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文主要讲述作为一名知识工作者如何对知识进行管理以及讲述在对知识进行管理的每个环节中推荐使用的工具。下面用思维导图给出了本文的整体框架：<br><img src="/images/知识管理.jpg" alt="打造自己的工作环境"><br><a id="more"></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说到管理，必然是需要对整个知识获取以及使用保存等环节进行控制。这必然少不了对知识产生，获取，运用等过程进行分解。那么知识的运用流程可以划分成多少个阶段呢。这里，我根据你的知识需要管理这本书，将知识管理划分成以下几个阶段：知识收集、知识加工、知识保存、分享、运用、创新。</p><ul><li>知识收集：主要是收集相关的信息，为了采集相关的信息我们需要知道可以从何处获取到相关信息，如何获取。</li><li>知识加工：这个阶段主要对知识进行理解，并且做相应的链接。在这个阶段中，我们可能需要知道如何高效的学习，有时候信息可能是一本书籍，或者是好几本书籍，那么我们可能需要学会如何快速的从一本书或者是好几本书中获取我们想要的信息。</li><li>知识保存：在对知识进行加工获取到相应的信息之后，我们需要对我们获取到的信息进行总结。</li><li>分享：我们为什么要对知识进行分享呢？分享知识有几点好处：1.能够找到志同道合的伙伴。2.可以通过对知识的分享来找到自己的一些盲区，遗漏或者是误解的地方。</li><li>运用：知识最终的目的往往是用，学以致用，这才是才是我们学习的最终目标。</li><li>创新：创新往往不是凭空产生的，它往往是基于前人的工作，这里知识的积累是必不可少的。</li></ul><h2 id="知识收集"><a href="#知识收集" class="headerlink" title="知识收集"></a>知识收集</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;知识收集主要包括了收集什么，从何处收集，如何收集的问题。这里面收集什么是根据具体情况而定的，这里主要讲下从何处收集，以及如何收集的问题。绝大多数的人都知道我们可以使用搜索引擎从网上获取到我们想要的信息。当时现在搜索引擎有很多，我们在碰到具体的问题的时候该如何选择才能够最好的搜索到我们想要的信息呢。这个问题可能很多人都没有想过。大部分人使用百度这个搜索引擎就以及基本可以解决所有的问题了。但是通过百度这个搜索引擎搜索到的信息很多时候不是最好匹配我们的问题的，特别是对于知识性的问题来说。一般来说百度对于娱乐八卦这方面的信息搜索能力是比较强的，而知识性的信息则比不上谷歌。所有当我们碰到知识性的信息的时候，最好使用谷歌来进行查找。而需要其他一些娱乐八卦相关信息的时候使用百度来进行搜索，当然很多时候在使用一个搜索引擎搜索不到的时候，往往我们都会尝试使用其它搜索引擎来进行搜索，只是当我们知道哪些问题通过哪个搜索引擎能够更快搜索到的时候，我们的效率就会更高。现在谷歌在中国以及没法直接访问了，只能通过购买vpn或者是借助翻墙工具来进行访问。在这里我推荐一款不错的翻墙工具，xxnet。它是一个开源项目，可以从github上获取，具体的配置信息在github上面已经给出了。另一个获取信息的途径是通过知识共享平台来获取，现在有几个很优秀的知识共享平台，比如知乎和简书，在上面分享的信息都很有见地。除了利用互联网，其它获取信息的渠道还有很多，比如一些传统的信息获取方式，通过沟通交流等，其它方面的信息获取方式就不多说了。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一个问题就是知识收集工具，我们的信息来源是多种多样的，那么我们就相应的需要一个能够收集多个渠道的信息的工具。这里最能满足这个需求的，当属印象笔记。它提供了一个很棒的浏览器插件，印象笔记（剪藏），支持将网页信息获取到印象笔记中，还提供了从邮件，微信，微博中获取信息到印象笔记中的方式。而且它支持多个平台的信息同步，我们可以在电脑、平板、手机上使用印象笔记来收集信息。而且还支持多种类型的信息源，比如图片，网页，声音等。基本上它能将我们产生的各种信息都收集到其中，并且多平台同步。所以作为一个知识收集工具，印象笔记是首选。</p><h2 id="知识加工"><a href="#知识加工" class="headerlink" title="知识加工"></a>知识加工</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在获取到信息之后，我们需要对信息进行整理，从中提炼出有价值的信息，可以实际的给我们所用的。在这个过程中，我们需要将收集到的信息进行分类，并且对信息进行理解、学习，最后可能还需要对相关的信息进行链接，归纳总结等。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里面对信息的分类是一个很关键的步骤，如果没有对信息进行分类，那么我们就相当于少了一个标签，想找到我们想要的信息只能从中一件一件的翻，这样的话工作量是很大的。但是如果你将他们进行分类，这样就类似于建立了一个个抽屉，你只需要打开装载你想要的信息的抽屉，就可以找到你想要的信息，这样就可以很快的找到你想要的信息。另一个好处是通过分类这些信息可以真正的用起来，当你只是将收集到的信息堆积起来的时候，信息越堆越多，这样就又回到你最初找到他们的时候的状态，在一大推信息中翻找，看到这种情况，我们往往会放置不管，然后最后把他们丢弃，即使里面确实是有你想要的信息。这一步可以使用印象笔记为每类信息建立笔记本，或者是一个汇总笔记来实现，当然还要记得为每个笔记打上标签，这样的话你想要从你收集到的信息中找到你想要的信息就很快了。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二个可以说是最重要的步骤是对知识进行理解。在很多人的概念中，学习可能就是指这一步骤了。在这一步骤也包含了很多的内容，其中最关键的一点是对学习的理解。对于这些已经有相关的书籍介绍了，其中包括思维导图，如何高效学习，如何阅读一本书等等。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在理解了信息之后，我们往往会对信息做相应的链接，归纳总结等，这个步骤是在各种信息发生碰撞之后产生的，也是基于对相关信息进行检索，分类这些步骤。这个过程往往基于你对信息的理解，组合方式。在这里就不做过多的叙述。</p><h2 id="知识保存"><a href="#知识保存" class="headerlink" title="知识保存"></a>知识保存</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在对信息进行加工之后，我们往往会得到一份更加精炼的信息，这些精炼的信息往往是对理解之前的信息的一个归纳总结，往往会以笔记的形式呈现。这个过程仍然推荐印象笔记来对知识进行保存。虽然它的编辑功能用起来并不是那么方便，但是我可以借助另一个工具，markdown标记语言来快速的对信息进行编辑，而不用过多的考虑它的格式，这个也是markdown语言的初衷。具体介绍可以参看以下链接：</p><h2 id="分享"><a href="#分享" class="headerlink" title="分享"></a>分享</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们为什么要进行知识分享呢，知识分享具有如下好处：<br><em>通过对知识的分享，我们对知识的理解可以更加深刻。</em>通过知识的分享可以找到志同道合的朋友。<br>*知识的分享也是对自己的一种推销，让招聘者可以更加清晰地看到你的能力。</p><h2 id="运用"><a href="#运用" class="headerlink" title="运用"></a>运用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学以致用，这才是我们学习的最终目标，所以我们最后要将所学的知识运用起来。这里对于知识如何运用不做过多的阐述，毕竟每个领域知识的运用方式不相同。对于计算机科学领域，知识的运用主要体现在通过程序设计或者是算法设计来满足我们的需求。</p><h2 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创新是知识管理的一个重要的环节，在我们运用知识解决问题的时候，必然会遇到各种各样的难题，这个时候就需要创新性的思维，来解决相关问题，使其满足我们的需求。对于人工智能算法而言，如何将算法做得更加准确速度更加快以满足实际的应用需求，这就需要大量的创新性工作，而这些创新性工作的基础便是对于已有的知识的学习理解。而创新性的知识之后则又成为了一项成熟的技术或者知识，如此形成一个环，不断迭代，推动着我们整个社会的发展。</p><h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p>1.开源翻墙工具xxnet  <strong><a href="https://github.com/XX-net/" target="_blank" rel="noopener">https://github.com/XX-net/</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> 工作环境 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2014_SPPnet_Spatial pyramid pooling in Deep convolutional networks for visual recognition(ECCV2014)Kaiming He</title>
      <link href="/post/60c293cd.html"/>
      <url>/post/60c293cd.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>先前针对分类的卷积神经网络需要固定大小的输入，这样需要对不同尺度的图片进行裁剪或者变形之后输入，这种做法会导致目标信息丢失或者带来不期望的图像扭曲，另外预定义的尺度对于不同的目标可能会不合适。这两个问题往往会降低准确率。如果能够解决这些问题，就有可能可以进一步提高分类准确率。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><p>针对这个问题，作者分析了为什么需要固定大小输入的原因，那是因为全连接层需要固定大小输入，为此作者提出了一个解决方案叫空间金字塔pooling，作者称其为SPPnet，它被加在最后一层卷积层上，将最后一层的卷积特征池化成固定大小的输出，为此它可以对输入的任意尺度大小的图片生成固定长度的表示。</p><ul><li>spp的好处（作者通过实验验证了每个优点都对准确率的提高有帮助）<br>1）不管输入图片大小如何，都可以生成固定大小的特征。<br>2）它将特征拆分成不同粒度的bin，类似于应用了多个不同尺度的slide window，相较于单个尺度的slide window，这种方式对目标的形变更加鲁棒<br>3）应用于不同粒度的slide window可以根据输入图像的大小进行自适应的变化。</li><li>技巧及重要结论<br>1）使用多层pooling提高了准确率。提升的主要原因是多层的金字塔层对于目标的形变以及在空间中的布局更加鲁棒。<br>2）这种方式不仅可以用于多尺度测试，也可以用于多尺度的训练方式。多尺度的训练，在训练集上的拟合情况与单尺度类似，但是在测试集上有更高的准确率。具体的做法是一个epoch使用一种尺度来训练。<br>3）使用全图输入，可以提高准确率。<br>4）feature map上的多尺度多视角的测试，可以提高准确率。<br>5) 使用多个模型进行boosting可以提高准确率<br>6）作者通过实验验证了sppnet的优势与具体的网络结构是不相关的，即证明了该方法的通用性。<br>7）该方法可以加速R-CNN的特征提取并且能够取得更好或者差不多的检出准确率，应用该方法无论图片中的目标有多少，都只需应用卷积神经网络对整张图片提取一次特征，然后再根据候选目标的位置利用spp从特征图上提取固定维度的特征向量，比起R-CNN，应用该方式可以提高24倍以上。<br>8）预训练使用的类别数，样本数，以及目标尺度对于结果都有一定的影响。</li><li>一些重要的细节<br>1）对于一般的分类网络，训练的时候都需要进行减均值的预处理方式。如果使用多种尺度进行训练，作者的做法是对于Imagenet直接将均值图片resize到目标尺度；对于VOC2007和Caltech101则是直接减去128.</li></ul><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><p>空间金字塔pooling是词袋模型（Bow）的一种扩展，它将图片划分成不同粗细粒度的多个部分，然后再对每个部分进行聚合。这种思想其实在传统的计算机视觉任务中一直发挥重要的作用，在深度学习还没有出现之前，它就已经被用在图像分类和检测中。SSP可以说是BOw的一种改进，因为SSP它能够保留空间信息。使用SSP层，网络的输入可以是任意的宽高比，任意的尺度。<br><img src="/post/60c293cd/F6E8EFB4-73E4-4AB5-BCE1-37BC2511F18C.png"><br>空间金字塔pooling会根据输出的大小，以及金字塔的层数，来决定每个金字塔层pooling的size和stride。比如4X4，2X2，1X1，他会根据pooling输出的大小以及输入的大小来推断每个pooling的size和stride。假设特征图大小为13x13,某一层金字塔层的输出大小为nxn，那么滑动窗口大小为$win=\lfloor a/n \rfloor$,stride大小为$str=\lceil a/n \rceil$.在利用该方式得到每一层金字塔层的特征之后，再将所有的特征拼接在一起。下图给出了一个3层金字塔pooling的例子：<br><img src="/post/60c293cd/3851CB6B-8622-4482-B2A3-580A8EB3EFBA.png"><br>作者也尝试使用了多尺度的训练方式，为了减少不同尺度网络的切换，作者在训练一个epoch的时候，只使用一种尺度来训练。</p><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><h2 id="4-1-分类："><a href="#4-1-分类：" class="headerlink" title="4.1 分类："></a>4.1 分类：</h2><h3 id="4-1-1-ImageNet分类任务"><a href="#4-1-1-ImageNet分类任务" class="headerlink" title="4.1.1 ImageNet分类任务"></a>4.1.1 ImageNet分类任务</h3><h4 id="4-1-1-1-训练细节："><a href="#4-1-1-1-训练细节：" class="headerlink" title="4.1.1.1 训练细节："></a>4.1.1.1 训练细节：</h4><p>1.训练数据为ImageNet2012，1000类。<br>2.图像先被按比例resize成小边为256,然后再从图片的中心和四个角crop 224X224大小的图片，共5张。<br>3.对图片进行水平翻转和颜色增强。<br>4.再最后两层全链接层中加入dropout<br>5.开始的学习率设置为0.01，之后当训练loss不降的情况下，再次减少10倍。<br>6.作者使用了单个6g显存的GPU，训练了2到4周。<br>7.使用了4层金字塔。6X6,3X3,2X2,1X1<br>8.使用的网络结构有4种，ZF-5，Convent*-5，Overfeat-5，Overfeat-7</p><h4 id="4-1-1-1-实验结果及结论"><a href="#4-1-1-1-实验结果及结论" class="headerlink" title="4.1.1.1 实验结果及结论"></a>4.1.1.1 实验结果及结论</h4><ol><li>使用多层pooling提高了准确率。提升的主要原因是多层的金字塔层对于目标的形变以及在空间中的布局更加鲁棒。<br>作者使用了单尺度的训练方式，在训练和测试的时候都使用了224X224,每张图片先resize到256X256，然后从四个角落和中心crop224X224，最后再结合翻转，构建10张训练和测试图片。测试的时候对这十张图片进行求平均。<img src="/post/60c293cd/1D1BA326-06A9-4CE0-8C42-011331359C67.png"></li><li>使用多个不同尺度的图片进行训练，提高了准确率。作者将图片resize到不同尺度，然后进行训练。训练的时候使用了两个尺度224和180。测试的时候仍然使用单个尺度224.然后仍然将图片resize到256后进行crop，每张图片构建新的10张图片。最后得到的结果比起单尺度的训练方式要更好。作者也尝试使用从[180,224]，之间抽取的一个随机尺度来训练。发现结果比起只使用两个尺度的要差，但是仍然比使用一个尺度的要好。可能的原因是测试的时候是224，而使用随机尺度的话，抽取的尺度很多都不是224，跟测试数据的分布有所偏差。<img src="/post/60c293cd/7F1B12FC-B648-481A-8EAC-8F860EE79903.png"></li><li>使用全图输入，可以提高准确率。作者将图片resize成短边为256，并保持宽高比不变。训练的时候仍然使用单尺度的训练。作者比较了中心crop和保留宽高比的效果，发现使用整张图片的效果更好。虽然使用整张图片比使用单个尺度多个视角的要差，但是当把他们结合在一起的时候仍然可以进一步提高多视角的准确率。并且提取的特征质量会更好。<img src="/post/60c293cd/EEDBC3AB-5140-4864-AF20-79ABADC84AB1.png"></li><li>feature map上的多尺度多视角的测试，可以提高准确率。作者将图片的短边resize到256，然后保留图片的宽高比，并从feature map上提取多个视角的特征用于测试，然后求平均。最后与在图片上的多个视角的测试进行对比，发现他们差不多。然后作者又使用多个尺度多个视角的测试，每个尺度提取18个视角，中心，四个角落，四个每个边的中心，整张图片，以及他们的水平翻转。作者通过多视角多尺度的方式，又将准确率进一步提高。<img src="/post/60c293cd/02E45D0F-B364-4A62-9AC7-0ACCA597D634.png">Krizhevsky的方法取得了ILSVRC2012比赛的冠军，而Overfeat，Howard,Zeiler&amp;Fergus则是ILSVRC2013比赛的最好的几个方法。</li><li>使用多个模型进行boosting可以提高准确率<br>下图是ILSVRC2014比赛的排名情况，作者使用了7个模型进行融合使得结果进一步提升。并取得第三名的成绩。<img src="/post/60c293cd/5CADD6BD-BA5E-4A68-8B94-847CE68A8DB1.png"></li></ol><h3 id="4-1-2-VOC2007分类任务"><a href="#4-1-2-VOC2007分类任务" class="headerlink" title="4.1.2 VOC2007分类任务"></a>4.1.2 VOC2007分类任务</h3><h4 id="4-1-2-1-训练细节"><a href="#4-1-2-1-训练细节" class="headerlink" title="4.1.2.1 训练细节"></a>4.1.2.1 训练细节</h4><ol><li>训练数据包括9963张图片，共20个类别。其中5011张图片用于训练，剩下的图片用于测试。性能评估方式使用mAP(mean Average Precision)</li><li>作者使用在ImageNet进行训练的网络来提取图片的特征，然后将这些特征用于重新训练SVM分类模型，训练的时候没有做任何数据增强，只对特征进行L2标准化。</li></ol><h4 id="4-1-2-2-实验结果和结论"><a href="#4-1-2-2-实验结果和结论" class="headerlink" title="4.1.2.2 实验结果和结论"></a>4.1.2.2 实验结果和结论</h4><img src="/post/60c293cd/E1149716-CC84-4242-9350-F45EC599A9C2.png"><ol><li>图中a列表明：越深层的特征越好</li><li>图中a，b列表明：使用fc层的特征获得更好的增益，主要由于多层的pooling</li><li>图中c列表明：全图输入的特征更好</li><li>图中d列使用尺度392更好的原因主要是：在VOC2007中目标比较小，而在Imagenet中目标比较大。这个结果表明：目标尺度会影响到分类的准确率，而SPP-net可以部分处理这个尺度不匹配的问题。</li><li>图中e列为将网络结构替换为Overfeat-7，并使用多尺度训练得到的结果</li></ol><h3 id="4-1-3-Caltech101-分类任务"><a href="#4-1-3-Caltech101-分类任务" class="headerlink" title="4.1.3 Caltech101 分类任务"></a>4.1.3 Caltech101 分类任务</h3><h4 id="4-1-3-1-训练细节（没有说清楚怎么训练）"><a href="#4-1-3-1-训练细节（没有说清楚怎么训练）" class="headerlink" title="4.1.3.1 训练细节（没有说清楚怎么训练）"></a>4.1.3.1 训练细节（没有说清楚怎么训练）</h4><ol><li>该数据集包括了9144张图片，共102个类别，其中1个是背景。</li><li>作者从每个类别中随机抽取30张图片用于训练，抽取50张图片以上用于测试。作者按照这种数据划分方式重复了10次，并将结果取平均，作为最后的结果。</li></ol><h4 id="4-1-3-2-实验结果和结论"><a href="#4-1-3-2-实验结果和结论" class="headerlink" title="4.1.3.2 实验结果和结论"></a>4.1.3.2 实验结果和结论</h4><img src="/post/60c293cd/E7485E63-A5D7-4265-B3DC-E99E22DD40B2.png"><ul><li>从结果来看Caltech101与voc2007有类似的结果：</li></ul><ol><li>从a，b可以看出spp-net要比no-spp net效果要好</li><li>从c，b可以看出全图输入要比crop好</li></ol><ul><li>但是也有一些不同的结论：</li></ul><ol><li>在Caltech101分类任务中全连接层的准确率要比spp层低。</li><li>在选择的多个测试尺度中224的尺度是最好的，这个主要是因为在Caltech101中，目标占据图像的区域与ImageNet差不多。</li><li>作者也尝试将图片reshape 成224X224，这样虽然保留了全图信息，但是会带来扭曲。从结果来看效果没有使用全图的好。使用reshape的准确率是89.91%，而使用全图输入没有扭曲的准确率达到91.44%。</li></ol><p>下图给出了不同方法在Voc2007分类数据集和Caltech101分类数据集上的结果。其中VQ，LCC，FK都是基于金字塔匹配的，剩下都是基于深度网络的。在基于深度网络的方法中，Oquab等人的方法和Chatfield等人的方法采用了finetune和多视角的测试。本文提到的方法只采用了单张全图，并且没有使用finetune，但是取得了与他们类似的结果。而在Caltech101测试集上，作者提出的方法远远超出当前最好的方法。<br><img src="/post/60c293cd/6EF61FF2-CB1B-4E35-9B1D-423B7A1275AD.png"></p><h2 id="4-2-检测："><a href="#4-2-检测：" class="headerlink" title="4.2 检测："></a>4.2 检测：</h2><h3 id="4-2-1-RCNN方法简介"><a href="#4-2-1-RCNN方法简介" class="headerlink" title="4.2.1 RCNN方法简介"></a>4.2.1 RCNN方法简介</h3><ol><li>R-CNN首先利用selective serach从每张图片中提取约2000个候选窗。</li><li>然后将每个候选窗reshape成227X227，并利用一个预先训练好的卷积网络去提取特征。</li><li>利用提取到的特征去训练一个2分类的SVM分类器。<br>虽然RCNN相对于先前的方法，效果很好，但是速度比较慢，每张图片需要对2000个候选窗口应用卷积网络去提取特征。特征提取是测试速度的主要瓶颈。</li></ol><h3 id="4-2-2-将SPP应用于RCNN中及训练细节"><a href="#4-2-2-将SPP应用于RCNN中及训练细节" class="headerlink" title="4.2.2 将SPP应用于RCNN中及训练细节"></a>4.2.2 将SPP应用于RCNN中及训练细节</h3><img src="/post/60c293cd/FDC60DCD-C90A-4D0B-8C7C-027E3F716633.png"><ul><li><p>作者将空间金字塔pooling应用到R-CNN中，可以提高训练和测试的速度，并且提高准确率。具体的：<br>1）使用“fast”模式的selective search从每张图片中提取2000个候选框<br>2）将图片resize到min(w,h)=s<br>3）利用单尺度训练的ZF-5网络直接提取整张图片的特征。<br>4）利用4层空间金字塔pooling提取每个候选框的特征，生成12800维的特征向量。<br>5）最后利用全链接层的特征提供给svm分类器，然后利用每个类别的svm分类器来得到每个类别的得分。训练svm的时候，作者使用了ground-truth作为正样本，使用与ground truth的IOU小于0.3的候选框作为负样本。并且负样本之间的IOU要小于0.7.最后使用hard negative mining的方式来训练svm，困难样本挖掘只迭代一次。<br>6）在测试阶段，在对每个候选框进行打分之后，作者使用了非最大化抑制的方式阈值为0.3，去除掉一些冗余的框。 </p></li><li><p>使用该方式，利用多尺度进行测试，可以进一步提高效果。作者发现，最好的一种方式是，对于不同尺度的候选窗口，只对该尺度下最接近224X224的候选窗口进行特征提取。 这样的话不同尺度的输入只需跑一次卷积神经网络对全图进行特征提取，然后让各个候选窗口按在该尺度下最接近224X224的原则分配到各个尺度下，然后应用spp对候选窗口进行特征提取。</p></li><li><p>作者也尝试使用fine-tune和bounding box回归。<br>1.fine-tune的时候作者只fine-tune全链接层，固定其他层。<br>2.fc8使用了方差为0.01的高斯分布来进行初始化。<br>3.正样本为IOU大于0.5，负样本为与正样本的IOU小于0.5大于等于0.1，每个batch包括1/4的正样本，3/4的负样本。<br>4.开始250k次迭代使用学习率为0.0001，之后50k次迭代的学习率设置为0.00001<br>5.使用bounding box 回归作为后处理方式。使用的特征是来自与conv5的空间金字塔pooling。使用的样本为IOU大于0.5的候选框。</p></li></ul><h3 id="4-2-3-实验结果和结论"><a href="#4-2-3-实验结果和结论" class="headerlink" title="4.2.3 实验结果和结论"></a>4.2.3 实验结果和结论</h3><h4 id="4-2-3-1-VOC-2007测试集"><a href="#4-2-3-1-VOC-2007测试集" class="headerlink" title="4.2.3.1 VOC 2007测试集"></a>4.2.3.1 VOC 2007测试集</h4><h5 id="1-实验结果"><a href="#1-实验结果" class="headerlink" title="1. 实验结果"></a>1. 实验结果</h5><ul><li>R-CNN使用Alex-5<img src="/post/60c293cd/7260FF56-9F92-45EE-AF1E-C5518975E377.png"></li><li>R-CNN使用ZF-5<img src="/post/60c293cd/0A4F8BF1-BAC3-4523-B919-09122750C955.png"></li><li>不同的目标检测方法在20类目标的map<img src="/post/60c293cd/36D86482-5A20-4E96-9115-98C91E13B691.png">作者使用另外一种候选生成方式EdgeBoxes进行测试，mAP为52.8.这是因为训练的时候使用的是Seletive search。当训练的时候同时使用Selective search和EdgeBoxes，测试时使用EdgeBoxes时，mAP达到了56.3.这是因为训练样本增加了。</li></ul><h5 id="2-结论"><a href="#2-结论" class="headerlink" title="2. 结论"></a>2. 结论</h5><p><strong>从结果来看，SPP跟RCNN差不多，但是速度方面SPP要比RCNN快很多</strong>。</p><ul><li>通过模型组合可以提高效果<br>作者使用相同的网络结构，不同的初始化方式在ImageNet上训练了另一个网络，然后重复上面的检测算法，另一个网络在整体的mAP上与原先的差不多，但是有11个类别要比原来好。两个模型存在较大的差异，可以互补，所以融合后效果提升了不少。<br>组合方式为：首先使用两个模型分别为测试图片上的所有候选进行打分；然后将结果合并在一起做非最大化抑制。<br>作者进一步发现模型组合的提升主要来自与卷积层。作者使用同个模型不同初始化方式对整个卷积网络进行finetune得到了提升，但是使用同个模型不同的初始化方式来fine-tune全连接没有提升。说明提升主要来自于卷积层。<img src="/post/60c293cd/B2AED20A-9D77-430D-90A2-499D50073A94.png"></li></ul><h4 id="4-2-3-2-ILSVRC-2014测试集"><a href="#4-2-3-2-ILSVRC-2014测试集" class="headerlink" title="4.2.3.2 ILSVRC 2014测试集"></a>4.2.3.2 ILSVRC 2014测试集</h4><h5 id="1-训练细节"><a href="#1-训练细节" class="headerlink" title="1. 训练细节"></a>1. 训练细节</h5><ul><li>ILSVRC 2014检测数据集，包括200个类别。不允许使用imagenet 1000类数据集。</li><li>450k训练数据；20k验证数据；40k测试数据</li><li>由于数据量以及类别数都比image net要少，这样的话性能没有使用imagenet来预训练好。为此作者使用了提供的499类的数据集。</li><li>由于目标尺度的分布在499类的CLS上是0.8，而在DET是0.5，为此作者把DET数据集resize成min(w,h)=400,而不是256.然后随机crop 224X224用于训练，只有当它与ground truth的重叠大于0.5时，才用来训练。</li></ul><h5 id="2-重要结论："><a href="#2-重要结论：" class="headerlink" title="2. 重要结论："></a>2. 重要结论：</h5><ul><li><p>类别数，样本数，以及目标尺度对于结果都有一定的影响。<br>作者对比了利用ILSVRC2014数据集的不同标签（大类标签有200，子类标签有499）来进行预训练的情况下，在Pascal VOC数据集上的检出效果。作者使用了pool5特征进行训练，在使用imagenet进行预训练的情况下，mAP是43.0%，在使用ILSVRC2014 200类进行与训练的情况下，mAP降到了32.7%，而使用ILSVRC2014 499类进行预训练的情况下，mAP提高到了35.9%。从结果来看，虽然使用200类和499类训练数据量并没有变化，但是效果却提升了，<strong>说明更多的类别有助于提高特征的质量</strong>。另外作者尝试了使用min(W,H)=400来训练，替代使用256来训练，mAP进一步提高到了37.8%。但是仍然没有使用imagenet进行预训练的好，说明数据量对于深度学习的重要性。</p></li><li><p>通过模型的组合可以进一步提升效果。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object detection </tag>
            
            <tag> SPPnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2014_Rich feature hierarchies for accurate object detection and semantic segmentation(CVPR2014)Ross Girshick</title>
      <link href="/post/2f9d64a9.html"/>
      <url>/post/2f9d64a9.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景及意义（动机）"><a href="#一、背景及意义（动机）" class="headerlink" title="一、背景及意义（动机）"></a>一、背景及意义（动机）</h1><p>这篇论文中作者提出了一个很牛逼的目标检测算法R-CNN，直接将PASCAL VOC数据集上的最好性能提高了30%。将深度学习应用到目标检测，主要会面临两个挑战，一个是如何使用深度学习去定位目标位置；另一个是在数据缺乏的情况下如何训练高质量的模型。在这个方法中作者结合了两个关键的思想来解决这两个问题：1）为了去定位和分割目标，可以应用一个能力很强的卷积神经网络去筛选候选目标 2）当标注的训练数据缺乏的情况下，运用辅助任务进行监督预训练和指定任务的fine-tune，可以让性能有一定的提升。</p><h1 id="二、使用什么方法来解决问题（创新点）"><a href="#二、使用什么方法来解决问题（创新点）" class="headerlink" title="二、使用什么方法来解决问题（创新点）"></a>二、使用什么方法来解决问题（创新点）</h1><ul><li>创新点<br>1）提出一个能有效进行目标检出的框架RCNN，在PASCAL VOC 2012数据集上，将mAP提高30%<br>2）当标注的训练数据缺乏的情况下，运用辅助任务进行<strong>监督预训练</strong>和指定任务的fine-tune，可以让性能有一定的提升。将mAP提高了8个点。</li><li>存在的问题<br>1）需要对候选区域的特征进行存储，需要较多存储空间<br>2）对每个候选区域都需要运行一次卷积神经网络来提取特征，速度慢</li><li>重要结论<br>1）对于深层神经网络来说，越高层的特征越是任务相关的，而低层的特征则更加的通用。</li></ul><h1 id="三、方法介绍"><a href="#三、方法介绍" class="headerlink" title="三、方法介绍"></a>三、方法介绍</h1><img src="/post/2f9d64a9/60C2B1DE-5087-4DB1-A7FF-9E37A633B311.png"><h2 id="3-1-R-CNN主要包括了3个模块："><a href="#3-1-R-CNN主要包括了3个模块：" class="headerlink" title="3.1 R-CNN主要包括了3个模块："></a>3.1 R-CNN主要包括了3个模块：</h2><ol><li>首先通过一个类别无关的proposal方法得到候选框。<br>这里proposal方法作者主要使用了selective search，从每张图片中提取2000个候选框。</li><li>然后将候选框reshape到指定输入大小并用一个卷积神经网络提取固定长度的特征。最后利用SVM来对每个目标进行打分，每个类别训练一个svm。<br>特征的提取主要使用了一个卷积神经网络，从每个区域提取4096维的特征。输入图片大小为227X227，预处理方式为减去均值图片，网络结构为5层卷积2层全连接层。</li><li>最后对每个类别运用非最大化抑制将冗余的检测框剔除掉。非最大化抑制主要是先找到得到最高的目标，如果它周围的目标的得分超过类别阈值，但是它与最好得分目标的IOU超过了某个阈值，那么将该目标剔除掉。</li></ol><h2 id="3-2-监督预训练和指定领域的finetune"><a href="#3-2-监督预训练和指定领域的finetune" class="headerlink" title="3.2 监督预训练和指定领域的finetune"></a>3.2 监督预训练和指定领域的finetune</h2><ol><li>监督预训练：作者使用了ILSVRC2012作为辅助的数据集，并利用该数据集图像层面的标注训练一个分类模型。</li><li>在检出训练集上进行finetune：将最后一层分类层替换为N+1类，其中1类为背景。将候选框与目标的IOU大于等于0.5作为正样本，其他作为负样本。利用先前的预训练模型来fine-tune，并把学习率设为原来的十分之一为0.001.在每次迭代中，均匀采阳32个正样本和96个负样本，batchsize大小为128。</li></ol><h2 id="3-3-目标类别分类器"><a href="#3-3-目标类别分类器" class="headerlink" title="3.3 目标类别分类器"></a>3.3 目标类别分类器</h2><ol><li>fine-tune完之后，作者没有使用finetune的网络的结果作为最终的结果。而是利用fine-tune完成的网络作为特征提取器，并运用histogram intersection kernel svm分类器，来得到最终每个目标的类别。这里作者训练svm的时候将训练集和验证集一起拿来训练。</li><li>这里负样本如何确定很关键，当使用IOU小于0.5为负样本时，mAP下降5个点，使用IOU<br>为0时为负样本，mAP下降4个点，最终作者通过网格搜索确定了IOU小于0.3为负样本最优。正样本则为ground-truth bounding boxes。</li><li>最后作者采用hard negative mining方法来加速拟合。通过利用困难负样本挖掘，只需要跑一个epoch，svm就拟合完成了。</li></ol><h1 id="四、实验结果及重要结论"><a href="#四、实验结果及重要结论" class="headerlink" title="四、实验结果及重要结论"></a>四、实验结果及重要结论</h1><h2 id="4-1-PASCAL-VOC-2010-12测试集"><a href="#4-1-PASCAL-VOC-2010-12测试集" class="headerlink" title="4.1 PASCAL VOC 2010-12测试集"></a>4.1 PASCAL VOC 2010-12测试集</h2><p>对于VOC2010-12，作者使用VOC2012训练集进行finetune，使用VOC2012训练集和验证集训练SVM模型。<br><img src="/post/2f9d64a9/9CC8733B-7E4E-42F7-8883-16071AD41E8E.png"></p><h2 id="4-2-ILSVRC2013检测数据集"><a href="#4-2-ILSVRC2013检测数据集" class="headerlink" title="4.2 ILSVRC2013检测数据集"></a>4.2 ILSVRC2013检测数据集</h2><img src="/post/2f9d64a9/1AA98F88-ECF5-460E-B945-1716902E643C.png"><p>在ILSVRC2013检测数据集上，OverFeat，NEC-MU,UvAEuvision，Toronto A 和UIUC-IFP都使用了卷积神经网络，但是效果却比作者的方法差很多，说明不同的用法对结果影响很大。</p><h2 id="4-3-可视化学习到的特征"><a href="#4-3-可视化学习到的特征" class="headerlink" title="4.3 可视化学习到的特征"></a>4.3 可视化学习到的特征</h2><img src="/post/2f9d64a9/D112FB1E-2CD7-418F-BC34-CA312FBE1B65.png"><p>作者选取了pool5层中6个激活单元，作为proposal的得分，执行非最大化抑制，最后对所有的结果精细排序，选择了激活值最大的前16个图片进行可视化。说明学习到的特征具有较好的判别性。</p><h2 id="4-4-Ablation-studies"><a href="#4-4-Ablation-studies" class="headerlink" title="4.4 Ablation studies"></a>4.4 Ablation studies</h2><img src="/post/2f9d64a9/3174984B-5572-4BC4-80C8-79C8F1D67CB2.png"><p>作者对fine-tune与不fine-tune不同网络层的特征进行了比对，发现pool5的特征相对于fc层比较通用，而fc层的特征则更多是任务相关的。</p><h3 id="4-4-1-在没有fine-tuning的情况下，比较不同层特征带来的效果。"><a href="#4-4-1-在没有fine-tuning的情况下，比较不同层特征带来的效果。" class="headerlink" title="4.4.1 在没有fine-tuning的情况下，比较不同层特征带来的效果。"></a>4.4.1 在没有fine-tuning的情况下，比较不同层特征带来的效果。</h3><ul><li>为了理解哪一层特征对于检出性能比较关键，作者分别用CNN（只使用ILSVRC2012进行预训练）的最后3层来测试VOC2007数据集上检出任务的效果。结果表明：pool5只使用了卷积特征就可以达到较好的效果，说明主要带来效果的是卷积层而不是全连接层；另外fc7要比fc6效果差，说明越高层的特征越是任务相关的。</li></ul><h3 id="4-4-2-在fine-tuning的情况下，比较不同层特征带来的效果。"><a href="#4-4-2-在fine-tuning的情况下，比较不同层特征带来的效果。" class="headerlink" title="4.4.2 在fine-tuning的情况下，比较不同层特征带来的效果。"></a>4.4.2 在fine-tuning的情况下，比较不同层特征带来的效果。</h3><ul><li>利用VOC2007trainval集对模型进行finetune，并利用finetune好的模型的最后3层来测试VOC2007数据集上检出任务的效果。利用finetune后的特征来训练，mAP的提升十分明显，足足提升了8个点。finetune后fc6和fc7比起pool5层提升的效果更加明显，进一步说明越高层的特征越是任务相关的，而低层特征则是更加的通用。</li></ul><h3 id="4-4-3-不同网络结构对结果的影响"><a href="#4-4-3-不同网络结构对结果的影响" class="headerlink" title="4.4.3 不同网络结构对结果的影响"></a>4.4.3 不同网络结构对结果的影响</h3><img src="/post/2f9d64a9/950B6E7F-961C-4485-9EF3-EAA4E7912EE2.png"><p>T-net比O-net效果更好，但是也更加耗时</p><h3 id="4-4-4-使用Bounding-box-回归可以提高定位的准确率"><a href="#4-4-4-使用Bounding-box-回归可以提高定位的准确率" class="headerlink" title="4.4.4 使用Bounding-box 回归可以提高定位的准确率"></a>4.4.4 使用Bounding-box 回归可以提高定位的准确率</h3>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> 计算机视觉 </category>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object detection </tag>
            
            <tag> rcnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/post/4a17b156.html"/>
      <url>/post/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
