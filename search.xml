<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2015_Fast R-CNN(ICCV2015)Ross Girshick]]></title>
    <url>%2Fpost%2Fa58d66d1.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）R-CNN虽然效果好，但是存在很多缺点：1）训练过程需要多个阶段。2）训练时间缓慢而且需要较大的存储空间。3）测试时间较慢。虽然SPPnet对R-CNN做了些许改进（针对R-CNN对于每个proposal都需要提取特征的情况进行优化，让每张图片只需要提取一次特征，作者主要是在特征提取和分类器之间最后的pooling层替换为一个空间金字塔pooling层），但是仍然存在很多缺点没有改进。针对R-CNN的这些缺点，作者提出一个更加快速，有效的目标检测方法叫Fast R-CNN。该方法在训练和测试速度以及性能方面，都要优于先前方法。该方法训练要比R-CNN快9倍，测试要快213倍；相较于SPPnet，训练快了3倍，测试快了10倍，并且都更加准确。 二、使用什么方法来解决问题（创新点）主要贡献 提出fast R-CNN框架，训练速度，测试速度以及准确率都比R-CNN和SPPnet要好 训练过程不需要多个阶段，并且使用了多任务loss（回归和分类放在一起训练）。 采用端到端的方式，训练可以更新所有网络层。（主要采用了ROI pooling层） 不需要把特征预先存储起来。 重要结论 多任务有助于提高性能。 多尺度虽然可以提升效果，但是提升的幅度并没有使用大模型并使用单尺度的高，反而会增加测试时间。所以权衡下，用单尺度，大模型会更好。 更多的数据更多的迭代次数效果更好。 softmax与svm效果差不多，但是使用softmax速度更快 更多的候选框，无法达到更好的效果 三、方法介绍 3.1 网络的框架和训练 fast rcnn的输入是整一张图片 在最后的卷基层使用了ROIpooling从对应的proposal提取特征送入全连接层。 使用两个分支，1个预测类别，一个用来修正当前类别的bounding box的位置。 3.2 RoI pooling 层：ROI pooling层是单层的空间金字塔pooling层。具体的会根据输入的卷积feature map的大小，以及输出的feature map的大小，计算每个子窗口的大小。然后对每个子窗口利用max pooling操作得到响应值。 3.3 对预训练网络的修改： 最后的maxpooling层改为使用ROIpooling，H=W=7 最后的全连接层和softmax层改为两个任务，一个是全连接层和softmax，一个是全连接层和类别相关的bounding-box回归。 网络修改为2个数据的输入，一个是图像，一个是图像的ROI。 3.4 利用检测数据来fine-tune整个网络：对于RCNN和SPPnet来说，fine-tune整个网络效率是很慢的，因为它们的采用策略是从每个图片中提取一个ROI用来训练。为此作者提出一个更加有效的训练方式，假如训练的mini-batch的ROI数为R，图片的候选数为N，则使用提取R/N张图片用于训练。采用这种方式的一个问题可能是过拟合问题，但是从结果来看并不会这样。 在fine-tune的时候，fast rcnn联和优化了两个目标，分类与回归。对于分类，采用了一个softmax with loss 来训练，对于回归作者采用了smooth L1 loss，回归的目标使用了ground truth 和proposal 的一个变换，使其对尺度不敏感。两个任务的权重都为1.0 每个minibatch的采样方式：从两张图片中采128个样本，每个图片64个样本。其中取1/4为正样本，即IOU大于0.5的proposal。3/4负样本，为IOU在0.1到0.5之间的proposal。数据增强方式只是按0.5的概率进行水平翻转。 梯度传播方式：在ROIpooling层会记录每个输出来自于哪一个输入，然后在方向传播时，将相同输入的梯度累加起来，继续往下传播。 用于分类和回归的全链接层分别采用标准差为0.01和0.001，0均值的高斯分布来进行初始化。bias初始化为0.所有的层的权重使用per-layer学习率为1.0，bias使用per-layer学习率为2.0。在VOC07和VOC12数据集上，头30k个迭代使用一个全局的学习率为0.001，后10k次迭代使用一个全局学习率为0.0001.当训练集更大的时候，会使用更多次迭代。动量设置为0.9，权重衰减系数设置为0.0005. 3.5 测试：测试的时候，首先提取proposal，然后利用fast rcnn得到每个proposal属于每个类别的得分，以及对应的bounding box 偏移。然后利用每个类别的得分以及非最大化抑制得到最后结果。使用truncated SVD可以加快测试时间 四、实验结果及重要结论 多任务学习提高mAP 多尺度虽然可以提升效果，但是提升的幅度并没有使用大模型并使用单尺度的高，反而会增加测试时间。所以权衡下，用单尺度，大模型会更好。 更多的数据更多的迭代次数效果更好。 softmax与svm效果差不多，但是使用softmax速度更快 更多的候选框，无法达到更好的效果]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Fast-RCNN</tag>
        <tag>tow-step detector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017_Xception:Deep learning with Depthwise Separable Convolutions_CVPR2017_CholletF]]></title>
    <url>%2Fpost%2Fd9144852.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）普通的卷积可以理解为同时映射通道之间以及空间之间的关系，而Inception模块则是让这个过程更加简单，它首先通过一系列的1X1卷积映射通道之间的关系，将输入数据映射到3到4个分离的空间；然后再通过3X3或者5X5卷积在这些小的3D空间中，映射通道之间和空间之间的相互关系。逐深度分离卷积可以理解为最大化towers的Inception 模块。是不是跨通道的关系和跨空间的关系可以完全解耦，是不是利用逐深度离散卷积替换掉Inception模块效果更好？基于这样的假设作者提出了一个新的网络结构，主要是将Inception模块替换为逐深度分离卷积，作者将其称为Xception。最后作者通过了实验验证该结构的有效性，在同等网络复杂度的情况下，Xception网络在Imagenet数据集上稍微比Inception V3好，并且在更大的数据集上提升效果更加明显。 二、使用什么方法来解决问题（创新点） 考虑最简单版本的Inception模块，只是用卷积tower,不使用平均tower，如figure2。这样就可以将其看做是一个大的1x1卷积在加上一个group卷积，如figure3。如果这个group达到极限，如figure4，也即每个group只有一个通道，那就是它的极限形式。这个形式与逐深度离散卷积基本等同。它与逐深度离散卷积的不同之处在于1）计算顺序不同，逐深度离散卷积是先计算逐通道卷积，再利用1X1卷积去计算跨通道卷积。2）逐深度离散卷积在计算逐通道卷积的时候一般不使用非线性函数。第一点一般没有影响，第二点可能对性能有所影响，所以后面作者对此进行了研究。通过这种变换关系，是不是利用逐深度离散卷积替换掉Inception模块更好呢，为此有了这个新的网络结构Xception.在这篇论文中作者只是研究了这种极限形式，而普通卷积和逐通道离散卷积的中间形式是否更好作者并没有进行研究。 三、实现细节3.1 网络结构 整体的网络结构如figure5所示，整个特征提取网络有36层，最后加入一个逻辑回归分类器。这36层被划分成14个模块，除了第一个和最后一个，每一个都有residual connections。代码https://keras.io/applications/#xception 四、实验4.1 模型及数据集说明由于Inception V3和提到的Xception模型复杂度差不多，所以作者选择了Inception V3来做对比实验。作者在两个任务上做对比试验，一个是ImageNet分类任务，该任务是包含1000个类别的单标签分类任务；另一个是JFT分类任务，该任务是有17000个类别的多标签分类任务。JFT数据集包括了350million张高分辨率的图片，总共类别数为17000个。为了评估模型在JFT上训练的效果，作者使用了另一个辅助数据集FastEval14k来进行评估。该数据集包括14000张图片，共6000个类别，平均每张图片标签数为36.5个。评估方式是使用带权重的MAP@100,这个score主要来自于该图片在社交媒体上出现的频率。 4.2 参数设置On ImageNet: Optimizer: SGD Momentum: 0.9 Initial learning rate: 0.045 Learning rate decay: decay of rate 0.94 every 2 epochs On JFT: Optimizer: RMSprop Momentum: 0.9 Initial learning rate: 0.001 Learning rate decay: decay of rate 0.9 every 3,000,000 samples 两个模型在这两个任务上都使用了相同的参数配置，而且这些参数配置是适配于Inception V3的，这就撇开了故意去选择对于Xception更好的参数 正则化: weight decay :Inception V3:4e-5xception: 1e-5 Dropout:Imagenet:0.5 bothJFT: none both Auxiliary loss:None 训练平台为60块 K80 GPU Imagenet采用同步梯度下降，跑3d JFT采用异步梯度下降，跑1个月没完全拟合，完全拟合需要3个月。 4.3 结果比较4.3.1 分类性能的比较 所有的评估都是用单模型单尺度，并且Imagenet任务都是验证集上的结果，JFT的结果是运行了1个月后的结果不是完全拟合的结果。从结果上可以看到，在这两个任务上，Xception都要优于Inception V3。并且在JFT上，提升更为明显，作者认为这是因为Inception V3的参数更加适配Imagenet分类任务导致的，如果Xception也进行参数的调优，提升效果会更加明显。 4.3.2 模型大小和速度的比较 从Table3中可以看出两个模型的大小相似，但是Xception要稍微比Inception V3慢。从模型大小可以看出Xception性能的提升不是来自于模型表达能力的增加，而是来自于逐深度离散卷积带来的性能提升。 4.3.3 residual connetions的影响 从结果可以看出residual connection确实有助于模型的拟合，不管在速度或者是模型性能，都带来了提升。residual connections只是对于这种网络结构很重要，但是并不是说主要的提升是它。作者也试了在vggnet中加入逐深度离散卷积，在参数数与InceptionV3接近的情况下，在JFT任务上也超过了Inception V3。说明该结构确实有用。 4.3.4 逐通道卷积操作后加入激活函数对结果的影响 从结果可以看出，在逐通道卷积操作之后不使用任何激活函数，效果是最好的，不管是拟合速度还是分类性能都更好。这可能是对于多通道的卷积使用激活函数更有用，而单通道的卷积不使用激活函数更加有用。]]></content>
      <categories>
        <category>深度学习</category>
        <category>模型结构优化</category>
      </categories>
      <tags>
        <tag>xception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016_R-FCN_Object Detection via Region-based Fully Convolutional Networks(NIPS2016)Jifeng Dai]]></title>
    <url>%2Fpost%2Ffe660c1.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）先前主流的目标检测方法，都是使用了一个全图片共享计算的全卷积子网和一个没有共享计算的ROI分类子网。这样的话没有共享计算的ROI分类子网往往比较耗时。最近最新的分类网络都被设计为全卷积网络的形式,比如resnet。那是不是在目标检测任务中也可以只使用全卷积网络进行共享计算来进行检测呢？作者发现如果直接运用全卷积网络来进行检测效果并不理想，这是因为分类任务是希望变化不敏感的，而检测任务则是希望变换敏感的，直接应用分类网络来进行检测效果不大理想。之前resnet作者为了获得比较好的准确率将ROIpooling插入到cov4和conv5卷积层之间，这样将网络变成了变换不敏感的全卷积子网和变换敏感的ROI子网，这样仍然与之前做法没区别，ROI子网仍然无法共享计算 。为此在这篇论文中，作者提出了基于区域的全卷积神经网络，用于更加准确，高效的目标检测任务。跟faster rcnn相比，RFCN不需要在每个区域运用一个子网络来对目标进行进一步的筛选，而是直接运用一个全图共享的全卷积神经网络直接得到结果，因此可以更快。为了达到这个目标，作者提出了一个位置敏感得分图，去解决在目标检测任务中，需要对变换敏感，而在分类任务中需要对变换不敏感这样一个两难的问题。该方法可以很自然地使用全卷积分类网络作为backbone。作者使用了resnet101，在PASCAL VOC数据集上达到了比较好的结果（在2007数据集上mAP为83.6%，在2012数据集上mAP为82.0%），并且速度比faster rcnn+resnet101快了2.5-20倍，平均每张图片只需170ms。 代码:https://github.com/daijifeng001/r-fcn 二、使用什么方法来解决问题（创新点） 为了解决分类网络对变换不敏感而检出任务则需要对变换敏感这个两难问题，作者使用一个指定通道数的卷积层作为全卷积网络的输出，该卷积层中每个通道都编码进了不同类型目标的位置信息，整个卷积层构成了位置敏感得分图。之后作者在该卷积层上面放置了位置敏感的池化层（RsROI pooling）去整合各个类型目标得分图的信息，这个操作并没有引入额外的参数。整个过程都是可以端到端训练并且是全卷积可共享计算的。如Figure1所示，C为类别数，k为空间划分的粒度，也即将目标所在区域划分成$k \times k$的网格，则每个目标将会有$k^2$个得分图，每个得分图对目标的不同位置做出响应，最终将会有$(C+1) \times k \times k $个得分图，多加的一类为背景。最后在RPN网络获取到候选目标位置的时候，会利用RsROI pooling将各个类别的各个位置的得分进行融合，作者采用取平均的方式，最后再使用softmax得到最终目标的分类置信度。对于目标的定位，作者也采用类似的方式，加多一个得分图分支，不过这里作者采用的是类别无关的方式，也即得分图通道数为$4 \times k \times k$。 三、实现细节 作者采用了2阶段的目标检测策略，先利用RPN网络得到候选框，然后再利用R-FCN对候选目标进一步的筛选和定位，整个检出系统如Figure2所示。给定一个候选的ROI区域，R-FCN主要去区分该区域目标的类别以及精确定位目标。在R-FCN中，所有的可学习到的权重层都是卷积层。最后的卷积层为每个类别生成一个k平方channel的位置敏感得分图。这个得分图编码了位置信息进去。在位置敏感得分图之后，作者使用了一个RsROI pooling层去整合各个类别的各个位置的得分信息，最后再使用softmax得到每个类别的置信度。 3.1 Backbone architeture作者采用ResNet-101作为backbone。首先利用Imagenet对其进行预训练，然后将最后的global average pooling和1000类的fc层移除，并在最后一层2048-d卷积层后放置一个通道数为1024-d的1x1卷积层对最后一层进行降维，最后再放置$(c+1) \times k \times k$-d的得分图。 3.2 Position-sensitive score maps &amp; Position-sensitive RoI pooling为了编码位置信息到每个ROI区域，作者将ROI区域划分为$k x k$个网格。对于大小为$w x h$的ROI区域，每个网格大小约为为$(w/k) \times (h/k)$。对于每个类别的每个网格信息，作者通过以下的RsPooling操作得到其得分：这里$Z_{i,j,c}$是每个channel的响应值，$x_0$和$y_0$是ROI区域左上角的位置。每个类别每个位置$(i，j)$的得分，是通过将该类别的对应位置的channel取出来，然后将与该位置相关的响应值求和取平均得到。在得到每个位置的得分之后，最后的结果通过对所有位置求平均得到。然后得到每个类别的score之后，再利用softmax得到最后每个类别的概率。loss采用交叉熵loss。对于回归任务的处理，作者也采用了类似的方式，但是使用了一个类别无关的回归策略，当然也可以使用一个类别相关的。为此接入的一个卷积层的channel数为4乘以k的平方。然后通过聚合每个位置每个坐标的得分，得到一个4维的向量。这里位置的变换参考R. Girshick. Fast R-CNN. In ICCV, 2015. 3.3 孔洞卷积 在FCN中使用孔洞卷积可以提高语义分割的效果。由于现在模型也是全卷积的形式，也可以利用孔洞卷积带来的增益。另外作者也将resnet101的stride从原来的32变成16，以此来增加得分图的分辨率。在conv4 stage及之前的卷积作者保持不变，而conv5的第一个block作者将stride从原来的2调整为1，并将conv5 stage的所有的卷积滤波器变成了孔洞卷积。作者将RPN放在了conv4 stage上，这样做也方便与之前resnet101+faster rcnn比较。最后作者通过实验发现使用孔洞卷积，mAP可以提高2.6个点。 3.4 可视化 Figure3和Figure4给出了当目标在ROI区域内和不在ROI区域内得分图的情况.可以看到，当目标在ROI区域内时各个位置得分都比较高，最后融合后最终的得分相应的也高；而当目标偏离ROI区域的时候，有些位置的得分就偏低，最后融合后最终的得分相应的也低。 四、实验4.1 PASCAL VOC目标检测任务训练相关设置： 目标函数：这里$c $ 为类别标签,$ c =0 $表示背景; $ t^* $为grouth truth box。分类损失函数为交叉熵；回归才是smooth-L1。 k * k = 7 x 7 正负样本构建准则:正样本为与ground-truth box的IOU大于0.5的侯选框，其它为负样本。训练的时候选择128个候选框进行训练。 OHEM方式训练：训练的时候采用在线困难样本挖掘进行训练。该方法有个特别好的优势就是在困难样本挖掘的时候，基本不需要什么计算量。 正则化：权重衰减系数为0.0005，动量为0.9 预处理：将图片短边缩放为600，采用单尺度训练。多尺度训练使用的尺度为{400,500,600,700,800} 学习率：在voc上，前20k次迭代学习率为0.001，后10k次迭代学习率为0.0001. batch-size：8个gpu每个gpu一张图片 采用faster rcnn提到的4-step交替训练，训练RPN和R-FCN。 推断相关设置： 在短边为600的图片上获取300个候选，并使用IOU为0.3的NMS进行过滤 4.1.1 多种全卷积目标检测策略的比较 作者比较了多种全卷积的目标检测策略，发现提到的方法要远远优于其他策略。 4.1.2 faster R-CNN+Resnet101和RFCN+Resnet101的比较 从结果可以看出，R-FCN与faster rcnn效果差不多，但是速度要快上很多。 4.2 MS COCO目标检测任务 数据集：80k训练集，40k验证集，20k测试集 学习率：前90k次迭代为0.001，后30k次迭代为0.0001 batchsize:8 交替训练方式：参考faster-rcnn 从结果可以看出，R-FCN与faster rcnn效果差不多，但是速度要快上很多。]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>RFCN</tag>
        <tag>two-setp detector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016_Deep Residual Learning for Image Recognition_CVPR2016_HeK et al]]></title>
    <url>%2Fpost%2F92de5aab.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）网络深度是深度神经网络性能提高的一个重要因素，但是在模型深度达到一定程度时，会出现梯度消失或者梯度爆炸的问题而很难训练。BN的出现极大程度解决了梯度消失或者爆炸带来的问题，使得训练更深层的网络更加容易。但是却带来了另一个问题，退化问题。随着网络深度的增加，模型在训练过程中准确率的增加会慢慢饱和，随后准确率反而开始下降。这个问题的出现表明并不是所有网络结构都很容易优化。为了解决深度神经网络网络层数太深而难以训练的问题，作者提出了残差学习框架。使用该框架可以解决由于网络层数太深而难以训练的问题。在此基础上作者提出resnet网络结构，该网络结构可以构建深度比vgg深很多倍，但是模型复杂度比vgg小的网络结构，并且效果要比vgg更好。使用提到的网络结构作者在ILSVRC分类任务以及COCO目标检测和分割任务中都取得了第一名。 二、使用什么方法来解决问题（创新点） 作者通过引入深度残差学习框架来解决退化问题。跟原本让堆叠的多层网络去学习一个潜在的映射不同，作者强制让他们去学习一个残差映射。假设原本学习的潜在映射是H，那现在堆叠的多层网络则是学习映射F，这里F=H-x。原本的映射则为H=F+x。作者认为这种方式要比原本的更加容易优化，因为在极端的情况下，当模型饱和的时候，输入端可以通过shortcut connection直接跳过非线性层。残差模块的构建也很简单，只需要通过shortcut connnection将输入端恒等映射到输出并加到原本堆叠的多层网络的输出上。这种做法的动机：为什么使用残差学习可以解决退化问题呢？作者给出了一个解释：模型通过多个非线性层来学习恒等映射是比较困难的。但是通过残差学习的形式，让多层非线性层去学习接近于0的输出从而让整体输出接近恒等映射则要容易很多。当网络达到一定复杂度的时候，很多时候会需要让多余的表达能力去拟合一个恒等映射，通过残差表示可以让模型更容易学习恒等映射，从而可以消除退化问题。最后作者通过实验证明了该方法十分有效。不仅很容易优化深层网络，而且也不会出现退化问题，反而能够很有效的利用深度的增加带来的增益。 三、实现细节1.网络结构 这里作者按照vgg网络的构建思路（a.相同的feature map大小，具有相同数量的filter b.当feature map大小减半的时候，对应的filter数量增加一倍。）构建了一个新网络，并在此基础上对其加入shortcuts连接，变成对应的resnet版本。构建的网络相较于vgg网络，具有更低的复杂度，约为VGG的18%。构建的resnet网络，当输入和输出维度相同的时候（如上图实线），使用恒等映射进行连接。当维度增加的时候（如上图虚线）作者考虑了两种操作。A.使用zero padding进行补齐 B.使用1X1的卷积增加维度，这两种操作stride都为2。 2.实现细节2.1 ImageNet分类任务训练时参数设置： 预处理：减去均值图片 数据增强：1）将图片的短边随机缩放到[246，480]之间进行尺度增强。2）从缩放后的图片中随机crop224 * 224大小的图片 3)对图片进行随机水平翻转。4）进行颜色增强。 在每个卷积和激活函数中间加入BN层。 正则化：权重衰减系数为0.0001；动量参数为0.9；没使用dropout。 优化方法：SGD;batch-size大小为256；起始学习率为0.1;学习率衰减方式为当误差无法下降的时候将学习率除以10;迭代次数为60* 10^4以上。 参数的初始化方式参考：K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 测试时参数设置： 采用多种尺度的全卷积形式，然后将得分取平均。尺度包括将短边resize为{224,256,384,480,640} 四、实验 4.1 Imagenet 分类任务（试验用的模型结构如上图）4.1.1 Plain Networks vs Residual Networks 从Figure4和Table2的结果可得出以下几个结论： 没加入残差结构的时候，普通的18层网络反而比普通的34层网络具有更大的训练误差和测试误差，出现了退化问题。当加入残差结构的时候，退化问题消失了，并且模型可以从增加的深度获取增益。 从Figure4中可以看到，34层的resnet相较于普通的网络训练误差低很多，可以看出resnet可以有效用于训练很深的网络。 虽然18层的resnet和普通版本虽然具有相似的准确率，但是相较于普通版本resnet版本拟合得更快。 4.1.2 Identity vs Projection Shortcuts 作者调查了3种形式的shortcut结构：A)zero-padding shortcuts用于增加维度，其它时候使用Identity shortcuts B）projection shortcuts用于增加维度，其它时候使用Identity shortcuts C）所有都是用projection shortcuts。发现projection shortcuts并不是处理退化问题的关键，shortcuts才是关键。 4.1.3 Bottlenet Architectures 为了在控制的训练时间内，探索残差模块对训练更深网络的效用，作者使用了bottlenet结构，将每个残差模块中的2层卷积替换为3层卷积。先使用1x1卷积进行降维，之后再使用1X1卷积扩张维度。这样使得复杂度与原本2层结构相似。构建的网络结构如table1中50层resnet,101层resnet和152层resnet。从table3和table4中可以看出更深的网络效果更好，而且不会出现退化问题。table4为提到的方法与最先进的几个方法的对比，从结果可以看出resnet效果具有很大的提升。 4.2 CIFAR-10 分类任务作者在CIFAR-10数据集上也得出了类似的结论另外作者也对响应值情况和是否能够训练更深的网络进行了探索: 作者也对卷积层的响应情况进行分析（Figure7），并得出以下结论：1）相较于普通网络，resnet网络的响应值更加小。2）层数更多的resnet，会产生更多小响应值。这两个结论与原本的动机想符合。 作者也尝试将网络层数增加到1000层，发现网络依然能够正常训练，说明残差结构对于训练深层网络是很有用的。 4.3 PASCAL 和 MSCOCO目标检测任务。 在目标检测任务中，作者简单的将vgg网络替换为res101就获得了很大的增益，说明该方法对于其他识别任务也同样有效。并以此网络结构作者赢得了ILSVRC和COCO2015各项比赛的第一，其中包括Imagenet 检测，Imagenet定位，COCO检测和COCO分割任务。]]></content>
      <categories>
        <category>深度学习</category>
        <category>模型结构优化</category>
      </categories>
      <tags>
        <tag>resnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python2.x和python3.x主要的差异]]></title>
    <url>%2Fpost%2Fb8f3def3.html</url>
    <content type="text"><![CDATA[参考：https://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/key_differences_between_python_2_and_3.ipynb#The-next-function-and-next-method 1.信息打印的区别，python3需要以函数的方式调用，而python2不需要%%script python2 a = (2, 3) print a (2, 3) %%script python3 a = (2, 3) print a File &quot;&lt;stdin&gt;&quot;, line 2 print a ^ SyntaxError: Missing parentheses in call to &#39;print&#39;. Did you mean print(a)? %%script python3 a = (2, 3) print(a) (2, 3) 2.整数除法的区别python2默认是整除，python3默认是非整除 %%script python2 a = 2 / 1 b = 2 // 1 c = 2 / 1.0 print a,b,c 2 2 2.0 %%script python3 a = 2 / 1 b = 2 // 1 c = 2 / 1.0 print(a,b,c) 2.0 2 2.0 3.字符串存储的区别，python3默认将字符串存储为unicode形式，而python2则是ASCLL方式，存储为unicode需要显式指定u’’ ，python3增加了两个类型，byte类型和bytearray类型%%script python2 s = &quot;I am Chinese.&quot; print type(s) s = u&quot;I am Chinese.&quot; print type(s) &lt;type &#39;str&#39;&gt; &lt;type &#39;unicode&#39;&gt; python3 字符串当前类型都为unicode 并增加了byte类型和bytearray类型 %%script python3 import sys print(&#39;Python %s.%s.%s&#39; %sys.version_info[:3]) a = &quot;I am Chinese.&quot; b = &quot;我是中国人&quot; c = a + b print(type(a),type(b),c) d = b&#39; bytes for storing data&#39; print(type(d)) e = bytearray(b&#39;bytearrays&#39;) print(type(e)) Python 3.7.0 &lt;class &#39;str&#39;&gt; &lt;class &#39;str&#39;&gt; I am Chinese.我是中国人 &lt;class &#39;bytes&#39;&gt; &lt;class &#39;bytearray&#39;&gt; 4.异常处理的区别，抛出异常和接收异常表达方式存在差异%%script python2 try: a = 1 / 0 except Exception, err: print err try: inputValue=&quot;test&quot; if type(inputValue)!=type(1): raise ValueError,&quot;input value is not a interger&quot; else: print inputValue except ValueError, err: print err integer division or modulo by zero input value is not a interger 区别主要在1) raise ValueError(“input value is not a interger”) 2) except ValueError as err: %%script python3 try: a = 1 / 0 except Exception as err: print(err) try: inputValue=&quot;test&quot; if type(inputValue)!=type(1): raise ValueError(&quot;input value is not a interger&quot;) else: print(inputValue) except ValueError as err: print(err) division by zero input value is not a interger 5.在列表推导式中，循环变量的作用域的区别%%script python2 i = 1 print &#39;before: i =&#39;, i print &#39;comprehension: &#39;, [i for i in range(5)] print &#39;after: i =&#39;, i before: i = 1 comprehension: [0, 1, 2, 3, 4] after: i = 4 %%script python3 i = 1 print(&#39;before: i =&#39;, i) print(&#39;comprehension: &#39;, [i for i in range(5)]) print(&#39;after: i =&#39;, i) before: i = 1 comprehension: [0, 1, 2, 3, 4] after: i = 1 6.在python3中取消了xrange函数，统一使用range在python3中range的表现跟python2中的xrange类似 %%script python2 a =[i for i in xrange(10)] print a print range(10) print xrange(10) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] xrange(10) %%script python3 a =[i for i in xrange(10)] print(a) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; NameError: name &#39;xrange&#39; is not defined %%script python3 a =[i for i in range(10)] print(a) print(range(10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] range(0, 10) 7.对于可迭代的对象，python3没有.next()方法，只有next();而python2两者都支持%%writefile text.txt a b c d e Writing text.txt %%script python2 f = open(&quot;text.txt&quot;,&quot;r&quot;) print f.next() print next(f) a b %%script python3 f = open(&quot;text.txt&quot;,&quot;r&quot;) print(next(f)) print(f.next()) a Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 3, in &lt;module&gt; AttributeError: &#39;_io.TextIOWrapper&#39; object has no attribute &#39;next&#39; 8.当比较不可比较的对象时，python3会抛出TypeError的异常%%script python2 print [1, 2] &gt; &#39;foo&#39; print (1, 2) &gt; &#39;foo&#39; print [1, 2] &gt; (1, 2) False True False %%script python3 try: print([1, 2] &gt; &#39;foo&#39;) except TypeError as err: print(err) try: print((1, 2) &gt; &#39;foo&#39;) except TypeError as err: print(err) try: print([1, 2] &gt; (1, 2)) except TypeError as err: print(err) &#39;&gt;&#39; not supported between instances of &#39;list&#39; and &#39;str&#39; &#39;&gt;&#39; not supported between instances of &#39;tuple&#39; and &#39;str&#39; &#39;&gt;&#39; not supported between instances of &#39;list&#39; and &#39;tuple&#39; 9.在python3中input()函数接收的对象都会以str类型存储，而python2则会根据具体输入形式以与该形式相符合的方式进行存储Python 2.7.6 [GCC 4.0.1 (Apple Inc. build 5493)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; my_input = input(&#39;enter a number: &#39;) enter a number: 123 &gt;&gt;&gt; type(my_input) &lt;type &#39;int&#39;&gt; &gt;&gt;&gt; my_input = raw_input(&#39;enter a number: &#39;) enter a number: 123 &gt;&gt;&gt; type(my_input) &lt;type &#39;str&#39;&gt; Python 3.7.0 (default, Sep 18 2018, 18:47:22) [Clang 9.1.0 (clang-902.0.39.2)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; my_input = input(&#39;enter a number: &#39;) enter a number: 123 &gt;&gt;&gt; type(my_input) &lt;class &#39;str&#39;&gt; &gt;&gt;&gt; my_input = raw_input(&#39;enter a number: &#39;) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; NameError: name &#39;raw_input&#39; is not defined 10.python3对于很多可迭代目标它会直接返回可迭代目标，而不是list，对于只需要用一次的数据这样可以节约内存，但是对于需要多次使用的数据这样做可能会影响效率。对于后者可以显式使用list函数。%%script python3 print(range(10)) print(list(range(10))) range(0, 10) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2014_SPPnet_Spatial pyramid pooling in Deep convolutional networks for visual recognition(ECCV2014)Kaiming He]]></title>
    <url>%2Fpost%2F60c293cd.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）先前针对分类的卷积神经网络需要固定大小的输入，这样需要对不同尺度的图片进行裁剪或者变形之后输入，这种做法会导致目标信息丢失或者带来不期望的图像扭曲，另外预定义的尺度对于不同的目标可能会不合适。这两个问题往往会降低准确率。如果能够解决这些问题，就有可能可以进一步提高分类准确率。 二、使用什么方法来解决问题（创新点）针对这个问题，作者分析了为什么需要固定大小输入的原因，那是因为全连接层需要固定大小输入，为此作者提出了一个解决方案叫空间金字塔pooling，作者称其为SPPnet，它被加在最后一层卷积层上，将最后一层的卷积特征池化成固定大小的输出，为此它可以对输入的任意尺度大小的图片生成固定长度的表示。 spp的好处（作者通过实验验证了每个优点都对准确率的提高有帮助）1）不管输入图片大小如何，都可以生成固定大小的特征。2）它将特征拆分成不同粒度的bin，类似于应用了多个不同尺度的slide window，相较于单个尺度的slide window，这种方式对目标的形变更加鲁棒3）应用于不同粒度的slide window可以根据输入图像的大小进行自适应的变化。 技巧及重要结论1）使用多层pooling提高了准确率。提升的主要原因是多层的金字塔层对于目标的形变以及在空间中的布局更加鲁棒。2）这种方式不仅可以用于多尺度测试，也可以用于多尺度的训练方式。多尺度的训练，在训练集上的拟合情况与单尺度类似，但是在测试集上有更高的准确率。具体的做法是一个epoch使用一种尺度来训练。3）使用全图输入，可以提高准确率。4）feature map上的多尺度多视角的测试，可以提高准确率。5) 使用多个模型进行boosting可以提高准确率6）作者通过实验验证了sppnet的优势与具体的网络结构是不相关的，即证明了该方法的通用性。7）该方法可以加速R-CNN的特征提取并且能够取得更好或者差不多的检出准确率，应用该方法无论图片中的目标有多少，都只需应用卷积神经网络对整张图片提取一次特征，然后再根据候选目标的位置利用spp从特征图上提取固定维度的特征向量，比起R-CNN，应用该方式可以提高24倍以上。8）预训练使用的类别数，样本数，以及目标尺度对于结果都有一定的影响。 一些重要的细节1）对于一般的分类网络，训练的时候都需要进行减均值的预处理方式。如果使用多种尺度进行训练，作者的做法是对于Imagenet直接将均值图片resize到目标尺度；对于VOC2007和Caltech101则是直接减去128. 三、方法介绍空间金字塔pooling是词袋模型（Bow）的一种扩展，它将图片划分成不同粗细粒度的多个部分，然后再对每个部分进行聚合。这种思想其实在传统的计算机视觉任务中一直发挥重要的作用，在深度学习还没有出现之前，它就已经被用在图像分类和检测中。SSP可以说是BOw的一种改进，因为SSP它能够保留空间信息。使用SSP层，网络的输入可以是任意的宽高比，任意的尺度。空间金字塔pooling会根据输出的大小，以及金字塔的层数，来决定每个金字塔层pooling的size和stride。比如4X4，2X2，1X1，他会根据pooling输出的大小以及输入的大小来推断每个pooling的size和stride。假设特征图大小为13x13,某一层金字塔层的输出大小为nxn，那么滑动窗口大小为$win=\lfloor a/n \rfloor$,stride大小为$str=\lceil a/n \rceil$.在利用该方式得到每一层金字塔层的特征之后，再将所有的特征拼接在一起。下图给出了一个3层金字塔pooling的例子：作者也尝试使用了多尺度的训练方式，为了减少不同尺度网络的切换，作者在训练一个epoch的时候，只使用一种尺度来训练。 四、实验结果及重要结论4.1 分类：4.1.1 ImageNet分类任务4.1.1.1 训练细节：1.训练数据为ImageNet2012，1000类。2.图像先被按比例resize成小边为256,然后再从图片的中心和四个角crop 224X224大小的图片，共5张。3.对图片进行水平翻转和颜色增强。4.再最后两层全链接层中加入dropout5.开始的学习率设置为0.01，之后当训练loss不降的情况下，再次减少10倍。6.作者使用了单个6g显存的GPU，训练了2到4周。7.使用了4层金字塔。6X6,3X3,2X2,1X18.使用的网络结构有4种，ZF-5，Convent*-5，Overfeat-5，Overfeat-7 4.1.1.1 实验结果及结论 使用多层pooling提高了准确率。提升的主要原因是多层的金字塔层对于目标的形变以及在空间中的布局更加鲁棒。作者使用了单尺度的训练方式，在训练和测试的时候都使用了224X224,每张图片先resize到256X256，然后从四个角落和中心crop224X224，最后再结合翻转，构建10张训练和测试图片。测试的时候对这十张图片进行求平均。 使用多个不同尺度的图片进行训练，提高了准确率。作者将图片resize到不同尺度，然后进行训练。训练的时候使用了两个尺度224和180。测试的时候仍然使用单个尺度224.然后仍然将图片resize到256后进行crop，每张图片构建新的10张图片。最后得到的结果比起单尺度的训练方式要更好。作者也尝试使用从[180,224]，之间抽取的一个随机尺度来训练。发现结果比起只使用两个尺度的要差，但是仍然比使用一个尺度的要好。可能的原因是测试的时候是224，而使用随机尺度的话，抽取的尺度很多都不是224，跟测试数据的分布有所偏差。 使用全图输入，可以提高准确率。作者将图片resize成短边为256，并保持宽高比不变。训练的时候仍然使用单尺度的训练。作者比较了中心crop和保留宽高比的效果，发现使用整张图片的效果更好。虽然使用整张图片比使用单个尺度多个视角的要差，但是当把他们结合在一起的时候仍然可以进一步提高多视角的准确率。并且提取的特征质量会更好。 feature map上的多尺度多视角的测试，可以提高准确率。作者将图片的短边resize到256，然后保留图片的宽高比，并从feature map上提取多个视角的特征用于测试，然后求平均。最后与在图片上的多个视角的测试进行对比，发现他们差不多。然后作者又使用多个尺度多个视角的测试，每个尺度提取18个视角，中心，四个角落，四个每个边的中心，整张图片，以及他们的水平翻转。作者通过多视角多尺度的方式，又将准确率进一步提高。 Krizhevsky的方法取得了ILSVRC2012比赛的冠军，而Overfeat，Howard,Zeiler&amp;Fergus则是ILSVRC2013比赛的最好的几个方法。 使用多个模型进行boosting可以提高准确率下图是ILSVRC2014比赛的排名情况，作者使用了7个模型进行融合使得结果进一步提升。并取得第三名的成绩。 4.1.2 VOC2007分类任务4.1.2.1 训练细节 训练数据包括9963张图片，共20个类别。其中5011张图片用于训练，剩下的图片用于测试。性能评估方式使用mAP(mean Average Precision) 作者使用在ImageNet进行训练的网络来提取图片的特征，然后将这些特征用于重新训练SVM分类模型，训练的时候没有做任何数据增强，只对特征进行L2标准化。 4.1.2.2 实验结果和结论 图中a列表明：越深层的特征越好 图中a，b列表明：使用fc层的特征获得更好的增益，主要由于多层的pooling 图中c列表明：全图输入的特征更好 图中d列使用尺度392更好的原因主要是：在VOC2007中目标比较小，而在Imagenet中目标比较大。这个结果表明：目标尺度会影响到分类的准确率，而SPP-net可以部分处理这个尺度不匹配的问题。 图中e列为将网络结构替换为Overfeat-7，并使用多尺度训练得到的结果 4.1.3 Caltech101 分类任务4.1.3.1 训练细节（没有说清楚怎么训练） 该数据集包括了9144张图片，共102个类别，其中1个是背景。 作者从每个类别中随机抽取30张图片用于训练，抽取50张图片以上用于测试。作者按照这种数据划分方式重复了10次，并将结果取平均，作为最后的结果。 4.1.3.2 实验结果和结论 从结果来看Caltech101与voc2007有类似的结果： 从a，b可以看出spp-net要比no-spp net效果要好 从c，b可以看出全图输入要比crop好 但是也有一些不同的结论： 在Caltech101分类任务中全连接层的准确率要比spp层低。 在选择的多个测试尺度中224的尺度是最好的，这个主要是因为在Caltech101中，目标占据图像的区域与ImageNet差不多。 作者也尝试将图片reshape 成224X224，这样虽然保留了全图信息，但是会带来扭曲。从结果来看效果没有使用全图的好。使用reshape的准确率是89.91%，而使用全图输入没有扭曲的准确率达到91.44%。 下图给出了不同方法在Voc2007分类数据集和Caltech101分类数据集上的结果。其中VQ，LCC，FK都是基于金字塔匹配的，剩下都是基于深度网络的。在基于深度网络的方法中，Oquab等人的方法和Chatfield等人的方法采用了finetune和多视角的测试。本文提到的方法只采用了单张全图，并且没有使用finetune，但是取得了与他们类似的结果。而在Caltech101测试集上，作者提出的方法远远超出当前最好的方法。 4.2 检测：4.2.1 RCNN方法简介 R-CNN首先利用selective serach从每张图片中提取约2000个候选窗。 然后将每个候选窗reshape成227X227，并利用一个预先训练好的卷积网络去提取特征。 利用提取到的特征去训练一个2分类的SVM分类器。虽然RCNN相对于先前的方法，效果很好，但是速度比较慢，每张图片需要对2000个候选窗口应用卷积网络去提取特征。特征提取是测试速度的主要瓶颈。 4.2.2 将SPP应用于RCNN中及训练细节 作者将空间金字塔pooling应用到R-CNN中，可以提高训练和测试的速度，并且提高准确率。具体的：1）使用“fast”模式的selective search从每张图片中提取2000个候选框2）将图片resize到min(w,h)=s3）利用单尺度训练的ZF-5网络直接提取整张图片的特征。4）利用4层空间金字塔pooling提取每个候选框的特征，生成12800维的特征向量。5）最后利用全链接层的特征提供给svm分类器，然后利用每个类别的svm分类器来得到每个类别的得分。训练svm的时候，作者使用了ground-truth作为正样本，使用与ground truth的IOU小于0.3的候选框作为负样本。并且负样本之间的IOU要小于0.7.最后使用hard negative mining的方式来训练svm，困难样本挖掘只迭代一次。6）在测试阶段，在对每个候选框进行打分之后，作者使用了非最大化抑制的方式阈值为0.3，去除掉一些冗余的框。 使用该方式，利用多尺度进行测试，可以进一步提高效果。作者发现，最好的一种方式是，对于不同尺度的候选窗口，只对该尺度下最接近224X224的候选窗口进行特征提取。 这样的话不同尺度的输入只需跑一次卷积神经网络对全图进行特征提取，然后让各个候选窗口按在该尺度下最接近224X224的原则分配到各个尺度下，然后应用spp对候选窗口进行特征提取。 作者也尝试使用fine-tune和bounding box回归。1.fine-tune的时候作者只fine-tune全链接层，固定其他层。2.fc8使用了方差为0.01的高斯分布来进行初始化。3.正样本为IOU大于0.5，负样本为与正样本的IOU小于0.5大于等于0.1，每个batch包括1/4的正样本，3/4的负样本。4.开始250k次迭代使用学习率为0.0001，之后50k次迭代的学习率设置为0.000015.使用bounding box 回归作为后处理方式。使用的特征是来自与conv5的空间金字塔pooling。使用的样本为IOU大于0.5的候选框。 4.2.3 实验结果和结论4.2.3.1 VOC 2007测试集1. 实验结果 R-CNN使用Alex-5 R-CNN使用ZF-5 不同的目标检测方法在20类目标的map 作者使用另外一种候选生成方式EdgeBoxes进行测试，mAP为52.8.这是因为训练的时候使用的是Seletive search。当训练的时候同时使用Selective search和EdgeBoxes，测试时使用EdgeBoxes时，mAP达到了56.3.这是因为训练样本增加了。 2. 结论从结果来看，SPP跟RCNN差不多，但是速度方面SPP要比RCNN快很多。 通过模型组合可以提高效果作者使用相同的网络结构，不同的初始化方式在ImageNet上训练了另一个网络，然后重复上面的检测算法，另一个网络在整体的mAP上与原先的差不多，但是有11个类别要比原来好。两个模型存在较大的差异，可以互补，所以融合后效果提升了不少。组合方式为：首先使用两个模型分别为测试图片上的所有候选进行打分；然后将结果合并在一起做非最大化抑制。作者进一步发现模型组合的提升主要来自与卷积层。作者使用同个模型不同初始化方式对整个卷积网络进行finetune得到了提升，但是使用同个模型不同的初始化方式来fine-tune全连接没有提升。说明提升主要来自于卷积层。 4.2.3.2 ILSVRC 2014测试集1. 训练细节 ILSVRC 2014检测数据集，包括200个类别。不允许使用imagenet 1000类数据集。 450k训练数据；20k验证数据；40k测试数据 由于数据量以及类别数都比image net要少，这样的话性能没有使用imagenet来预训练好。为此作者使用了提供的499类的数据集。 由于目标尺度的分布在499类的CLS上是0.8，而在DET是0.5，为此作者把DET数据集resize成min(w,h)=400,而不是256.然后随机crop 224X224用于训练，只有当它与ground truth的重叠大于0.5时，才用来训练。 2. 重要结论： 类别数，样本数，以及目标尺度对于结果都有一定的影响。作者对比了利用ILSVRC2014数据集的不同标签（大类标签有200，子类标签有499）来进行预训练的情况下，在Pascal VOC数据集上的检出效果。作者使用了pool5特征进行训练，在使用imagenet进行预训练的情况下，mAP是43.0%，在使用ILSVRC2014 200类进行与训练的情况下，mAP降到了32.7%，而使用ILSVRC2014 499类进行预训练的情况下，mAP提高到了35.9%。从结果来看，虽然使用200类和499类训练数据量并没有变化，但是效果却提升了，说明更多的类别有助于提高特征的质量。另外作者尝试了使用min(W,H)=400来训练，替代使用256来训练，mAP进一步提高到了37.8%。但是仍然没有使用imagenet进行预训练的好，说明数据量对于深度学习的重要性。 通过模型的组合可以进一步提升效果。]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>SPPnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2014_Rich feature hierarchies for accurate object detection and semantic segmentation(CVPR2014)Ross Girshick]]></title>
    <url>%2Fpost%2F2f9d64a9.html</url>
    <content type="text"><![CDATA[一、背景及意义（动机）这篇论文中作者提出了一个很牛逼的目标检测算法R-CNN，直接将PASCAL VOC数据集上的最好性能提高了30%。将深度学习应用到目标检测，主要会面临两个挑战，一个是如何使用深度学习去定位目标位置；另一个是在数据缺乏的情况下如何训练高质量的模型。在这个方法中作者结合了两个关键的思想来解决这两个问题：1）为了去定位和分割目标，可以应用一个能力很强的卷积神经网络去筛选候选目标 2）当标注的训练数据缺乏的情况下，运用辅助任务进行监督预训练和指定任务的fine-tune，可以让性能有一定的提升。 二、使用什么方法来解决问题（创新点） 创新点1）提出一个能有效进行目标检出的框架RCNN，在PASCAL VOC 2012数据集上，将mAP提高30%2）当标注的训练数据缺乏的情况下，运用辅助任务进行监督预训练和指定任务的fine-tune，可以让性能有一定的提升。将mAP提高了8个点。 存在的问题1）需要对候选区域的特征进行存储，需要较多存储空间2）对每个候选区域都需要运行一次卷积神经网络来提取特征，速度慢 重要结论1）对于深层神经网络来说，越高层的特征越是任务相关的，而低层的特征则更加的通用。 三、方法介绍 3.1 R-CNN主要包括了3个模块： 首先通过一个类别无关的proposal方法得到候选框。 这里proposal方法作者主要使用了selective search，从每张图片中提取2000个候选框。 然后将候选框reshape到指定输入大小并用一个卷积神经网络提取固定长度的特征。最后利用SVM来对每个目标进行打分，每个类别训练一个svm。 特征的提取主要使用了一个卷积神经网络，从每个区域提取4096维的特征。输入图片大小为227X227，预处理方式为减去均值图片，网络结构为5层卷积2层全连接层。 最后对每个类别运用非最大化抑制将冗余的检测框剔除掉。非最大化抑制主要是先找到得到最高的目标，如果它周围的目标的得分超过类别阈值，但是它与最好得分目标的IOU超过了某个阈值，那么将该目标剔除掉。 3.2 监督预训练和指定领域的finetune 监督预训练：作者使用了ILSVRC2012作为辅助的数据集，并利用该数据集图像层面的标注训练一个分类模型。 在检出训练集上进行finetune：将最后一层分类层替换为N+1类，其中1类为背景。将候选框与目标的IOU大于等于0.5作为正样本，其他作为负样本。利用先前的预训练模型来fine-tune，并把学习率设为原来的十分之一为0.001.在每次迭代中，均匀采阳32个正样本和96个负样本，batchsize大小为128。 3.3 目标类别分类器 fine-tune完之后，作者没有使用finetune的网络的结果作为最终的结果。而是利用fine-tune完成的网络作为特征提取器，并运用histogram intersection kernel svm分类器，来得到最终每个目标的类别。这里作者训练svm的时候将训练集和验证集一起拿来训练。 这里负样本如何确定很关键，当使用IOU小于0.5为负样本时，mAP下降5个点，使用IOU为0时为负样本，mAP下降4个点，最终作者通过网格搜索确定了IOU小于0.3为负样本最优。正样本则为ground-truth bounding boxes。 最后作者采用hard negative mining方法来加速拟合。通过利用困难负样本挖掘，只需要跑一个epoch，svm就拟合完成了。 四、实验结果及重要结论4.1 PASCAL VOC 2010-12测试集对于VOC2010-12，作者使用VOC2012训练集进行finetune，使用VOC2012训练集和验证集训练SVM模型。 4.2 ILSVRC2013检测数据集 在ILSVRC2013检测数据集上，OverFeat，NEC-MU,UvAEuvision，Toronto A 和UIUC-IFP都使用了卷积神经网络，但是效果却比作者的方法差很多，说明不同的用法对结果影响很大。 4.3 可视化学习到的特征 作者选取了pool5层中6个激活单元，作为proposal的得分，执行非最大化抑制，最后对所有的结果精细排序，选择了激活值最大的前16个图片进行可视化。说明学习到的特征具有较好的判别性。 4.4 Ablation studies 作者对fine-tune与不fine-tune不同网络层的特征进行了比对，发现pool5的特征相对于fc层比较通用，而fc层的特征则更多是任务相关的。 4.4.1 在没有fine-tuning的情况下，比较不同层特征带来的效果。 为了理解哪一层特征对于检出性能比较关键，作者分别用CNN（只使用ILSVRC2012进行预训练）的最后3层来测试VOC2007数据集上检出任务的效果。结果表明：pool5只使用了卷积特征就可以达到较好的效果，说明主要带来效果的是卷积层而不是全连接层；另外fc7要比fc6效果差，说明越高层的特征越是任务相关的。 4.4.2 在fine-tuning的情况下，比较不同层特征带来的效果。 利用VOC2007trainval集对模型进行finetune，并利用finetune好的模型的最后3层来测试VOC2007数据集上检出任务的效果。利用finetune后的特征来训练，mAP的提升十分明显，足足提升了8个点。finetune后fc6和fc7比起pool5层提升的效果更加明显，进一步说明越高层的特征越是任务相关的，而低层特征则是更加的通用。 4.4.3 不同网络结构对结果的影响 T-net比O-net效果更好，但是也更加耗时 4.4.4 使用Bounding-box 回归可以提高定位的准确率]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的基本概念]]></title>
    <url>%2Fpost%2F0.html</url>
    <content type="text"><![CDATA[一、深度学习简介 深度学习是机器学习的一个子领域，它使用多层非线性信息处理及抽象，用于监督和非监督的特征学习和表示、分类及模式识别。深度学习在不同的学习任务上都取得了很大的成功,特别是在Imagenet的图像分类任务和LFW的人脸识别任务上更是超过了人类水平。相比于传统方法，深度学习具有很多优势。首先深度学习通用性更强，而传统方法针对不同的任务需要设计不同的特征。深度学习相较于传统方法可以自动从原始数据中提取特征，所以同个算法可以应用到各种类似的任务中来，比如目标检测任务，它可以同时用于人脸检测，行人检测，一般物体检测任务上。其次深度学习学习的特征具有很强的迁移能力，比如它在ImagetNet分类任务上学习的特征，在目标检测任务上也可以获得非常好的效果。最后一个优势是它的工程开发、优化、维护成本较低。深度学习计算主要是卷积和矩阵乘，针对这种计算优化，所有的深度学习算法都可以提升性能；另外，通过组合现有的层，我们可以实现大量复杂网络结构和算法，开发维护成本也很低。随着深度学习的发展，当前也存在着很多种不同类型的网络体系结构，如深度自编码网络、卷积神经网络、时间递归神经网络等。其中应用最广泛的是卷积神经网络。 二、卷积神经网络的发展过程 卷积神经网络是一种深度学习网络结构，它的发明灵感来自于生物的自然视觉感知机制。在1990年，LeCun等人$^{[1]}$发表了开创性的论文建立了CNN的现代框架，并随后提出了对它的改进$^{[2]}$。他们开发了一个多层的人工神经网络叫做LeNet-5，用于识别手写体数字图片。与其它神经网络类似，LeNet-5具有多个网络层，并且可以通过反向传播算法$^{[3]}$来训练。它可以从输入图片中学习到有效的表示，这也让它可以直接通过原图来进行识别，而不需要先对图片提取特征。但是由于当时缺少大规模的训练数据以及受到计算能力的限制，他们的网络在更复杂的问题上(如大规模的图片和视频分类任务)无法表现得很好。 自2006年以来，很多的方法已经被开发出来用于改进卷积神经网络$^{[5-8]}$。其中最著名的是Krizhevsky等人提出的经典的CNN体系结构，AlexNet$^{[6]}$，该体系结构在图像分类任务上的性能大大超过了先前的方法，在图像分类任务上的top-1和top-5错误率分别从47.1%和28.2%降低到37.5%和17.0%。AlexNet在网络结构上与LeNet-5类似，但是网络结构比LeNet-5更加深。随着AlexNet的成功，很多的工作被提出来改进它的性能。其中，最有代表性的工作是ZFNet$^{[9]}$，VGGNet$^{[7]}$, GoogleNet$^{[8]}$,和ResNet$^{[10]}$。随着更多改进方法的提出，当前卷积神经网络在很多任务上都获得很高的性能，并且在某些任务中超过了人类水平。 三、卷积神经网络的基本部件及训练方法简介 我们将输入数据输入到网络中，并经过卷积层、池化层、全连接层，最后到达输出层输出结果的过程称为前向传播。前向传播得到的估计值通常与实际结果会有误差，模型的训练过程就是不断缩小估计值与实际值之间的误差的过程。在这个过程中我们利用损失函数来衡量估计值和实际结果的误差带来的损失，并试图让该损失尽可能小，为此我们将误差带来的损失从输出层向隐含层传播，直至传播到输入层，这个过程称为反向传播。在传播过程中通过某种误差调整策略，如随机梯度下降，调整模型参数，使得模型误差减小。整个模型的训练过程就是不断迭代前向传播，反向传播并调整模型参数这个过程，直至最后模型收敛。在模型收敛之后，模型的每次预测过程就是一次前向传播过程。虽然当前存在很多不同的卷积神经网络体系结构，但是它们的基本部件是很相似的。如LeNet-5，它主要包含3种部件，卷积层、池化层和全连接层。下面给出了卷积神经网络的各个部件及其训练方法的简单介绍。 图一.LeNet-5网络结构 1.卷积层 卷积层的目的是学习输入的特征表示，如图一所示，卷积层主要由多个卷积核构成，每个卷积核被用于计算不同的特征图。图二给出了卷积计算的一个例子，其中图二(a)为卷积核，图二(b)中蓝色矩阵为输入的特征图，绿色矩阵为利用图二(a)中的卷积核对输入特征图进行卷积计算的结果。输出的矩阵中的每个神经元与输入特征图的局部区域相连，这个局部区域就是当前神经元在上一层的感受野。输出矩阵的计算方式是利用卷积核对每个局部区域进行卷积，即逐元素相乘再相加。在得到卷积结果之后，新的特征图可以通过对每个从局部区域卷积计算得到的神经元应用非线性激活函数得到。这里值得注意的是，同个特征图的生成过程中，输入的多个局部区域共享同个卷积核。最终多个特征图的生成，则是由多个不同的卷积核计算得到。我们把在第l层的第k个特征图中的第(i,j)个位置(这里(i,j)表示第i行第j列)的特征值表示为$z^l_{i,j,k}$，则： 其中w表示第l层第k个滤波器的权重，b表示该滤波器对应的偏置项，x表示当前响应值对应的输入块，其中心位置对应特征图中的第(i,j)个位置。这里w为当前特征图中每个特征值所共享。这种权重共享方式具有多种优势，它可以降低模型复杂度，并且可以让网络更加容易训练。在卷积神经网络中引入非线性激活函数，可以让多层网络学习到非线性特征。用 $z^l_{i,j,k}$ 表示非线性激活函数，则卷积特征值的激活值$a^l_{i,j,k}$可以表示为： 经典的激活函数是sigmoid、tanh、ReLU和Leaky ReLU，图三中给出了这四种常见的激活函数。图四中显示了两个卷积层从数字图片7中学习到的特征图，其中左边为第一层卷积层的特征图，大小为6X28X28，右边为第三层卷积层的特征图，大小为16X10X10。从特征图可以看出底层卷积核主要用于学习低层次的信息，如边缘和曲线，而更高层的卷积核则用于学习更加抽象的信息。通过叠加多个卷积层和池化层，我们可以逐渐提取到更加高层次的特征表示。 图二 (a).卷积核示例 图二(b).卷积计算示例 图三.常用的激活函数 图四.从数字图片7中学到的特征 2.池化层 池化层通过减少特征图分辨率的方式让网络具有平移和旋转不变性。它经常放在两个卷积层之间。典型的池化操作是平均池化和最大池化。图五给出了池化操作的例子，其中图五(a)为平均池化，图五(b)为最大池化。池化层的每个特征图与它前一层的卷积层对应的特征图相连接。用$pool(.)$表示池化函数，对于卷积层的每个特征图$a^l_{:,:,k}$，对应的池化层的特征值可以表示为： 其中$R_{i,j}$是卷积层特征图中第(i,j)个位置周围的一个局部区域。平均池化取池化区域的平均值，而最大池化则是取池化区域的最大值。 图五(a) 平均池化示例 图五(b) 最大池化示例 3.全连接层 在经过多个卷积和池化层之后，可能会连接一个或者多个全连接层来进行高层次的推理。图六给出了全连接层的例子，它们将前一层的所有的神经元作为输入，并将它们与当前层的每个神经元相连接，通过这种方式来生成全局的语义信息。这里的全连接层并不是必须的，它有时候也可以用1X1的卷积层来替代。 图六.全链接层示例 4.输出层及损失函数 卷积神经网络的最后一层是输出层。对于分类任务，最常用的是softmax操作。另一个比较常用的方法是svm，它可以结合卷积层的特征来处理多种不同的分类任务。用K表示类别数，则其对应的softmax函数可以表示为：其中$\sigma(z)_j）$表示第j个类别的概率。对于回归任务，输出层一般为普通神经元。用$\theta$表示所有可学习的参数(比如卷积核的权重和偏置项)，对于具体任务的最优参数，可以通过优化其对应的损失函数来得到。假设我们有N个有标签的训练样本数据，将其表示为{(x(n),y(n))};nЄ{[1,…,N]}，这里x(n)表示第n个输入数据，y(n)是其对应的标签，用o(n)表示卷积神经网络的输出，则其损失函数可以表示为： 对于分类任务，损失函数一般选择交叉熵损失函数；对于回归任务，则一般为均方误差。 图七.带有一层隐含层的前馈神经网络 ５.反向传播算法 训练卷积神经网络是一个全局优化的问题，通过最小化损失函数，我们可以找到最合适的参数集。通常我们采用随机梯度下降方法来优化CNN网络。CNN网络中梯度的计算主要采用了反向传播算法，其中主要利用了链式法则来对不同网络层的权重进行求导。下面给出反向传播的一个简单例子，假设我们的网络结构如图七所示并假设损失函数为均方误差，则损失函数可以表示为：其中x是输入数据，y是其对应的标签；σ为激活函数，这里选取sigmoid函数为激活函数；W1，b1、W2，b2分别为第一层和第二层的权重和偏置项。我们将中间变量定义为：则其前向传播过程如下：对应的中间变量的梯度即反向传播如下:通过链式法则可以很容易计算到各个参数的梯度，如W2和b2的梯度： [1] B. B. Le Cun, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, Handwritten digit recognition with a back-propagation network, in: Proceedings of the Advances in Neural Information Processing Systems (NIPS), 1989, pp. 396–404.[2] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of IEEE 86 (11) (1998) 2278–2324.[3] R. Hecht-Nielsen, Theory of the backpropagation neural network, Neural Networks 1 (Supplement-1) (1988) 445–448.[4] W. Zhang, K. Itoh, J. Tanida, Y. Ichioka, Parallel distributed processing model with local space-invariant interconnections and its optical architecture, Applied optics 29 (32) (1990) 4790–4797.[5] X.-X. Niu, C. Y. Suen, A novel hybrid cnn–svm classifier for recognizing handwritten digits, Pattern Recognition 45 (4) (2012) 1318–1325.[6] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition challenge, International Journal of Conflict and Violence (IJCV) 115 (3) (2015) 211–252.[7] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.[8] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1–9.[9] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: Proceedings of the European Conference on Computer Vision (ECCV), 2014, pp. 818–833.[10] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolution neural network</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018_[CGAN]CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation_MICCAI2018_Dakai Jin]]></title>
    <url>%2Fpost%2Fd5dc028e.html</url>
    <content type="text"><![CDATA[该论文发表于MICCAI2018 一、动机 在医疗图像领域，数据难获取，而且类内存在着很大的差异。为了缓解数据问题，作者在这篇文章中调查了通过仿真出来的数据，是否能够改善P-HNN模型在肺结节分割任务上的效果。 二、主要创新点 为了生成仿真数据，作者使用了GAN模型，有效地学习肺结节在3D空间中的属性分布。 为了让仿真的结节能够更好的融入到背景中去，作者使用了真结节的背景信息，即把真结节从图片中抠走。 最后为了能够进一步让结节看起来更加的真实作者提出了一个新颖的multi-mask重构loss. 三、具体实现3.1 3D CGAN 网络结构 生成网络的输入是消除掉中心结节但是包含结节背景信息的图像。通过编码和解码过程，生成网络试图恢复消除掉的中心结节。 判别器的输入包括了扣掉中心结节的图片以及原始图片。 在生成网络的解码器的前两个卷积层作者加入了dropout. 作者在网络中利用strided卷积代替了pooling层。 包括生成网络的编码器，以及判别网络。并在strided 卷积后使用了Leaky relu激活函数。 在生成器的最后一层作者使用了Tanh激活函数 3.2 模型loss CGAN loss multi-mask L1 loss 这里的M表示消除掉的结节位置，N是对M进行膨胀后的结果。这里的multi-mask是指对消除掉的结节位置的重构loss以及对其膨胀后膨胀位置像素的重构loss，分别赋予不同的loss 权重。比较合适的是3到6个。这里作者给前者赋予1.0，后者赋予5.0. 这个模型的loss由这两部分加权构成 loss包括了两部分，一部分是普通的CGAN loss；另一部分是multi-mask loss。这两部分的权重分别为1.0和100.0. 3.3 相关参数 作者使用了标准的GAN训练方法，交替优化G和D。 在训练G的时候，作者通过去最大化logD(x,G(x)),而不是去最小化log(1-D(x,G(x)))。 作者使用了Adam优化方法，并设置初始学习率为0.0001，对于生成器动量参数设置为0.5，对于判别器动量参数设置为0.999。 3.4 数据和结果3.4.1 作者使用了LIDC数据集对GAN模型进行训练1.真实结节从ct图像中crop出来，crop的3个纬度的是结节直径的3到3.5倍大。最后将crop出来的图片缩放到64X64X64。2.消除掉结节的输入图片，由真实结节图片消除掉中心直径为32的球体生成。3.对于结节直接小于5mm的，不用于训练GAN.(c) 用全图的L1loss,不用判别网络(d) 用了GAN和全图的L1 loss(e) 用了GAN和中心消除位置的L1 loss(f) 用了GAN和multi-mask L1 loss 3.4.2 利用训练好的GAN模型生成一些仿真结节，用于fine-tune P-HNN模型P-HNN模型在之前的实验中对于在肺壁的结节分割效果较差，主要原因是训练数据中缺少肺壁附近的结节。 作者首先从LIDC数据集中挑选了34张图，了解数据中缺少的外围结节大致是如何的。 然后从相对健康的CT影像中挑选42个。并从其中随机挑选了30个类似的位置。每个位置距离肺壁距离在8到20mm之间。crop图片的大小为32到80mm之间。 接着将crop的图片缩放到64X64X64。生成仿真数据之后，再将其缩放回去，然后黏贴回原来的位置。 最后利用他们来fine-tune P-HNN模型。 最后的结果显示，生成的结节确实可以提高分割效果。]]></content>
      <categories>
        <category>深度学习</category>
        <category>医学图像处理</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>lung nodule</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object detection]]></title>
    <url>%2Fpost%2F4ec81054.html</url>
    <content type="text"><![CDATA[object detection（rcnn,the way to endtoend）本文主要简单介绍了rcnn如何一步步改进到支持端到端训练的faster-rcnn。 基础概念 mAP（mean average precision） precision：检测出相关的内容占检测出的内容的比例。 average precision：举个例子，比如说当前文档相关的主题有5个，检测出了3个，其中3个相关的主题rank分别为2,3,4，则这个文档的average precision就是(1/2+2/3+3/4+0+0)/5 。 mAP:则是每个文档的average precision的平均值。这里的文档对应于目标检测的每张图片，而相关主题则对应到图片中目标的个数 IoU（交并比） 两个区域的交集比上两个区域的并集 non maxinum supression 与得分最高的区域的IOU高于某个阈值时，将其排除掉 基于region proposeal加classification这一框架的几个重要工作1.rcnn 几个重要的创新 selection search提取候选区域 利用cnn 提取重要的特征（需要将特征保存到硬盘中，用于svm的训练） 利用svm对不同的候选框进行分类 利用回归来获取更加准确的位置 缺点： 训练需要分成多个阶段完成 训练和存储代价大 检测速度缓慢 2.spp-net 几个重要的贡献 rcnn速度缓慢的原因是它需要分别对每个proposal提取特征，而没有共享计算 输入整张图片，然后利用spp层提取不同尺度大小的featuremap，最后将它们级联起来，最后输出一个特定维度的特征向量。 spp-net使得rcnn在检测的时候提升了10到100倍。训练速度提升了3倍。 缺点 训练仍然需要多个阶段。仍然需要提取特征训练svm； 最后仍然需要利用回归模型来获取更加精确的定位。 需要将特征保存到硬盘中。 网络训练的时候梯度没法穿过spp层。 3.fast rcnn 解决需要将同张图的每个候选区域分别输入网络提取特征的问题直接输入原图，并利用cnn以及roipooling来提取每个roi区域的特征，并利用cnn对每个roi区域进行分类以及定位 roipooling层：单尺度的ssp层，输出特定大小的特征向量，类似于根据情况调整pooling层的kenal size。 多任务的loss：softmax层以及bounding box 回归 几个重要的贡献 提升了训练以及检测的速度。 训练只需要单个阶段就可以完成。使用了多任务的loss层。 可以调整网络的所有层。 不需要将特征保存到硬盘中 缺点： 仍然需要利用selection search 4.faster rcnn 提出了利用PRN来提取候选区域，并让PRN与fast rcnn共享网络。使得目标检测成为真正的一种endtoend的方式，并将速度进一步提升到可以进行实时检测的目的。]]></content>
      <categories>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>rcnn</tag>
        <tag>spp-net</tag>
        <tag>fast rcnn</tag>
        <tag>faster rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime基础用法汇总]]></title>
    <url>%2Fpost%2Ff7b78535.html</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sublime由于支持多种插件的功能扩展，以及其优秀的编辑能力，得到了越来越多人特别是linux平台下的程序开发人员的青睐。但是想要发挥他强大的功能优势，还是需要学习以及熟悉这些功能，在这里整理了网上关于sublime插件基础用法的相关资料，供需要的人查看。sublime简明教程简书上有人整理的sublime学习资源]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用git来管理各种文档]]></title>
    <url>%2Fpost%2F6de20b39.html</url>
    <content type="text"></content>
      <categories>
        <category>工作环境</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[terminator+tmux打造超级终端]]></title>
    <url>%2Fpost%2F12fd4503.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[利用sublime文本编辑工具和markdown标记语言写印象笔记]]></title>
    <url>%2Fpost%2F78cc5777.html</url>
    <content type="text"></content>
      <categories>
        <category>工作环境</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建属于你自己的工作环境]]></title>
    <url>%2Fpost%2Fd9ff40b9.html</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文主要讲述作为一名知识工作者如何对知识进行管理以及讲述在对知识进行管理的每个环节中推荐使用的工具。下面用思维导图给出了本文的整体框架：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说到管理，必然是需要对整个知识获取以及使用保存等环节进行控制。这必然少不了对知识产生，获取，运用等过程进行分解。那么知识的运用流程可以划分成多少个阶段呢。这里，我根据你的知识需要管理这本书，将知识管理划分成以下几个阶段：知识收集、知识加工、知识保存、分享、运用、创新。 知识收集：主要是收集相关的信息，为了采集相关的信息我们需要知道可以从何处获取到相关信息，如何获取。 知识加工：这个阶段主要对知识进行理解，并且做相应的链接。在这个阶段中，我们可能需要知道如何高效的学习，有时候信息可能是一本书籍，或者是好几本书籍，那么我们可能需要学会如何快速的从一本书或者是好几本书中获取我们想要的信息。 知识保存：在对知识进行加工获取到相应的信息之后，我们需要对我们获取到的信息进行总结。 分享：我们为什么要对知识进行分享呢？分享知识有几点好处：1.能够找到志同道合的伙伴。2.可以通过对知识的分享来找到自己的一些盲区，遗漏或者是误解的地方。 运用：知识最终的目的往往是用，学以致用，这才是才是我们学习的最终目标。 创新：创新往往不是凭空产生的，它往往是基于前人的工作，这里知识的积累是必不可少的。 知识收集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;知识收集主要包括了收集什么，从何处收集，如何收集的问题。这里面收集什么是根据具体情况而定的，这里主要讲下从何处收集，以及如何收集的问题。绝大多数的人都知道我们可以使用搜索引擎从网上获取到我们想要的信息。当时现在搜索引擎有很多，我们在碰到具体的问题的时候该如何选择才能够最好的搜索到我们想要的信息呢。这个问题可能很多人都没有想过。大部分人使用百度这个搜索引擎就以及基本可以解决所有的问题了。但是通过百度这个搜索引擎搜索到的信息很多时候不是最好匹配我们的问题的，特别是对于知识性的问题来说。一般来说百度对于娱乐八卦这方面的信息搜索能力是比较强的，而知识性的信息则比不上谷歌。所有当我们碰到知识性的信息的时候，最好使用谷歌来进行查找。而需要其他一些娱乐八卦相关信息的时候使用百度来进行搜索，当然很多时候在使用一个搜索引擎搜索不到的时候，往往我们都会尝试使用其它搜索引擎来进行搜索，只是当我们知道哪些问题通过哪个搜索引擎能够更快搜索到的时候，我们的效率就会更高。现在谷歌在中国以及没法直接访问了，只能通过购买vpn或者是借助翻墙工具来进行访问。在这里我推荐一款不错的翻墙工具，xxnet。它是一个开源项目，可以从github上获取，具体的配置信息在github上面已经给出了。另一个获取信息的途径是通过知识共享平台来获取，现在有几个很优秀的知识共享平台，比如知乎和简书，在上面分享的信息都很有见地。除了利用互联网，其它获取信息的渠道还有很多，比如一些传统的信息获取方式，通过沟通交流等，其它方面的信息获取方式就不多说了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一个问题就是知识收集工具，我们的信息来源是多种多样的，那么我们就相应的需要一个能够收集多个渠道的信息的工具。这里最能满足这个需求的，当属印象笔记。它提供了一个很棒的浏览器插件，印象笔记（剪藏），支持将网页信息获取到印象笔记中，还提供了从邮件，微信，微博中获取信息到印象笔记中的方式。而且它支持多个平台的信息同步，我们可以在电脑、平板、手机上使用印象笔记来收集信息。而且还支持多种类型的信息源，比如图片，网页，声音等。基本上它能将我们产生的各种信息都收集到其中，并且多平台同步。所以作为一个知识收集工具，印象笔记是首选。 知识加工&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在获取到信息之后，我们需要对信息进行整理，从中提炼出有价值的信息，可以实际的给我们所用的。在这个过程中，我们需要将收集到的信息进行分类，并且对信息进行理解、学习，最后可能还需要对相关的信息进行链接，归纳总结等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里面对信息的分类是一个很关键的步骤，如果没有对信息进行分类，那么我们就相当于少了一个标签，想找到我们想要的信息只能从中一件一件的翻，这样的话工作量是很大的。但是如果你将他们进行分类，这样就类似于建立了一个个抽屉，你只需要打开装载你想要的信息的抽屉，就可以找到你想要的信息，这样就可以很快的找到你想要的信息。另一个好处是通过分类这些信息可以真正的用起来，当你只是将收集到的信息堆积起来的时候，信息越堆越多，这样就又回到你最初找到他们的时候的状态，在一大推信息中翻找，看到这种情况，我们往往会放置不管，然后最后把他们丢弃，即使里面确实是有你想要的信息。这一步可以使用印象笔记为每类信息建立笔记本，或者是一个汇总笔记来实现，当然还要记得为每个笔记打上标签，这样的话你想要从你收集到的信息中找到你想要的信息就很快了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二个可以说是最重要的步骤是对知识进行理解。在很多人的概念中，学习可能就是指这一步骤了。在这一步骤也包含了很多的内容，其中最关键的一点是对学习的理解。对于这些已经有相关的书籍介绍了，其中包括思维导图，如何高效学习，如何阅读一本书等等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在理解了信息之后，我们往往会对信息做相应的链接，归纳总结等，这个步骤是在各种信息发生碰撞之后产生的，也是基于对相关信息进行检索，分类这些步骤。这个过程往往基于你对信息的理解，组合方式。在这里就不做过多的叙述。 知识保存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在对信息进行加工之后，我们往往会得到一份更加精炼的信息，这些精炼的信息往往是对理解之前的信息的一个归纳总结，往往会以笔记的形式呈现。这个过程仍然推荐印象笔记来对知识进行保存。虽然它的编辑功能用起来并不是那么方便，但是我可以借助另一个工具，markdown标记语言来快速的对信息进行编辑，而不用过多的考虑它的格式，这个也是markdown语言的初衷。具体介绍可以参看以下链接： 分享&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们为什么要进行知识分享呢，知识分享具有如下好处：通过对知识的分享，我们对知识的理解可以更加深刻。 通过知识的分享可以找到志同道合的朋友。*知识的分享也是对自己的一种推销，让招聘者可以更加清晰地看到你的能力。 运用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学以致用，这才是我们学习的最终目标，所以我们最后要将所学的知识运用起来。这里对于知识如何运用不做过多的阐述，毕竟每个领域知识的运用方式不相同。对于计算机科学领域，知识的运用主要体现在通过程序设计或者是算法设计来满足我们的需求。 创新&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创新是知识管理的一个重要的环节，在我们运用知识解决问题的时候，必然会遇到各种各样的难题，这个时候就需要创新性的思维，来解决相关问题，使其满足我们的需求。对于人工智能算法而言，如何将算法做得更加准确速度更加快以满足实际的应用需求，这就需要大量的创新性工作，而这些创新性工作的基础便是对于已有的知识的学习理解。而创新性的知识之后则又成为了一项成熟的技术或者知识，如此形成一个环，不断迭代，推动着我们整个社会的发展。 相关链接1.开源翻墙工具xxnet https://github.com/XX-net/]]></content>
      <categories>
        <category>工作环境</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fpost%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
